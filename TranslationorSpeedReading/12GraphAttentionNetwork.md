# Graph Attention Network

以下所有内容只是<https://zhuanlan.zhihu.com/p/531294308>的复制。
目的是方便注释和笔记。

## 摘要

论文提出了一种基于图结构数据的新型神经网络结构，即图注意网络（GATs），它利用掩码的自我注意力层来解决以前基于图卷积或其近似的方法的缺点。通过堆叠节点能够参与其邻域特征的层，**论文可以（隐式地）为邻域中的不同节点指定不同的权重，而无需任何代价高昂的矩阵操作（如求逆）或事先了解图结构**。通过这种方式，同时解决了基于谱的图神经网络的几个关键挑战，并使论文的模型易于适用于归纳式（inductive）问题和直推式（transductive）问题。论文的GAT模型已经实现或匹配了四个已建立的直推式和归纳式图基准的最新结果：Cora、Citeseer和Pubmed引文网络数据集，以及蛋白质-蛋白质相互作用数据集（其中测试图在训练期间仍然看不到）。

## 1 介绍

卷积神经网络（CNN）已成功应用于解决图像分类（He et al.，2016）、语义分割（J́egou et al.，2017）或机器翻译（Gehring et al.，2016）等问题，其中底层数据表示具有网格状结构。这些体系结构通过将其应用于所有输入位置，有效地重用了具有可学习参数的本地过滤器。

然而，许多有趣的任务涉及的数据不能用网格状结构表示，而是位于不规则的域中。这是3D网格、社交网络、电信网络、生物网络或大脑连接体的情况。此类数据通常可以图（Graph）的形式表示。

文献中曾多次尝试将神经网络扩展到处理任意结构图。早期的工作使用递归神经网络处理图域中表示为有向无环图的数据（Frasconi等人，1998年；Sperduti&Starita，1997年）。Gori et al.（2005）和Scarselli et al.（2009）引入了图神经网络（GNN），作为递归神经网络的推广，可以直接处理更一般的图类，例如循环图、有向图和无向图。GNN由一个迭代过程组成，该过程传播节点状态直至平衡；然后是神经网络，该网络根据每个节点的状态为其生成输出。Li等人（2016）采纳并改进了这一想法，他们提议在传播步骤中使用门控递归单位（Cho等人，2014）。

然而，人们对将卷积推广到图域越来越感兴趣。这方面的进展通常分为谱方法和非谱方法。

一方面，谱方法使用图的谱表示，并已成功应用于节点分类。在Bruna et al.（2014）中，卷积运算通过计算图Laplacian的特征分解在傅里叶域中定义，从而导致潜在的密集计算和非空间局部化滤波器。这些问题在后续工作中得到了解决。Henaff et al.（2015）引入了具有平滑系数的光谱滤波器的参数化，以使其在空间上局部化。后来，Defferard et al.（2016）提出通过图Laplacian的切比雪夫展开来近似滤波器，消除了计算Laplacian特征向量的需要，并产生了空间局部化滤波器。最后，Kipf&Welling（2017）通过限制过滤器在每个节点周围的一步邻域中运行，简化了之前的方法。然而，在上述所有谱方法中，学习滤波器依赖于拉普拉斯特征基，拉普拉斯特征基依赖于图结构。因此，在特定结构上训练的模型不能直接应用于具有不同结构的图。

另一方面，也有非谱方法（Duvenaud et al.，2015；Atwood&Towsley，2016；Hamilton et al.，2017），它们直接在图上定义卷积，在空间上相邻的组上操作。这些方法的挑战之一是定义一个与不同规模的社区合作并保持CNN权重共享特性的操作符。在某些情况下，这需要学习每个节点度的特定权重矩阵（Duvenaud et al.，2015），使用转移矩阵的幂定义邻域，同时学习每个输入通道和邻域度的权重（Atwood&Towsley，2016），或提取和规范化包含固定数量节点的邻域（Niepert et al.，2016）。Monti et al.（2016）提出了混合模型CNN（MoNet），这是一种空间方法，可将CNN架构统一概括为图。Hamilton等人（2017）引入了GraphSAGE，这是一种以归纳方式计算节点表示的方法。该技术通过对每个节点的固定大小邻域进行采样，然后对其执行特定的聚合器（例如对所有采样邻域的特征向量进行平均，或通过递归神经网络对其进行反馈的结果）。这种方法在几个大型归纳基准测试中取得了令人印象深刻的性能。

在许多基于序列的任务中，注意力机制几乎已成为事实上的标准（Bahdanau等人，2015年；Gehring等人，2016年）。注意力机制的一个好处是，它们允许处理大小可变的输入，关注输入中最相关的部分以做出决策。当使用注意机制计算单个序列的表示时，通常称为自注意力或内部注意力。与递归神经网络（RNN）或卷积一起，自注意力被证明对机器阅读（Cheng等人，2016）和学习句子表征（Lin等人，2017）等任务有用。然而，Vaswani等人（2017）表明，自注意力不仅可以改进基于RNN或卷积的方法，而且足以构建一个强大的模型，在机器翻译任务中获得最先进的性能。

受这项最新工作的启发，论文引入了一种基于注意力的体系结构来执行图结构数据的节点分类。其思想是通过关注其邻居，遵循自注意力策略，计算图中每个节点的隐藏表示。注意力体系结构有几个有趣的特性：（1）操作是有效的，因为它可以跨节点邻居对并行化；（2） 通过对邻域指定任意权重，可以将其应用于具有不同程度的图节点；（3）该模型直接适用于归纳学习问题，包括模型必须推广到完全看不见的图的任务。论文在四个具有挑战性的基准上验证了所提出的方法：Cora、Citeseer和Pubmed引文网络以及诱导性蛋白质-蛋白质相互作用数据集，实现或匹配最先进的结果，突出了基于注意力的模型在处理任意结构图时的潜力。

值得注意的是，正如Kipf&Welling（2017）和Atwood&Towsley（2016）一样，论文的工作也可以重新表述为MoNet（Monti等人，2016）的一个特殊实例。此外，论文跨边共享神经网络计算的方法让人想起关系网络（Santoro et al.，2017）和VAIN（Hoshen，2017）的公式，其中对象或代理之间的关系通过共享机制成对聚合。类似地，论文提出的注意力模型可以与Duan等人（2017）和Denil等人（2017）的工作相联系，他们使用邻域注意力操作来计算环境中不同对象之间的注意力系数。其他相关方法包括局部线性嵌入（LLE）（Roweis和Saul，2000）和内存网络（Weston等人，2014）。LLE在每个数据点周围选择固定数量的邻居，并学习每个邻居的权重系数，以将每个点重建为其邻居的加权和。第二个优化步骤提取点的特征嵌入。内存网络也与论文的工作有一些联系，特别是如果将节点的邻域解释为内存，内存用于通过关注其值来计算节点特征，然后通过将新特征存储在同一位置来更新。