# Time is of the Essence: a Joint Hierarchical RNN and Point Process Model for Time and Item Predictions 速读

时间才是本质：对于Time和Item预测的一个联合分层结构的RNN和点过程模型

## abstract

近年来，基于会话的推荐已成为一种越来越适用的推荐类型。由于会话由事件序列组成，这种类型的建议自然适合于递归神经网络（RNN）。为了处理特定的问题或数据，已经提出了一些扩展此类模型的补充。

两个这样的扩展是1.）会话间关系的建模，用于捕获用户会话的长期依赖关系，

以及2.）对用户项交互的时间方面进行建模。前者允许基于会话的建议在提供新建议时利用扩展的会话历史和会话间信息。后者已被用于为用户何时返回服务提供最先进的预测，以及改进建议。在这项工作中，我们将这两个扩展组合在一个联合模型中，用于推荐和返回时间预测任务。该模型包括用于会话间和会话内项目建议的分层RNN，并扩展了用于会话间时间间隔的点过程模型。实验结果表明，与基线回归时间预测模型相比，所提出的模型显著改善了两个数据集在强基线上的建议，同时改善了回归时间预测。

## 1. 简介

随着越来越多的信息可用，在网上查找相关信息是一个越来越具有挑战性的问题。试图查找特定内容的用户可能会进入信息过载状态，无法有效地解释可用信息。如今，推荐系统被广泛应用于帮助用户浏览大量网络内容（视频、音频、书籍、产品等）。这些系统的指导原则是应用预测建模，利用用户与系统的交互历史推断用户的偏好。然而，在许多情况下，不可能追踪长期的互动历史，相反，只有关于近期互动会话的信息。由于过去几年越来越关注数据隐私，这种情况可能会变得更加普遍。基于会话的推荐系统通过能够使用会话本身中可用的信息来解决这个问题。具体而言，**预测问题被转换为序列预测问题**，其中，应用具有用户在**一组会话中采取的一系列动作和交互的历史来预测给定会话或新会话中的下一个k个动作**[5]。

回归时间预测是另一个重要的挑战，已经使用不同的策略进行了探索。这里的主要挑战是开发一个时间预测模型，该模型使用以前的会话信息来推断用户何时返回或启动给定应用程序或服务中的新会话。这项任务之所以重要，有两个原因：1。）通过web服务的返回时间动态建模用户保留率，可以深入了解手头的服务，使服务决策者能够更好地优化服务，以最大限度地延长服务时间。这是网络经济中一个有价值的指标，它是由广告推动的[10]。2.）在推荐系统的上下文中，时间信息和推荐之间存在相互作用，可以利用这些信息利用预测模型，更好地理解用户参与推荐系统的情况[24]。

在本文中，我们联合处理这两个任务，基于这样一个假设，即用户参与项目和时间信息之间的相互作用有利于为这两个任务提供良好的预测。**我们提出了一种新的神经模型，能够使用递归神经网络（RNN）进行下一项预测，并使用点过程进行返回时间预测**。例如，考虑图1中所示的上下文，其中对于给定的用户有一系列会话，并且在每个会话中有一系列关于用户-项目交互（点击、购买等）与一系列项目（由有色圆表示）。这里还有关于用户返回服务并启动新会话之前的时间的信息。鉴于上述动机，有必要构建一个模型，该模型能够捕获会话间和会话内的用户-项目动态，并使用过去的用户-项目动态和时间信息来预测新会话何时开始，以及提供建议。总而言之，这项工作的主要贡献是：

•定义并引入时间层次递归神经网络（THRNN）：会话间和会话内建议及返回时间预测的联合模型。它由三个主要组件组成，一个用于表示会话序列的模块，一个用于每个会话中用户项交互序列的模块，一个用于会话之间时间间隔的模块。前两个模块采用分层递归神经网络（HRNN）结构，第三个模块采用与HRNN共享表示的点过程。

•训练过程中的调整机制，允许时间模型调整短期、中期或长期预测的重点。该机制包括在损失函数中添加一个控制参数，允许我们在训练时控制时间信息的重要性在推荐和返回时间预测任务中对所提出模型性能的实验评估。结果表明，与强基线模型相比，这两项任务都有所改进，并表明联合建模对回归时间预测任务尤其有利。最后，对于回归时间预测任务，我们证明了短、中、长时间的控制机制是有效的。


给定一组用户-项目交互；应变={（u，i，j，tj）|u，i，j∈ N、 t∈ R+}，每个元组表示用户u在时间tj与会话j中的项i交互。我们的目标是学习一个函数，该函数估计一个会话中新项目的得分、下一个会话中初始建议的得分，以及当前会话和下一个会话之间的时间间隔预测。更正式地说，考虑到我们的训练集应变和会话中某个位置的索引j，我们需要一个函数f（应变，j），它将输出一个元组（sj+1，fj+1，ηtj+1），其中包含会话内推荐分数sj+1、下一个会话初始项分数fj+1和时间间隔ηtj+1。因此，我们的目标是建立一个模型，该模型可以预测一次会议和下一次会议的新项目，以及预测下一次会议的时间间隔（返回时间预测）。



近年来，许多深度学习技术被成功地用于推荐。特别是，RNN的使用已被证明对基于会话的推荐是有希望的。基于会话的推荐在历史上一直是通过基于项对项/邻域的方法[20]来处理的，没有考虑会话数据的顺序性质。为这种连续性建模的自然选择是基于RNN的模型，如[5,7,16,19,22]中给出的模型。[5]中介绍的GRU2REC体系结构被广泛认为是第一个应用基于RNN的推荐模型并实现最先进结果的体系结构。作者假设有附加会话的用户是匿名的，没有任何用户历史记录，并假设会话彼此独立。每个会话被建模为GRU单元的单层，后跟一个前馈层。GRU单元具有循环连接，提供状态并捕获会话的时间动态，而前馈层输出每个项目的分数。提出的模型在两个不同的数据集上进行了测试，在[18]的评估指标中获得了20-30%的增益，这是一个基于最近邻的模型，被认为是性能最好的基线。[14]对[5]中基于会话的推荐和GRU2REC的多种方法进行了系统比较。[19]中介绍了GRU2REC模型的扩展，第二级RNN试图捕捉会话间动态。所提出的模型是一个分层RNN，其中一个级别考虑会话间动态，另一个级别考虑会话内动态。与假设完全匿名用户和完全独立会话不同，此扩展允许考虑一些简单且低成本的用户历史，以便为实时会话提供更好的建议。用户历史记录是以前会话的抽象表示形式。按时间顺序向会话间RNN提供有限数量的最新会话表示，并将输出用作会话内RNN的初始隐藏状态。

这背后的动机是处理冷启动问题。与[5]相比，[19]在为项目添加嵌入层时获得了最佳效果。此外[19]没有使用交错会话小批量方案，而是选择填充会话。他们也没有对输出进行采样，也没有使用任何成对损失。[16]中的工作提出了一种与[19]中的模型非常相似的体系结构。他们还提出了一个会话间RNN层，以便在用户以以前会话的形式访问有限的用户历史时处理冷启动问题。该分层模型与[19]中提出的模型之间的主要区别在于[16]只考虑了基于会话内RNN的最后隐藏状态的会话表示方案。此外，这些会话表示是通过使用双曲正切激活函数将最后一个隐藏状态传递给最终的单层前馈层来创建的。他们还尝试将会话间网络的输出传播到会话内RNN中的所有时间步长，这在一定程度上使模型复杂化，但对于所评估的其中一个数据集来说，取得了稍好的结果。




事实证明，分层模型总体上表现最好，差距很大。在[22]中，对[5]中提出的方法进行了改进，提出了一个数据增强预处理步骤，以提高模型的稳健性，并建议输出一个项目嵌入，而不是每个项目的单独分数，以加快推荐速度。[7]中的工作提出了一种方法，通过使用项目余弦相似性度量在第一步查找与实时会话相似的会话，将最近邻方法用于基于会话的推荐任务。对于预测，他们使用最近邻建议和RNN建议的加权和。


关于使用RNN的其他著名工作集中于如何最好地处理上下文和功能丰富的输入，如[6,13,21]。在[13]中，通过使用基于输入上下文的不同权重集进行训练和评估，扩展了一个简单的会话内RNN模型。例如，该上下文可以是事件发生时不同粒度、位置或天气的时间和日期。在[6]中，重点是在RNN输入中提供功能丰富的输入。在[21]中，作者提出了一类新的上下文递归推荐神经网络（CRNNs），它以两种不同的方式考虑上下文信息：一种情况下，通过结合上下文嵌入和输入嵌入，类似于[23]，另一种情况下，通过将其纳入模型动力学。注意机制也被用于RNN设置中，以提供建议[11]。这里的想法是在隐藏状态上使用注意机制，以提供更好的表示，从而预测后续项目。



尽管RNN天生就能够捕获一些时间依赖性，但这在很大程度上取决于序列中事件的顺序。事件或会议之间的时间间隔、季节、年份、工作日和一天中的时间等方面都是时间方面，可能会影响许多领域的理想建议，但单凭事件的顺序很难捕捉到。已经有人尝试通过在RNN中直接为简单推荐任务[13]提供此类信息来进行时间感知推荐，以及为预测下一个会话的返回时间的任务建模时间[2,8,26]。




[2]中提出的模型——基于单层RNN——除了推荐下一个项目外，还试图预测用户的返回时间。时间预测被建模为一个标记点过程，强度函数取决于历史和下一事件发生的时间。类似地，在结合推荐和时间建模时，[8]中提出的模型使用生存分析来建模时间，而不是标记点过程。另一个区别是，该模型用于下一个篮子推荐任务，而不仅仅是单个项目序列。此外，时间建模基于篮子/会话之间的时间间隔，而不是每次选择之间的时间间隔。这意味着直接使用单级RNN进行会话间建模，与[19]和[16]不同，两者都在其分层RNN模型的两个级别之一中实现某种形式的会话间建模。将最终模型的推荐能力与两个基于因子分解的基线和一个基于神经网络的基线进行了比较。结果表明，在两个不同的数据集上，它的表现优于所有这些。将时间预测与表达点过程（其中一个是霍克斯点过程）进行比较，得到的时间误差比所有这些过程都小。最后，在[26]中，从观测开始，RNN单元擅长建模实体的顺序，但不提供实体之间时间间隔的任何固有支持，他们建议通过显式时间门将时间方面纳入LSTM。


此外，人们还致力于使用点过程强度建模，作为对能够进行时间建模的混合递归神经网络的启发。Du等人[3]的方法在于创建一种新型的递归神经单元，具有两个输出，一个用于时间模型，另一个用于一般预测模型。本文的主要贡献是提出了一种强度函数的建模方法，该方法结合了用于长时间依赖和近期预测的隐藏层，并结合了通常的预测损失和基于强度函数的点过程的负对数似然。这产生了一个能够预测标记和标记时间的模型，该模型具有通过神经结构学习的时间模型的强度函数。肖等人[25]也采取了类似的方法，通过神经架构对点过程的强度函数进行建模，但在这种情况下，使用了两个RNN：一个用于时间序列数据，捕捉背景强度率，另一个用于事件数据，捕捉事件之间的长程关系。最后，Mei和Eisner[15]提出了一种以Hawkes过程为起点的连续时间LSTM，并对LSTM的内部动力学进行了修改，以实现该目标。由此产生的模型是一个与霍克斯过程具有类似性质的LSTM。



我们提出了一个系统，旨在预测下次会议的返回时间，并推荐下一个项目。它基于层次RNN（HRNN）模型，并通过时间模型部分进行了增强。HRNN的输入是之前会话的表示以及上下文信息，然后是会话中的项目表示。图2显示了该系统的概述。




HRNN受[19]的启发，由会话间RNN和会话内RNN组成。GRU单元用于会话内和会话间RNN。选择GRU是因为它能够解决消失梯度问题，并且发现它在这个问题和模型结构上比LSTM工作得更好。与[19]一样，会话间RNN被馈送固定数量的前一个会话表示。此外，在该模型中，与会话相关的上下文信息被连接到该输入。会话间RNN的最终隐藏状态被传播到主要代表会话间信息的会话内RNN，并用于返回时间预测任务。会话内RNN使用会话间RNN的最后一个隐藏状态作为初始状态，并在输入中使用项表示。对于嵌入到输入中的每个项目，RNN的相应输出被传递到线性层，该线性层输出每个目标项目的分数。通过选择得分最高的项目给出推荐。


与会话相关的附加上下文信息与会话内RNN的最后一个隐藏状态连接，表示会话间RNN的输入。我们考虑在模型的主要设置中使用的三种类型的嵌入：1）项目嵌入，2）会话间间隙时间嵌入和3）用户嵌入。这种嵌入的目的是学习更精细的动力学和嵌入实体的表示。例如，如果两个不同的艺术家经常被具有相似品味的用户聆听，嵌入层很可能会学习到在某种意义上彼此相似的艺术家表现。通过使用RNN，除了更一般的“相似性度量”，这种表示还可以学习时间动态。这种具象知识的一个简单例子可以是：“艺术家A几乎总是在艺术家B之前被聆听”。嵌入还可以学习相异性和非线性。例如，在考虑课间休息时间时，24小时和48小时（周期性日常行为）之间的间隔大小可能比12小时和24小时之间的间隔大小之间的相关性更高。

3.1.1项目嵌入。这些嵌入表示数据集中的每个唯一项。学习到的嵌入由会话内RNN直接处理，因此仅通过模型这一部分的损失进行训练。这意味着项目嵌入将影响网络的早期部分，但作为输入，而不是计算图形。因此，它们的梯度不能流回嵌入层，也不能用于进一步训练嵌入。由于所考虑的数据集中存在大量项目，这种嵌入的维度是迄今为止最大的。

3.1.2会话间隔时间嵌入。这些嵌入表示会话之间的时间间隔。首先对时间间隔进行归一化，然后将其划分为离散桶。然后，生成的bucket id可用于索引嵌入表/层，以传播相应的嵌入。研究了两种不同的归一化方案。首先给出一个上界，它设置了间隙时间的阈值，之后我们不认为用户足够活跃以提供准确的时间预测。大于此界限的间隔时间设置为上限。第一种标准化方案，将间隙时间范围划分为均匀大的桶。这样做的好处是，间隙时间范围内的所有值都将属于大小相同的一个存储桶，因此间隙时间不会与更高或更低的间隙时间位于同一个存储桶中。使用这种方法的一个缺点是，早期的“流行”存储桶可能会过于拥挤，而后期的存储桶可能最终几乎是空的，这会使此类嵌入难以训练。人们还需要高分辨率来覆盖较小时间间隔范围内的细微差异，这进一步增加了稀疏桶的问题。在第二个标准化方案中，间隔时间范围被统一划分为多个桶。这会导致不同存储桶中的间隔时间分布更均匀，但代价是更大间隔时间的分辨率更粗糙，而相应的存储桶比之前的存储桶覆盖更多时间。我们观察到，第二种方案在小分辨率下表现更好，但总体上优于分辨率更高的统一方案。由于无论是模型性能还是运行时间，高分辨率都不是问题，因此一致性被认为是更好的选择。


3.1.3用户嵌入。这些嵌入主要受[9]的启发，学习会话表示历史之外的用户行为对于较长的用户历史尤其有用。例如，如果用户在最近几次会话中的行为有点不寻常和零星，则带有用户嵌入的model可以包含有关长期用户行为的信息，这有助于理解/覆盖最近的嘈杂行为。



该模型的主要目标是预测下一个会话开始之前的时间，给定以抽象会话表示形式存在的固定长度历史和相应的上下文时间信息。时间建模很大程度上受到了[3]中工作的启发，在[3]中，时间建模用于预测给定单个前一项序列的下一项建议的时间，以及改进建议。在他们的模型中，所选项目之间的间隔时间被认为是从标记点过程中得出的。标记点过程的参数化由作者定义，取决于之前建模为RNN的选择历史、相应的选择间隔时间以及最后一次选择的时间。在我们的系统中，历史记录基于会话表示，而不是单个项目。因此，相应的时间间隔是历史上会话表示之间的时间间隔。这种会话间建模以及级联嵌入与[9]中所做的更为相似。在点过程中，我们将强度函数定义为：






在图4中，我们展示了所提出的回归时间预测模型与两个基线Hawkes模型（带有调整参数α）的性能∈ [0.3,0.5,0.9]在等式5中。对于短时间间隔，会话嵌入和点过程强度的联合模型给出了比基线更好的预测。结果与每个时间间隔的观察次数一起绘制，这提供了关于会话中的用户-项目交互如何与模型相关的深刻信息。我们观察到，在用户-项目观察较多的时间间隔内，联合模型始终优于基线。这是直观的，因为时间模型和闭会期间模型之间有共同的学习参数，从这个意义上说，这些参数不仅适合时间间隔，而且也适合用户-项目交互动态。我们看到，调整α允许我们控制哪些时间间隔对模型最重要。然而，调整最重要的时间间隔是以对被认为不太重要的时间间隔进行更糟糕的返回时间预测为代价的。

在本文中，我们介绍了一种新的联合模型，该模型能够提供会话内和会话间的建议以及会话间返回时间预测。我们的结果表明，联合建模会话内和会话间的用户项目交互，无论是针对建议还是时间预测，都有利于基于会话的项目建议和返回时间预测任务。联合模型在推荐和时间预测任务方面与最新技术具有竞争力。特别是在后一种情况下，与在线社交网络中时间预测的最先进模型（霍克斯点过程模型）相比，我们报告了显著的改进。此外，我们在损失函数中引入了一种控制和权衡短期、中期和长期预测精度的机制，结果表明它是有效的。例如，在这一框架内的未来研究可能涉及我们如何融合多个损失，每个损失侧重于不同的时间间隔，以实现更全面的损失。