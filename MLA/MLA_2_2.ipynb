{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Design a NIN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路\n",
    "1. 确定输入的维度。\n",
    "2. 确定卷积的形状。\n",
    "3. 确定1*1的卷积。\n",
    "4. 池化。\n",
    "5. 输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 老师这个cell是在网上找的[参考](https://zh-v2.d2l.ai/chapter_convolutional-modern/nin.html)\n",
    "import tensorflow as tf\n",
    "\n",
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(num_channels, kernel_size, strides=strides,\n",
    "                               padding=padding, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\n",
    "                               activation='relu')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结：\n",
    "1. 一般卷积的操作有3个地方需要注意：\n",
    "   1. batch的作用是对应输出的批次。\n",
    "   2. filter是一组卷积核，这组卷积核的数量对应于输入的channel的数量。同时filter的数量决定了下一层的channel的数量。也就是说filter中的每一个卷积核会对应输入里面的每一个channel进行卷积，然后将filter中所有卷积核的结果再求和，得到输出channel中的一片。\n",
    "   3. 卷积层之间变换的卷积核都是二维的。\n",
    "2. 1*1的卷积操作需要注意的：\n",
    "   1. 1*1的卷积不会改变输出卷积层相对于输入卷积层的长和宽（注意这里没有提到channel），但是一般都会修改channel的数量。\n",
    "   2. 它的主要目的在于尽可能的提取非线性特征。\n",
    "3. 卷积层向全连接层变化的时候的是采用了一种比较特殊的卷积操作。\n",
    "   1. 每个filter就只有一个卷积核。\n",
    "   2. 卷积核的形状和输入层的形状一致。这个形状包括长、宽、channel数。这个卷积核是一个**三维**的卷积核。和前面的卷积核不同，前面的卷积层向卷积层之间的卷积核都是**二维**的。\n",
    "   3. filter的数量和全连接层向量的数量相同。\n",
    "   4. 在tensorflow里面这一个操作是通过Flatten层来完成过渡的（注意这里只是过渡）。[参考中的1.4就做了说明](https://www.cnblogs.com/peng8098/p/keras_7.html)，官方手册里面并没有详细说明原理，只说明了结果。在实际的编写中尝试进行了使用这种卷积核的卷积操作，但是是无法在summary中看到降低维度的结果的。[已经下载下来参考也说得非常清晰](../references/whatisFullyConnectedLayer.pdf)。\n",
    "4. 要注意使用池化层来降低模型的维度，不然模型的维度太高了PC电脑要运行超过1分钟。\n",
    "5. 一定要注意输入输出的维度。这是在定制模型的过程中非常重要的地方。自己心里要设计好这种维度，要做到心中有数。\n",
    "   1. [这个作者的使用tensorflow编写模型的方式非常值的借鉴](https://blog.csdn.net/qq_42308217/article/details/110209432)。在下面的cell里面也展示了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2D (Conv2D)              (1, 256, 256, 128)        102528    \n",
      "_________________________________________________________________\n",
      "firstPoolLayer (MaxPooling2D (1, 127, 127, 128)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1m1_0 (Conv2D)        (1, 127, 127, 256)        33024     \n",
      "_________________________________________________________________\n",
      "secondPoolLayer (MaxPooling2 (1, 63, 63, 256)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1m1_1 (Conv2D)        (1, 63, 63, 512)          131584    \n",
      "_________________________________________________________________\n",
      "LastPoolLayer (MaxPooling2D) (1, 20, 20, 512)          0         \n",
      "_________________________________________________________________\n",
      "ConvolutiontoFullyConnected  (1, 204800)               0         \n",
      "_________________________________________________________________\n",
      "FirstFullyConnectedLayer (De (1, 1028)                 210535428 \n",
      "_________________________________________________________________\n",
      "SecondFullyConnectedLayer (D (1, 1028)                 1057812   \n",
      "_________________________________________________________________\n",
      "output (Reshape)             (1, 1, 1, 1028)           0         \n",
      "=================================================================\n",
      "Total params: 211,860,376\n",
      "Trainable params: 211,860,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 这是我自己的实现。但是中间的卷积层向全连接层的过渡和我自己的理解不一样。\n",
    "# 在tensorflow里面使用了flatten来完成卷积层向全连接层的过渡。\n",
    "import numpy as np\n",
    "\n",
    "# 使用tensorflow的实现。\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# input = np.arange(512*512*32, dtype=float).reshape(512, 512, 32)\n",
    "# output = np.arange(128*128*64, dtype=float).reshape(128, 128, 64)\n",
    "\n",
    "# 输出的通道数\n",
    "input_filters = 1028\n",
    "\n",
    "outputFilters =64\n",
    "# input_dim=115, use_bias=True,\n",
    "model = tf.keras.models.Sequential()\n",
    "# model.add(layers.InputLayer(input_shape=(512, 512, 32), name=\"input\", dtype=\"float32\"))\n",
    "\n",
    "# 如果没有理解错的画，Conv2D会自动适应上一层的通道数。也就是说每一个filter中卷积核的个数是输入层\n",
    "# 的通道数。这也就解释了为什么没有使用3维卷积来处理类似(512, 512, 32)的情况。而如果使用3维的卷积\n",
    "# 核，在适应输入的通道数这个维度，然后在使用model.build(input_shape=(1, 512, 512, 32))实现输入\n",
    "# 的时候第一个维度表示的batch。也就是输入的批次数。如果只有1个批次，那么可以填1即可。\n",
    "# 在使用model.add(layers.InputLayer(input_shape=(512, 512, 32), name=\"input\", dtype=\"float32\"))\n",
    "# 实现的时候，就不用输入batch这个参数。\n",
    "model.add(layers.Conv2D(128, kernel_size=5,\n",
    "                        strides=2, activation=\"relu\", padding=\"same\", name=\"conv2D\"))\n",
    "# 最后的全连接层并不是通过dense来实现的，而是通过1*1的卷积来实现的。这里有点没有想明白。\n",
    "# 难道不是通过一次1*1的卷积之后，后面的才是2层全连接层吗？\n",
    "# model.add(layers.Dense(2048, activation=\"relu\"))\n",
    "# model.add(layers.Dense(2048, activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(pool_size=3, strides=2, name=\"firstPoolLayer\"))\n",
    "model.add(layers.Conv2D(256, kernel_size=1,\n",
    "                        strides=1, activation=\"relu\", padding=\"same\", name=\"Conv2D_1m1_0\"))\n",
    "model.add(layers.MaxPool2D(pool_size=3, strides=2, name=\"secondPoolLayer\"))\n",
    "model.add(layers.Conv2D(512, kernel_size=1,\n",
    "                        strides=1, activation=\"relu\", padding=\"same\", name=\"Conv2D_1m1_1\"))\n",
    "model.add(layers.MaxPool2D(pool_size=5, strides=3, name=\"LastPoolLayer\"))\n",
    "# 我理解的卷积层向全连接层的过渡是使用了一个和卷积层的长、宽、通道数一样的卷积核的卷积层，通道数是输出向量的长度\n",
    "# 下面这一层就是。\n",
    "# 是不是在使用dense的时候，tensorflow自适应的卷积核的大小？但是在输出summary的时候维度并没有变化。\n",
    "# model.add(layers.Conv2D(128, kernel_size=31,\n",
    "#                         strides=1, activation=\"relu\", padding=\"same\", name=\"Conv3DtoFullyConnected_1\"))\n",
    "\n",
    "# tensorflow好像使用的是flatten来实现卷积层向全连接层的变化的。\n",
    "model.add(layers.Flatten(name=\"ConvolutiontoFullyConnected\"))\n",
    "model.add(layers.Dense(1028, activation=\"relu\", name=\"FirstFullyConnectedLayer\"))\n",
    "model.add(layers.Dense(1028, activation=\"relu\", name=\"SecondFullyConnectedLayer\"))\n",
    "# 最后为1个元素的输出作为\n",
    "# model.add(layers.Dense(1, activation=\"relu\", name=\"output\"))\n",
    "# 最后一个输出的形状是1*1*1028。这个地方还需要向老师确认。\n",
    "model.add(layers.Reshape((1, 1, 1028), name=\"output\"))\n",
    "\n",
    "# input_shape 这里是指定输入的结构，其中第一个是batch，第二个和第三个是图片的长宽，\n",
    "# 第三个是通道数，如果是RGB那么就是3。\n",
    "model.build(input_shape=(1, 512, 512, 32))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 55, 55, 48)        17472     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 27, 27, 128)       153728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 13, 13, 192)       221376    \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 13, 13, 192)       331968    \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 13, 13, 128)       221312    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2048)              9439232   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1000)              2049000   \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 16,630,440\n",
      "Trainable params: 16,630,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 参考AlexNet实现。[参考](https://blog.csdn.net/qq_42308217/article/details/110209432)\n",
    "\n",
    "from tensorflow.keras import layers, models, Model, Sequential\n",
    "im_height=224\n",
    "im_width=224\n",
    "class_num=1000\n",
    "input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")  # output(None, 224, 224, 3)\n",
    "x = layers.ZeroPadding2D(((1, 2), (1, 2)))(input_image)   #valid和same都不能满足输出，因此需要手动padding处理 output(None, 227, 227, 3)\n",
    "x = layers.Conv2D(48, kernel_size=11, strides=4, activation=\"relu\")(x)       # output(None, 55, 55, 48)\n",
    "x = layers.MaxPool2D(pool_size=3, strides=2)(x) #padding默认等于valid  # output(None, 27, 27, 48)\n",
    "x = layers.Conv2D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)  # output(None, 27, 27, 128)\n",
    "x = layers.MaxPool2D(pool_size=3, strides=2)(x)                              # output(None, 13, 13, 128)\n",
    "x = layers.Conv2D(192, kernel_size=3, padding=\"same\", activation=\"relu\")(x)  # output(None, 13, 13, 192)\n",
    "x = layers.Conv2D(192, kernel_size=3, padding=\"same\", activation=\"relu\")(x)  # output(None, 13, 13, 192)\n",
    "x = layers.Conv2D(128, kernel_size=3, padding=\"same\", activation=\"relu\")(x)  # output(None, 13, 13, 128)\n",
    "x = layers.MaxPool2D(pool_size=3, strides=2)(x)                              # output(None, 6, 6, 128)\n",
    "\n",
    "x = layers.Flatten()(x)                         # output(None, 6*6*128=4608)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048, activation=\"relu\")(x)    # output(None, 2048)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048, activation=\"relu\")(x)    # output(None, 2048)\n",
    "x = layers.Dense(class_num)(x)                  # output(None, 5)\n",
    "\n",
    "predict = layers.Softmax()(x)#将输出转化成为一个概率分布\n",
    "\n",
    "model = models.Model(inputs=input_image, outputs=predict)\n",
    "model.summary()\n",
    "# predict = layers.Softmax()(x)#将输出转化成为一个概率分布\n",
    "\n",
    "# model = models.Model(inputs=input_image, outputs=predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 梯度使用\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) \n",
    "print(dy_dx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
