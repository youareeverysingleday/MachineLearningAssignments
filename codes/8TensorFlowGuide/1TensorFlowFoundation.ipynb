{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow基本操作\n",
    "\n",
    "1. 使用的tensorflow版本是2.8.0 \n",
    "2. 参考视频<https://www.bilibili.com/video/BV1Ub4y1e7P3?p=4>\n",
    "3. **重点-参考网页**<https://zhuanlan.zhihu.com/p/377280469>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 6 * 5矩阵\n",
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "\n",
    "# 方阵\n",
    "R_Square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建指定大小的矩阵\n",
    "\n",
    "推荐使用第二种和第三种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建全零的tensor矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =5\n",
    "K = 4\n",
    "n = 7\n",
    "# 方法一\n",
    "P = tf.zeros([m, K], dtype=float)\n",
    "Q = tf.zeros([K, n], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建指定大小的tensor矩阵。**推荐**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.10860592, 0.02340075, 0.5073297 , 0.88656706, 0.9476791 ],\n",
      "       [0.41208768, 0.68982804, 0.6123209 , 0.51606447, 0.58443165],\n",
      "       [0.9573356 , 0.75104904, 0.08379865, 0.08022112, 0.5221114 ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **推荐**。使用tensorflow自己的函数创建随机产生的矩阵。而且矩阵中数值的分布满足每个元素都从均值为0、标准差为1的标准高斯分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9814952  -0.75365895 -0.02519071 -0.8719553 ]\n",
      " [-0.10746921  1.5164526  -1.0351312  -0.64127964]\n",
      " [ 1.5217911  -1.5514657   0.63164455 -0.17058079]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 4])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 创建全1矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE = tf.ones(shape=[3,4])\n",
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建单位矩阵\n",
    "\n",
    "对角矩阵是方阵，一般记为$diag(a_1,a_2,\\cdots,a_n)$。\\\n",
    "单位矩阵是对角矩阵的特例，所有对角线上的元素全为1。\\\n",
    "tf可以创建不是方阵的在$i=j$的位置上的类似对角矩阵的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = tf.eye(3)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_other = tf.eye(3,5)\n",
    "E_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将np.array转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "R_Square_tf = tf.convert_to_tensor(R_Square, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=30>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(R_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取tensor矩阵指定位置的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 2., 1., 3., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 1., 3., 0., 4., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor的形状\n",
    "\n",
    "1. 若有一个维度为-1，那么tensorflow会自动推导。\n",
    "2. tf.reshape(tensor, shape, name=None)\n",
    "    1. tensor：要改变维度（形状）的张量；\n",
    "    2. shape：希望变成什么维度；\n",
    "    3. name：操作的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "P = tf.reshape(R_tf[1, :], [1, -1])\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q = tf.reshape(R_tf[:,2], [-1, 1])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor之间的运算有两种\n",
    "1. 按元素的运算\n",
    "2. 按线性代数之间的预算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32) tf.Tensor(\n",
      "[[ 0.65293133 -1.0241536  -1.4574317   1.778316  ]\n",
      " [ 0.7952607  -1.4387778   0.06255197  0.3530161 ]\n",
      " [ 0.49021912  0.98361623 -0.93200964 -0.86789125]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.ones([3, 4], dtype=float)\n",
    "Q = tf.random.normal(shape=[3, 4])\n",
    "print(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 1.6529313 , -0.02415359, -0.45743167,  2.778316  ],\n",
       "       [ 1.7952607 , -0.4387778 ,  1.062552  ,  1.3530161 ],\n",
       "       [ 1.4902191 ,  1.9836162 ,  0.06799036,  0.13210875]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P + Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.34706867,  2.0241537 ,  2.4574318 , -0.778316  ],\n",
       "       [ 0.20473927,  2.438778  ,  0.937448  ,  0.64698386],\n",
       "       [ 0.5097809 ,  0.01638377,  1.9320097 ,  1.8678913 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.65293133, -1.0241536 , -1.4574317 ,  1.778316  ],\n",
       "       [ 0.7952607 , -1.4387778 ,  0.06255197,  0.3530161 ],\n",
       "       [ 0.49021912,  0.98361623, -0.93200964, -0.86789125]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 1.5315546 , -0.97641605, -0.6861385 ,  0.56232977],\n",
       "       [ 1.2574493 , -0.6950343 , 15.986708  ,  2.832732  ],\n",
       "       [ 2.039904  ,  1.0166566 , -1.0729502 , -1.1522181 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P / Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素求幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1.9211642 , 0.35910028, 0.2328335 , 5.919879  ],\n",
       "       [2.2150183 , 0.23721752, 1.0645498 , 1.4233541 ],\n",
       "       [1.632674  , 2.674109  , 0.3937616 , 0.41983595]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素判断tensor比较大小、判断是否相等，返回的是每个位置的boolen型值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P == Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False,  True],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P < Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, False],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P > Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素对tensor所有元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[19.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui = tf.matmul(tf.reshape(R_Square_tf[2, :], [1, -1]),tf.reshape(R_Square_tf[:, 3], [-1, 1])) - R_Square_tf[2,3]\n",
    "print(eui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间数乘\n",
    "\n",
    "1. 计算的是矩阵每个对应位置元素相乘。\n",
    "2. 需要两个矩阵形状一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 8., 15.,  0.,  2.,  0.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(R_Square_tf[2, :], R_Square_tf[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数乘以矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12.  0.  3.  0. 15.]\n",
      " [ 3.  6.  3.  9. 15.]\n",
      " [12. 15.  9.  3.  0.]\n",
      " [ 6.  9.  0.  6. 15.]\n",
      " [15.  3. 12.  0.  0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "print(a * R_Square_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor类型判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(eui, tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(eui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的类型转换\n",
    "\n",
    "1. 参考网页<https://wenku.baidu.com/view/9a3b8439fc00bed5b9f3f90f76c66137ef064f55.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些都是错误的，在tf 2.0之后都不支持了。\n",
    "\n",
    "# 将字符串转化为tf.float32（默认）和tf.int32\n",
    "# tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "\n",
    "# 转化为tf.float64\n",
    "# tf.to_double(eui, name='ToDouble')\n",
    "\n",
    "# 转化为tf.float32\n",
    "# tf.to_float(eui, name='ToFloat')\n",
    "\n",
    "# 转化为tf.int32\n",
    "# tf.to_int32(eui, name='ToInt32')\n",
    "\n",
    "# 转化为tf.int64\n",
    "# tf.to_int64(eui, name='ToInt64')\n",
    "\n",
    "# 转化为dtype指定的类型\n",
    "# tf.cast(x, dtype, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0. 1. 2. 3. 4.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.arange(5)\n",
    "print(x_0)\n",
    "x_1 = tf.convert_to_tensor(x_0, dtype=tf.int32)\n",
    "print(x_1)\n",
    "x_2 = tf.cast(x_1, dtype=tf.float32)\n",
    "print(x_2)\n",
    "x_3 = tf.cast(x_2, dtype=tf.int32)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵计算平方\n",
    "\n",
    "1. 计算的是每个元素的平方值\n",
    "2. tf.math还有这个类，下面应该有很多数学方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[361.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui_square = tf.square(eui)\n",
    "print(eui_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1.  4.  1.  9. 25.]\n",
      " [16. 25.  9.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "R_tf_square = tf.square(R_tf)\n",
    "print(R_tf_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算tensor所有行或者所有列的和\n",
    "\n",
    "1. 使用tf.reduce_sum来完成\n",
    "2. 建议都需要填写keepdims=True参数，用于保证tensor的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    This is the reduction operation for the elementwise `tf.math.add` op.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    of the entries in `axis`, which must be unique. If `keepdims` is true, the\n",
      "    reduced dimensions are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "      >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "      >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "      >>> x.numpy()\n",
      "      array([[1, 1, 1],\n",
      "             [1, 1, 1]], dtype=int32)\n",
      "      >>> # sum all the elements\n",
      "      >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "      >>> tf.reduce_sum(x).numpy()\n",
      "      6\n",
      "      >>> # reduce along the first dimension\n",
      "      >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> tf.reduce_sum(x, 0).numpy()\n",
      "      array([2, 2, 2], dtype=int32)\n",
      "      >>> # reduce along the second dimension\n",
      "      >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "      >>> tf.reduce_sum(x, 1).numpy()\n",
      "      array([3, 3], dtype=int32)\n",
      "      >>> # keep the original dimensions\n",
      "      >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "      array([[3],\n",
      "             [3]], dtype=int32)\n",
      "      >>> # reduce along both dimensions\n",
      "      >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "      >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "      >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> # 2 + 2 + 2 = 6\n",
      "      >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "      6\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10.]\n",
      " [12.]\n",
      " [13.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]], shape=(6, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按行求和\n",
    "row_sum = tf.reduce_sum(R_tf, 1, keepdims=True)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16. 14. 11. 10. 16.]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按列求和\n",
    "column_sum = tf.reduce_sum(R_tf, 0, keepdims=True)\n",
    "print(column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(67.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对所有元素求总和\n",
    "\n",
    "total_sum = tf.reduce_sum(tf.reduce_sum(R_tf, 0))\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor指定位置的值\n",
    "\n",
    "1. 重点-参考网址<https://blog.csdn.net/qq_34418352/article/details/106399327>\n",
    "2. tensor本身不能直接修改指定位置的值，需要转化为Variable之后，配合assign（动词，分配的意思）一起来完成这个操作。\n",
    "3. 使用的是tf.Variable.assign和tf.Variable.assign_add来进行修改。也就是说支队tensorflow variable类型才能使用。<https://www.jianshu.com/p/efcb86940896>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6, 5) dtype=float32, numpy=\n",
      "array([[16.,  0.,  1.,  0., 25.],\n",
      "       [ 1., 99.,  1.,  9., 25.],\n",
      "       [16., 25.,  9.,  1.,  0.],\n",
      "       [ 4.,  9.,  0.,  4., 25.],\n",
      "       [25.,  1., 16.,  0.,  0.],\n",
      "       [ 0.,  9.,  4., 16.,  1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "R_tf_square_variable = tf.Variable(R_tf_square)\n",
    "R_tf_square_variable[1, 1].assign(99)\n",
    "print(R_tf_square_variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第二种方法\n",
    "\n",
    "1. 这种方法也太不直接了。不建议使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1. 99.  1.  9. 25.]\n",
      " [16. 25. 44.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def ModifyTensor(input_tensor, position=None, value=None):\n",
    "    input_tensor = input_tensor.numpy()\n",
    "    input_tensor[tuple(position)] = value\n",
    "    return input_tensor\n",
    "# new_tensor\n",
    "R_tf_square_variable = tf.py_function(ModifyTensor, inp=[R_tf_square_variable, [2,2], 44], \n",
    "                            Tout=R_tf_square_variable.dtype)\n",
    "print(type(R_tf_square_variable))\n",
    "print(R_tf_square_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变量的定义及使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a + 2) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义矩阵变量\n",
    "3. 修改特定位置的值\n",
    "4. 修改一行值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.5850269 , 0.9802736 , 0.5136742 , 0.6372984 , 0.7807223 ],\n",
      "       [0.25877562, 0.40871054, 0.04242261, 0.23912692, 0.9555652 ],\n",
      "       [0.8883501 , 0.36233076, 0.61173517, 0.75619566, 0.20715494]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.5850269 , 0.9802736 , 0.5136742 , 0.6372984 , 0.7807223 ],\n",
      "       [0.25877562, 5.        , 0.04242261, 0.23912692, 0.9555652 ],\n",
      "       [0.8883501 , 0.36233076, 0.61173517, 0.75619566, 0.20715494]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.5850269 , 0.9802736 , 0.5136742 , 0.6372984 , 0.7807223 ],\n",
      "       [0.25877562, 5.        , 0.04242261, 0.23912692, 0.9555652 ],\n",
      "       [1.        , 2.        , 3.        , 4.        , 5.        ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)\n",
    "print(a)\n",
    "a[1, 1].assign(5)\n",
    "print(a)\n",
    "a[2, :].assign(tf.constant([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. tf.Variable.assign_add的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a.assign_add(2)) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 循环访问tensor变量中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display P[i, :].shape=(5,) and P[i, :]=[0.6027672  0.58806735 0.3175965  0.97791225 0.05595433]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.95088804 0.7635493  0.02096015 0.8311248  0.6454362 ]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.19769007 0.30768836 0.58275825 0.8228489  0.2591434 ]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.6027672  0.95088804 0.19769007]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.58806735 0.7635493  0.30768836]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.3175965  0.02096015 0.58275825]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.97791225 0.8311248  0.8228489 ]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.05595433 0.6454362  0.2591434 ]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "K = 5\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"Display P[i, :].shape={} and P[i, :]={}\".format(P[i, :].shape, P[i, :]))\n",
    "\n",
    "for j in range(K):\n",
    "    print(\"Display P[:, j].shape={} and P[:, j]={}\".format(P[i, :].shape, P[:, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 修改指定行，不能用=来进行赋值。必须用assign来赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.6027672 , 0.58806735, 0.3175965 , 0.97791225, 0.05595433],\n",
       "       [1.5536553 , 1.3516166 , 0.33855665, 1.809037  , 0.70139056],\n",
       "       [0.19769007, 0.30768836, 0.58275825, 0.8228489 , 0.2591434 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,:].assign(P[1,:] + P[0,:])\n",
    "# P[1,:] = P[1,:] + P[0,:]\n",
    "# print(P[1,:])\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 修改指定列\n",
    "这个地方要与第9点进行比较，并没有出现将列拿出来单独计算时的问题！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.6027672 , 0.58806735, 0.3175965 , 0.97791225, 0.05595433],\n",
      "       [1.5536553 , 1.3516166 , 0.33855665, 1.809037  , 0.70139056],\n",
      "       [0.19769007, 0.30768836, 0.58275825, 0.8228489 , 0.2591434 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[1.1908345 , 0.58806735, 0.3175965 , 0.97791225, 0.05595433],\n",
      "       [2.905272  , 1.3516166 , 0.33855665, 1.809037  , 0.70139056],\n",
      "       [0.5053784 , 0.30768836, 0.58275825, 0.8228489 , 0.2591434 ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "\n",
    "P[:, 0].assign(P[:, 0] + P[:, 1])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **读取tensor矩阵中的一列时，直接读取出来的维度不对！需要使用reshape来调整维度**。\n",
    "\n",
    "在读取一个维度的时候，读取出来的是3个标量，而不是1*3的向量！！！在做乘法的时候表现出来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.8778042  0.39796147 0.8786582 ], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.8778042 ]\n",
      " [0.39796147]\n",
      " [0.8786582 ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "P = tf.Variable(np.random.rand(m, n), dtype=float)\n",
    "\n",
    "# 注意两者的维度不同！！！\n",
    "print(P[:,1])\n",
    "print(tf.reshape(P[:,1], [3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！！ c是一个2维的矩阵，只不过维度是[1,1]。这个和直接P[1,1]有区别！！！而且这两个值不能直接进行计算！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.7009537]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.7009537]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.matmul(tf.reshape(P[:,1], [1,m]), tf.reshape(P[:,1], [m,1]))\n",
    "print(c)\n",
    "\n",
    "d = c - tf.Variable(1.0)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这个报错！就不能将一个二维矩阵赋值给一个标量。\n",
    "但是按一般的理解P[1,1]取出来的值也应该是一个二维矩阵，但tensorflow做了严格的区分。\n",
    "只能通过c[0,0]来转换一下再进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.23728238, 0.8778042 , 0.17647153, 0.2783435 , 0.60681015],\n",
      "       [0.6326235 , 1.4       , 0.9818423 , 0.35244426, 0.48224324],\n",
      "       [0.70820564, 0.8786582 , 0.16330479, 0.44251525, 0.9727258 ]],\n",
      "      dtype=float32)> tf.Tensor([[1.7009537]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor(1.4, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P, c)\n",
    "print(P[1,1])\n",
    "# P[1,1].assign(c)\n",
    "# print(P, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将一个值赋值给矩阵中指定的位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.23728238, 0.8778042 , 0.17647153, 0.2783435 , 0.60681015],\n",
      "       [0.6326235 , 0.39796147, 0.9818423 , 0.35244426, 0.48224324],\n",
      "       [0.70820564, 0.8786582 , 0.16330479, 0.44251525, 0.9727258 ]],\n",
      "      dtype=float32)> 1.4\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.23728238, 0.8778042 , 0.17647153, 0.2783435 , 0.60681015],\n",
      "       [0.6326235 , 1.4       , 0.9818423 , 0.35244426, 0.48224324],\n",
      "       [0.70820564, 0.8786582 , 0.16330479, 0.44251525, 0.9727258 ]],\n",
      "      dtype=float32)> 1.4\n"
     ]
    }
   ],
   "source": [
    "d = 1.4\n",
    "print(P, d)\n",
    "P[1,1].assign(d)\n",
    "print(P, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.750307  , 0.64765006, 0.4337014 , 0.44570795, 0.55293304],\n",
       "       [0.44474578, 1.0878681 , 0.464176  , 0.48777348, 0.28537613],\n",
       "       [0.27321017, 0.12698379, 0.42727664, 0.30518657, 0.829223  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,1].assign(c[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. tensor之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. tensor与数值之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'float'>\n",
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = 1.0\n",
    "\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. tf.slice的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.06443966 0.68876046 0.886289   0.5814505  0.5782631 ]], shape=(1, 5), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "sr = tf.slice(a, [0,0], [1,5])\n",
    "print(sr)\n",
    "print(type(sr))\n",
    "print(sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.06443966]\n",
      " [0.05307393]\n",
      " [0.35451323]], shape=(3, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = tf.slice(a, [0,0], [3,1])\n",
    "print(sc)\n",
    "print(type(sc))\n",
    "print(sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片了之后无法赋值！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.06443966]], shape=(1, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 1)\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.06443966, 0.68876046, 0.886289  , 0.5814505 , 0.5782631 ],\n",
      "       [0.05307393, 0.84136057, 0.1802258 , 0.45127547, 0.82278544],\n",
      "       [0.35451323, 0.04036029, 0.9093625 , 0.3617856 , 0.62880754]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "so = tf.slice(a, [0,0], [1,1])\n",
    "print(so)\n",
    "print(type(so))\n",
    "print(so.shape)\n",
    "\n",
    "so = 44\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片和索引\n",
    "\n",
    "tensorflow对tensor的处理可以向numpy一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.9539905  -0.40281832  2.6023557  -0.10324871 -3.0646675 ]\n",
      " [-0.97238547  1.1560242  -1.7245569   1.1357219  -0.6476316 ]\n",
      " [ 2.2544699  -0.06456217  0.774       1.0327958  -1.1530969 ]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 5])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([ 2.2544699 , -0.06456217,  0.774     ,  1.0327958 , -1.1530969 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[-0.97238547,  1.1560242 , -1.7245569 ,  1.1357219 , -0.6476316 ],\n",
       "        [ 2.2544699 , -0.06456217,  0.774     ,  1.0327958 , -1.1530969 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[-1], P[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对矩阵中的多个元素进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [ 2.2544699 , -0.06456217,  0.774     ,  1.0327958 , -1.1530969 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(P)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor可以与数值比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if 1999 > eui_square:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看tensor矩阵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseResourceVariable.eval of <tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.750307  , 0.64765006, 0.4337014 , 0.44570795, 0.55293304],\n",
      "       [0.44474578, 1.0878681 , 0.464176  , 0.48777348, 0.28537613],\n",
      "       [0.27321017, 0.12698379, 0.42727664, 0.30518657, 0.829223  ]],\n",
      "      dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(P.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求向量的范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function norm_v2 in module tensorflow.python.ops.linalg_ops:\n",
      "\n",
      "norm_v2(tensor, ord='euclidean', axis=None, keepdims=None, name=None)\n",
      "    Computes the norm of vectors, matrices, and tensors.\n",
      "    \n",
      "    This function can compute several different vector norms (the 1-norm, the\n",
      "    Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\n",
      "    matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n",
      "    \n",
      "    Args:\n",
      "      tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n",
      "      ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n",
      "        `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n",
      "        p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n",
      "        `tensor` is a matrix and equivalent to 2-norm for vectors.\n",
      "        Some restrictions apply:\n",
      "          a) The Frobenius norm `'fro'` is not defined for vectors,\n",
      "          b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n",
      "             `2`, `np.inf` are supported.\n",
      "        See the description of `axis` on how to compute norms for a batch of\n",
      "        vectors or matrices stored in a tensor.\n",
      "      axis: If `axis` is `None` (the default), the input is considered a vector\n",
      "        and a single vector norm is computed over the entire set of values in the\n",
      "        tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n",
      "        `norm(reshape(tensor, [-1]), ord=ord)`.\n",
      "        If `axis` is a Python integer, the input is considered a batch of vectors,\n",
      "        and `axis` determines the axis in `tensor` over which to compute vector\n",
      "        norms.\n",
      "        If `axis` is a 2-tuple of Python integers it is considered a batch of\n",
      "        matrices and `axis` determines the axes in `tensor` over which to compute\n",
      "        a matrix norm.\n",
      "        Negative indices are supported. Example: If you are passing a tensor that\n",
      "        can be either a matrix or a batch of matrices at runtime, pass\n",
      "        `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n",
      "        computed.\n",
      "      keepdims: If True, the axis indicated in `axis` are kept with size 1.\n",
      "        Otherwise, the dimensions in `axis` are removed from the output shape.\n",
      "      name: The name of the op.\n",
      "    \n",
      "    Returns:\n",
      "      output: A `Tensor` of the same type as tensor, containing the vector or\n",
      "        matrix norms. If `keepdims` is True then the rank of output is equal to\n",
      "        the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n",
      "        if `axis` is an integer, the rank of `output` is one less than the rank\n",
      "        of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n",
      "        than the rank of `tensor`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `ord` or `axis` is invalid.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Mostly equivalent to numpy.linalg.norm.\n",
      "    Not supported: ord <= 0, 2-norm for matrices, nuclear norm.\n",
      "    Other differences:\n",
      "      a) If axis is `None`, treats the flattened `tensor` as a vector\n",
      "       regardless of rank.\n",
      "      b) Explicitly supports 'euclidean' norm as the default, including for\n",
      "       higher order tensors.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4. 0. 1. 0. 5.]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.4807405, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算的是向量的。\n",
    "row_vector = tf.reshape(R_tf[0, :], [1, -1])\n",
    "print(row_vector)\n",
    "\n",
    "L1 = tf.norm(row_vector, ord=1)\n",
    "print(L1)\n",
    "\n",
    "L2 = tf.norm(row_vector, ord=2)\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([16. 14. 11. 10. 16.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([6.4807405 6.3245554 7.141428  6.4807405 6.4807405 5.477226 ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵所有行或者所有列的范数。\n",
    "\n",
    "# 按列求L1范数\n",
    "L1_matrix = tf.norm(R_tf, ord=1, axis=0)\n",
    "print(L1_matrix)\n",
    "\n",
    "# 按行求L2范数\n",
    "L2_matrix = tf.norm(R_tf, ord=2, axis=1)\n",
    "print(L2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matrix_inverse in module tensorflow.python.ops.gen_linalg_ops:\n",
      "\n",
      "matrix_inverse(input, adjoint=False, name=None)\n",
      "    Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\n",
      "    \n",
      "    \n",
      "    The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\n",
      "    form square matrices. The output is a tensor of the same shape as the input\n",
      "    containing the inverse for all input submatrices `[..., :, :]`.\n",
      "    \n",
      "    The op uses LU decomposition with partial pivoting to compute the inverses.\n",
      "    \n",
      "    If a matrix is not invertible there is no guarantee what the op does. It\n",
      "    may detect the condition and raise an exception or it may simply return a\n",
      "    garbage result.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.\n",
      "        Shape is `[..., M, M]`.\n",
      "      adjoint: An optional `bool`. Defaults to `False`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.linalg.inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8799998  -0.8399998  -0.92        1.7199999   1.12      ]\n",
      " [ 0.39999998  0.19999999  0.6000001  -0.6        -0.6       ]\n",
      " [ 0.99999976  0.99999976  0.99999994 -1.9999998  -0.99999994]\n",
      " [-1.4799998  -0.6399999  -1.3200002   2.12        1.5200001 ]\n",
      " [ 0.7039999   0.47199994  0.536      -0.9759999  -0.696     ]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.linalg.inv(R_Square_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
      "    Transposes `a`, where `a` is a Tensor.\n",
      "    \n",
      "    Permutes the dimensions according to the value of `perm`.\n",
      "    \n",
      "    The returned tensor's dimension `i` will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
      "    of the input tensor. Hence by default, this operation performs a regular\n",
      "    matrix transpose on 2-D input Tensors.\n",
      "    \n",
      "    If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
      "    then the values of `a` are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.transpose(x)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    \n",
      "    Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
      "    \n",
      "    If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
      "    \n",
      "    >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "    ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    >>> tf.transpose(x, conjugate=True)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
      "    array([[1.-1.j, 4.-4.j],\n",
      "           [2.-2.j, 5.-5.j],\n",
      "           [3.-3.j, 6.-6.j]])>\n",
      "    \n",
      "    'perm' is more useful for n-dimensional tensors where n > 2:\n",
      "    \n",
      "    >>> x = tf.constant([[[ 1,  2,  3],\n",
      "    ...                   [ 4,  5,  6]],\n",
      "    ...                  [[ 7,  8,  9],\n",
      "    ...                   [10, 11, 12]]])\n",
      "    \n",
      "    As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
      "    \n",
      "    To take the transpose of the matrices in dimension-0 (such as when you are\n",
      "    transposing matrices where 0 is the batch dimension), you would set\n",
      "    `perm=[0,2,1]`.\n",
      "    \n",
      "    >>> tf.transpose(x, perm=[0, 2, 1])\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[ 1,  4],\n",
      "            [ 2,  5],\n",
      "            [ 3,  6]],\n",
      "            [[ 7, 10],\n",
      "            [ 8, 11],\n",
      "            [ 9, 12]]], dtype=int32)>\n",
      "    \n",
      "    Note: This has a shorthand `linalg.matrix_transpose`):\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 1. 4. 2. 5. 0.]\n",
      " [0. 2. 5. 3. 1. 3.]\n",
      " [1. 1. 3. 0. 4. 2.]\n",
      " [0. 3. 1. 2. 0. 4.]\n",
      " [5. 5. 0. 5. 0. 1.]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.transpose(R_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对一元方程求导。参考<https://blog.csdn.net/AwesomeP/article/details/123787448>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里定义了一个变量x，使用tf.variable()声明，与普通张量一样，该变量拥有形状、类型和值这3种属性。变量与普通张量的一个重要区别是，它默认能够被Tensorflow的自动求导机制求导，因此常用于定义机器模型的参数。\n",
    "\n",
    "补充知识:tf.Variable () 将变量标记为“可训练”，**被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # tf为2.*版本\n",
    "x = tf.Variable(initial_value = 3.0) # 定义变量x，初始化为3\n",
    "with tf.GradientTape() as tape: # 在tf.GradientTape()的上下文中，所有的计算步骤都会被记录，用以求导\n",
    "    y = tf.square(x) # y = x的平方\n",
    "y_grad = tape.gradient(y,x) # 计算y关于x的导数\n",
    "print(y)      # 输出3的平方  tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "print(y_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中使用了tf.Variable()定义x，下面我们展示使用tf.constant()定义x，注意两者之间求导时的不同，需要tape.watch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.constant(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    tape.watch(x) \n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对多元函数求偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_will_grad = tf.Variable(np.random.rand(3, 5), dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "理解：当两个向量的形状不符合向量（矩阵的试验不成功）计算的规则时\n",
    "1. 将两个向量分别按照二元计算的两个向量的形状进行扩展，变成为相同的形状。\n",
    "2. 然后将两个向量再进行二元计算。\n",
    "\n",
    "如下面的例子所示，将A只有一列，B只有一行。A按照B的行数进行复制，B按照A的列数进行复制，然后都形成$3 \\times 2$的矩阵在进行加法计算。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>中的说明如下：我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]])>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(3), (3, 1))\n",
    "B = tf.reshape(tf.range(2), (1, 2))\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.ones([2,2])\n",
    "D = tf.ones([3,3])\n",
    "C, D\n",
    "\n",
    "# 这个时候C和D不能使用广播机制来进行计算。\n",
    "# C + D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "这部分看到了代码但是还不理解作用。\n",
    "现在的理解是：定义一个变量，python给它分配了一块内存。经过一个运算之后，再次给这个变量赋值时，变量指向了另外一块地址。而原来定义变量时分配的内容没有释放。而tensorflow直接解决了这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. python的例子。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>说明如下：在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。**这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow的例子。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "Variables是TensorFlow中的可变容器，它们提供了一种存储模型参数的方法。 我们可以通过assign将一个操作的结果分配给一个Variable。 为了说明这一点，我们创建了一个与另一个张量Y相同的形状的Z， 使用zeros_like来分配一个全0的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1624794696\n",
      "id(Z): 1624794696\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n",
    "\n",
    "由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。\n",
    "\n",
    "但是，TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[-0.07695889,  2.9685268 ,  2.1961167 , 13.35896   ,  9.202129  ],\n",
       "       [ 4.1451774 , -1.911814  ,  5.847124  , -2.6691012 ,  3.7045562 ],\n",
       "       [ 2.2746692 ,  8.426576  , 10.312523  ,  0.99427396,  1.6280987 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n",
    "    A = X + Y  # 当不再需要时，分配将被复用\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变为其他类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "A = tf.ones([3,5])\n",
    "O = A.numpy()\n",
    "print(type(O), O)\n",
    "C = tf.constant(A)\n",
    "print(type(C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**：要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。\n",
    "对constant和variable都需要先转为numpy在进行提取标量的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.6], dtype=float32), 4.599999904632568, 4.599999904632568, 4)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([4.6]).numpy()\n",
    "B, B.item(), float(B), int(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4], dtype=float32), 4.400000095367432, 4.400000095367432, 4)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = tf.Variable([4.4]).numpy()\n",
    "V, V.item(), float(V), int(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "降维是通过按行或者按列进行求和/平均值等方式来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[ 1.376909  , -1.5879223 ,  0.49439245,  0.75640595, -1.2799418 ],\n",
       "        [-0.81265724, -0.3435352 , -1.8292148 , -0.36711973,  0.44913104],\n",
       "        [-1.2214979 , -1.9299084 , -1.0152807 , -0.5607647 ,  1.0992157 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=0表示按照列来进行求和。\n",
    "axis=1按照行来进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 3., 3., 3., 3.], dtype=float32)>,\n",
       " TensorShape([5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = tf.reduce_sum(Y, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>,\n",
       " TensorShape([3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = tf.reduce_sum(Y, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，可以指定降维的坐标轴。这里是二维的所以轴只有0和1。对于更高维的可以指定更多的坐标轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(Y, axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非降维求和\n",
    "\n",
    "使用的参数是keepdims=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = tf.reduce_sum(Y, axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按指定轴进行累积求和\n",
    "\n",
    "这种操作的详细说明：第一行的值是自己。第二行是第一行的值加上第二行的值。第三行是更新后第二行的值再加上第三行的值。以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵与向量之间的乘法\n",
    "\n",
    "要求$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$和$\\boldsymbol{x} \\in \\mathbb{R}^n$。注意A的列数和x的元素个数相同。得到的结果$\\boldsymbol{Ax} \\in \\mathbb{R}^{m \\times 1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 3]), TensorShape([1, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# B = tf.constant([[1],[2],[3]])\n",
    "B = tf.constant([[1,2,3]])\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[14, 32, 50]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "[相关的数学定义和说明详见](../../mathematics/LinearAlgebra.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算所需要的向量和矩阵\n",
    "import tensorflow as tf\n",
    "x = tf.constant([1,2,3,4,5])\n",
    "A = tf.random.normal([3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对向量计算$L_1$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对向量计算$L_2$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对矩阵计算$L_F$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
