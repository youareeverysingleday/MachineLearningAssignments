{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow基本操作\n",
    "\n",
    "1. 使用的tensorflow版本是2.8.0 \n",
    "2. 参考视频<https://www.bilibili.com/video/BV1Ub4y1e7P3?p=4>\n",
    "3. **重点-tensorflow几个最基本的操作参考**<https://zhuanlan.zhihu.com/p/377280469>\n",
    "4. **重点-tensorflow关于导数和线性代数的参考**<https://zh-v2.d2l.ai/>中第2章的内容。而且这部分内容非常简单易学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 6 * 5矩阵\n",
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "\n",
    "# 方阵\n",
    "R_Square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建指定大小的矩阵\n",
    "\n",
    "推荐使用第二种和第三种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建全零的tensor矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =5\n",
    "K = 4\n",
    "n = 7\n",
    "# 方法一\n",
    "P = tf.zeros([m, K], dtype=float)\n",
    "Q = tf.zeros([K, n], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建指定大小的tensor矩阵。**推荐**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 4) dtype=float32, numpy=\n",
      "array([[0.49916318, 0.7969988 , 0.17459068, 0.78337413],\n",
      "       [0.61681736, 0.75326604, 0.15732774, 0.85706615],\n",
      "       [0.37963682, 0.38678092, 0.11590412, 0.37901422],\n",
      "       [0.09480467, 0.5647535 , 0.87270737, 0.96465135],\n",
      "       [0.4027385 , 0.89151186, 0.40022704, 0.96593887]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **推荐**。使用tensorflow自己的函数创建随机产生的矩阵。而且矩阵中数值的分布满足每个元素都从均值为0、标准差为1的标准高斯分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.04316835 -0.16610618 -1.1671634  -0.7541653 ]\n",
      " [ 1.2491441   0.20424995  0.7165843  -0.20915909]\n",
      " [-1.0940877   2.14482     0.31099    -0.40084326]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 4])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 创建全1矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE = tf.ones(shape=[3,4])\n",
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建单位矩阵\n",
    "\n",
    "对角矩阵是方阵，一般记为$diag(a_1,a_2,\\cdots,a_n)$。\\\n",
    "单位矩阵是对角矩阵的特例，所有对角线上的元素全为1。\\\n",
    "tf可以创建不是方阵的在$i=j$的位置上的类似对角矩阵的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = tf.eye(3)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_other = tf.eye(3,5)\n",
    "E_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将np.array转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "R_Square_tf = tf.convert_to_tensor(R_Square, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的长度\n",
    "\n",
    "通过os.len()实际上获取的是矩阵或者向量第一个维度的值。\\\n",
    "\\\n",
    "对于向量获取的是元素个数。\\\n",
    "对于2维矩阵获取的是行数。\\\n",
    "对于3维矩阵获取的也是行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 0. 1. 0. 5.]\n",
      " [1. 2. 1. 3. 5.]\n",
      " [4. 5. 3. 1. 0.]\n",
      " [2. 3. 0. 2. 5.]\n",
      " [5. 1. 4. 0. 0.]\n",
      " [0. 3. 2. 4. 1.]], shape=(6, 5), dtype=float32)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(R_tf)\n",
    "print(len(R_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]], shape=(2, 3, 4), dtype=float32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "T = tf.ones([2,3,4])\n",
    "print(T)\n",
    "print(len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(R_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取tensor矩阵指定位置的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 2., 1., 3., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 1., 3., 0., 4., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor的形状\n",
    "\n",
    "1. 若有一个维度为-1，那么tensorflow会自动推导。\n",
    "2. tf.reshape(tensor, shape, name=None)\n",
    "    1. tensor：要改变维度（形状）的张量；\n",
    "    2. shape：希望变成什么维度；\n",
    "    3. name：操作的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "P = tf.reshape(R_tf[1, :], [1, -1])\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q = tf.reshape(R_tf[:,2], [-1, 1])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor之间的运算有两种\n",
    "1. 按元素的运算\n",
    "2. 按线性代数之间的预算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32) tf.Tensor(\n",
      "[[-1.1426829   0.7680505   0.8110923  -0.575984  ]\n",
      " [-0.46203265 -0.16829708  0.029613   -0.12245481]\n",
      " [ 1.1846809  -1.3347958  -1.8159322   0.32614648]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.ones([3, 4], dtype=float)\n",
    "Q = tf.random.normal(shape=[3, 4])\n",
    "print(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.14268291,  1.7680504 ,  1.8110924 ,  0.424016  ],\n",
       "       [ 0.5379673 ,  0.83170295,  1.029613  ,  0.8775452 ],\n",
       "       [ 2.184681  , -0.33479583, -0.81593215,  1.3261465 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P + Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 2.142683  ,  0.23194951,  0.18890768,  1.575984  ],\n",
       "       [ 1.4620327 ,  1.168297  ,  0.970387  ,  1.1224548 ],\n",
       "       [-0.18468094,  2.334796  ,  2.8159323 ,  0.6738535 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-1.1426829 ,  0.7680505 ,  0.8110923 , -0.575984  ],\n",
       "       [-0.46203265, -0.16829708,  0.029613  , -0.12245481],\n",
       "       [ 1.1846809 , -1.3347958 , -1.8159322 ,  0.32614648]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.8751334 ,  1.3019978 ,  1.2329053 , -1.7361593 ],\n",
       "       [-2.1643493 , -5.9418736 , 33.76896   , -8.166278  ],\n",
       "       [ 0.8441091 , -0.74917823, -0.55068135,  3.066107  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P / Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素求幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0.31896213, 2.1555598 , 2.2503648 , 0.56215143],\n",
       "       [0.6300018 , 0.8451028 , 1.0300559 , 0.8847459 ],\n",
       "       [3.2696433 , 0.2632119 , 0.16268618, 1.3856183 ]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素判断tensor比较大小、判断是否相等，返回的是每个位置的boolen型值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P == Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [ True, False, False, False]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P < Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [False,  True,  True,  True]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P > Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素对tensor所有元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[19.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui = tf.matmul(tf.reshape(R_Square_tf[2, :], [1, -1]),tf.reshape(R_Square_tf[:, 3], [-1, 1])) - R_Square_tf[2,3]\n",
    "print(eui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间数乘\n",
    "\n",
    "1. 计算的是矩阵每个对应位置元素相乘。\n",
    "2. 需要两个矩阵形状一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 8., 15.,  0.,  2.,  0.], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(R_Square_tf[2, :], R_Square_tf[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数乘以矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12.  0.  3.  0. 15.]\n",
      " [ 3.  6.  3.  9. 15.]\n",
      " [12. 15.  9.  3.  0.]\n",
      " [ 6.  9.  0.  6. 15.]\n",
      " [15.  3. 12.  0.  0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "print(a * R_Square_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor类型判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(eui, tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(eui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的类型转换\n",
    "\n",
    "1. 参考网页<https://wenku.baidu.com/view/9a3b8439fc00bed5b9f3f90f76c66137ef064f55.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些都是错误的，在tf 2.0之后都不支持了。\n",
    "\n",
    "# 将字符串转化为tf.float32（默认）和tf.int32\n",
    "# tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "\n",
    "# 转化为tf.float64\n",
    "# tf.to_double(eui, name='ToDouble')\n",
    "\n",
    "# 转化为tf.float32\n",
    "# tf.to_float(eui, name='ToFloat')\n",
    "\n",
    "# 转化为tf.int32\n",
    "# tf.to_int32(eui, name='ToInt32')\n",
    "\n",
    "# 转化为tf.int64\n",
    "# tf.to_int64(eui, name='ToInt64')\n",
    "\n",
    "# 转化为dtype指定的类型\n",
    "# tf.cast(x, dtype, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0. 1. 2. 3. 4.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.arange(5)\n",
    "print(x_0)\n",
    "x_1 = tf.convert_to_tensor(x_0, dtype=tf.int32)\n",
    "print(x_1)\n",
    "x_2 = tf.cast(x_1, dtype=tf.float32)\n",
    "print(x_2)\n",
    "x_3 = tf.cast(x_2, dtype=tf.int32)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵计算平方\n",
    "\n",
    "1. 计算的是每个元素的平方值\n",
    "2. tf.math还有这个类，下面应该有很多数学方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[361.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui_square = tf.square(eui)\n",
    "print(eui_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1.  4.  1.  9. 25.]\n",
      " [16. 25.  9.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "R_tf_square = tf.square(R_tf)\n",
    "print(R_tf_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算tensor所有行或者所有列的和\n",
    "\n",
    "1. 使用tf.reduce_sum来完成\n",
    "2. 建议都需要填写keepdims=True参数，用于保证tensor的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=67.0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(R_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    This is the reduction operation for the elementwise `tf.math.add` op.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    of the entries in `axis`, which must be unique. If `keepdims` is true, the\n",
      "    reduced dimensions are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "      >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "      >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "      >>> x.numpy()\n",
      "      array([[1, 1, 1],\n",
      "             [1, 1, 1]], dtype=int32)\n",
      "      >>> # sum all the elements\n",
      "      >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "      >>> tf.reduce_sum(x).numpy()\n",
      "      6\n",
      "      >>> # reduce along the first dimension\n",
      "      >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> tf.reduce_sum(x, 0).numpy()\n",
      "      array([2, 2, 2], dtype=int32)\n",
      "      >>> # reduce along the second dimension\n",
      "      >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "      >>> tf.reduce_sum(x, 1).numpy()\n",
      "      array([3, 3], dtype=int32)\n",
      "      >>> # keep the original dimensions\n",
      "      >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "      array([[3],\n",
      "             [3]], dtype=int32)\n",
      "      >>> # reduce along both dimensions\n",
      "      >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "      >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "      >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> # 2 + 2 + 2 = 6\n",
      "      >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "      6\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10.]\n",
      " [12.]\n",
      " [13.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]], shape=(6, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按行求和\n",
    "row_sum = tf.reduce_sum(R_tf, 1, keepdims=True)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16. 14. 11. 10. 16.]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按列求和\n",
    "column_sum = tf.reduce_sum(R_tf, 0, keepdims=True)\n",
    "print(column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(67.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对所有元素求总和\n",
    "\n",
    "total_sum = tf.reduce_sum(tf.reduce_sum(R_tf, 0))\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor指定位置的值\n",
    "\n",
    "1. 重点-参考网址<https://blog.csdn.net/qq_34418352/article/details/106399327>\n",
    "2. tensor本身不能直接修改指定位置的值，需要转化为Variable之后，配合assign（动词，分配的意思）一起来完成这个操作。\n",
    "3. 使用的是tf.Variable.assign和tf.Variable.assign_add来进行修改。也就是说支队tensorflow variable类型才能使用。<https://www.jianshu.com/p/efcb86940896>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6, 5) dtype=float32, numpy=\n",
      "array([[16.,  0.,  1.,  0., 25.],\n",
      "       [ 1., 99.,  1.,  9., 25.],\n",
      "       [16., 25.,  9.,  1.,  0.],\n",
      "       [ 4.,  9.,  0.,  4., 25.],\n",
      "       [25.,  1., 16.,  0.,  0.],\n",
      "       [ 0.,  9.,  4., 16.,  1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "R_tf_square_variable = tf.Variable(R_tf_square)\n",
    "R_tf_square_variable[1, 1].assign(99)\n",
    "print(R_tf_square_variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第二种方法\n",
    "\n",
    "1. 这种方法也太不直接了。不建议使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1. 99.  1.  9. 25.]\n",
      " [16. 25. 44.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def ModifyTensor(input_tensor, position=None, value=None):\n",
    "    input_tensor = input_tensor.numpy()\n",
    "    input_tensor[tuple(position)] = value\n",
    "    return input_tensor\n",
    "# new_tensor\n",
    "R_tf_square_variable = tf.py_function(ModifyTensor, inp=[R_tf_square_variable, [2,2], 44], \n",
    "                            Tout=R_tf_square_variable.dtype)\n",
    "print(type(R_tf_square_variable))\n",
    "print(R_tf_square_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变量的定义及使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a + 2) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义矩阵变量\n",
    "3. 修改特定位置的值\n",
    "4. 修改一行值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9208133 , 0.6979436 , 0.97693753, 0.34720075, 0.6390737 ],\n",
      "       [0.95835626, 0.09005744, 0.9184809 , 0.938954  , 0.88379335],\n",
      "       [0.14109929, 0.73994035, 0.85653913, 0.39300245, 0.02206565]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9208133 , 0.6979436 , 0.97693753, 0.34720075, 0.6390737 ],\n",
      "       [0.95835626, 5.        , 0.9184809 , 0.938954  , 0.88379335],\n",
      "       [0.14109929, 0.73994035, 0.85653913, 0.39300245, 0.02206565]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9208133 , 0.6979436 , 0.97693753, 0.34720075, 0.6390737 ],\n",
      "       [0.95835626, 5.        , 0.9184809 , 0.938954  , 0.88379335],\n",
      "       [1.        , 2.        , 3.        , 4.        , 5.        ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)\n",
    "print(a)\n",
    "a[1, 1].assign(5)\n",
    "print(a)\n",
    "a[2, :].assign(tf.constant([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. tf.Variable.assign_add的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a.assign_add(2)) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 循环访问tensor变量中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display P[i, :].shape=(5,) and P[i, :]=[0.8330593  0.5363534  0.35161617 0.9450023  0.8986846 ]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.34658846 0.12806404 0.75541556 0.28657988 0.46947244]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.39482537 0.7493379  0.04562725 0.20900919 0.9103988 ]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.8330593  0.34658846 0.39482537]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.5363534  0.12806404 0.7493379 ]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.35161617 0.75541556 0.04562725]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.9450023  0.28657988 0.20900919]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.8986846  0.46947244 0.9103988 ]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "K = 5\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"Display P[i, :].shape={} and P[i, :]={}\".format(P[i, :].shape, P[i, :]))\n",
    "\n",
    "for j in range(K):\n",
    "    print(\"Display P[:, j].shape={} and P[:, j]={}\".format(P[i, :].shape, P[:, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 修改指定行，不能用=来进行赋值。必须用assign来赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.8330593 , 0.5363534 , 0.35161617, 0.9450023 , 0.8986846 ],\n",
       "       [1.1796478 , 0.66441745, 1.1070317 , 1.2315822 , 1.368157  ],\n",
       "       [0.39482537, 0.7493379 , 0.04562725, 0.20900919, 0.9103988 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,:].assign(P[1,:] + P[0,:])\n",
    "# P[1,:] = P[1,:] + P[0,:]\n",
    "# print(P[1,:])\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 修改指定列\n",
    "这个地方要与第9点进行比较，并没有出现将列拿出来单独计算时的问题！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.8330593 , 0.5363534 , 0.35161617, 0.9450023 , 0.8986846 ],\n",
      "       [1.1796478 , 0.66441745, 1.1070317 , 1.2315822 , 1.368157  ],\n",
      "       [0.39482537, 0.7493379 , 0.04562725, 0.20900919, 0.9103988 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[1.3694127 , 0.5363534 , 0.35161617, 0.9450023 , 0.8986846 ],\n",
      "       [1.8440652 , 0.66441745, 1.1070317 , 1.2315822 , 1.368157  ],\n",
      "       [1.1441633 , 0.7493379 , 0.04562725, 0.20900919, 0.9103988 ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "\n",
    "P[:, 0].assign(P[:, 0] + P[:, 1])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **读取tensor矩阵中的一列时，直接读取出来的维度不对！需要使用reshape来调整维度**。\n",
    "\n",
    "在读取一个维度的时候，读取出来的是3个标量，而不是1*3的向量！！！在做乘法的时候表现出来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.9396742 0.9872342 0.3744547], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.9396742]\n",
      " [0.9872342]\n",
      " [0.3744547]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "P = tf.Variable(np.random.rand(m, n), dtype=float)\n",
    "\n",
    "# 注意两者的维度不同！！！\n",
    "print(P[:,1])\n",
    "print(tf.reshape(P[:,1], [3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！！ c是一个2维的矩阵，只不过维度是[1,1]。这个和直接P[1,1]有区别！！！而且这两个值不能直接进行计算！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.9978353]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.9978353]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.matmul(tf.reshape(P[:,1], [1,m]), tf.reshape(P[:,1], [m,1]))\n",
    "print(c)\n",
    "\n",
    "d = c - tf.Variable(1.0)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这个报错！就不能将一个二维矩阵赋值给一个标量。\n",
    "但是按一般的理解P[1,1]取出来的值也应该是一个二维矩阵，但tensorflow做了严格的区分。\n",
    "只能通过c[0,0]来转换一下再进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9454814 , 0.9396742 , 0.8047441 , 0.95595646, 0.05356204],\n",
      "       [0.9076742 , 0.9872342 , 0.06128765, 0.2918757 , 0.51428944],\n",
      "       [0.45993954, 0.3744547 , 0.22579852, 0.4914325 , 0.2983626 ]],\n",
      "      dtype=float32)> tf.Tensor([[1.9978353]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor(0.9872342, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P, c)\n",
    "print(P[1,1])\n",
    "# P[1,1].assign(c)\n",
    "# print(P, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将一个值赋值给矩阵中指定的位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9454814 , 0.9396742 , 0.8047441 , 0.95595646, 0.05356204],\n",
      "       [0.9076742 , 0.9872342 , 0.06128765, 0.2918757 , 0.51428944],\n",
      "       [0.45993954, 0.3744547 , 0.22579852, 0.4914325 , 0.2983626 ]],\n",
      "      dtype=float32)> 1.4\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9454814 , 0.9396742 , 0.8047441 , 0.95595646, 0.05356204],\n",
      "       [0.9076742 , 1.4       , 0.06128765, 0.2918757 , 0.51428944],\n",
      "       [0.45993954, 0.3744547 , 0.22579852, 0.4914325 , 0.2983626 ]],\n",
      "      dtype=float32)> 1.4\n"
     ]
    }
   ],
   "source": [
    "d = 1.4\n",
    "print(P, d)\n",
    "P[1,1].assign(d)\n",
    "print(P, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.9454814 , 0.9396742 , 0.8047441 , 0.95595646, 0.05356204],\n",
       "       [0.9076742 , 1.9978353 , 0.06128765, 0.2918757 , 0.51428944],\n",
       "       [0.45993954, 0.3744547 , 0.22579852, 0.4914325 , 0.2983626 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,1].assign(c[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. tensor之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. tensor与数值之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'float'>\n",
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = 1.0\n",
    "\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. tf.slice的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.2024943  0.73880816 0.8060177  0.73968226 0.5556808 ]], shape=(1, 5), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "sr = tf.slice(a, [0,0], [1,5])\n",
    "print(sr)\n",
    "print(type(sr))\n",
    "print(sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.2024943 ]\n",
      " [0.47977898]\n",
      " [0.38519588]], shape=(3, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = tf.slice(a, [0,0], [3,1])\n",
    "print(sc)\n",
    "print(type(sc))\n",
    "print(sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片了之后无法赋值！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.2024943]], shape=(1, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 1)\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.2024943 , 0.73880816, 0.8060177 , 0.73968226, 0.5556808 ],\n",
      "       [0.47977898, 0.92517334, 0.8731453 , 0.85707146, 0.45367923],\n",
      "       [0.38519588, 0.10640299, 0.6395433 , 0.8140577 , 0.68500656]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "so = tf.slice(a, [0,0], [1,1])\n",
    "print(so)\n",
    "print(type(so))\n",
    "print(so.shape)\n",
    "\n",
    "so = 44\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片和索引\n",
    "\n",
    "tensorflow对tensor的处理可以向numpy一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.1364086   0.35504657 -1.355262   -1.1461542   0.01607939]\n",
      " [-0.9848871  -1.6975371  -0.0937865   1.4891666   0.7412058 ]\n",
      " [-0.09960404 -1.5323157  -0.07304938 -1.6892858  -0.06962445]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 5])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([-0.09960404, -1.5323157 , -0.07304938, -1.6892858 , -0.06962445],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[-0.9848871 , -1.6975371 , -0.0937865 ,  1.4891666 ,  0.7412058 ],\n",
       "        [-0.09960404, -1.5323157 , -0.07304938, -1.6892858 , -0.06962445]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[-1], P[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对矩阵中的多个元素进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [-0.09960404, -1.5323157 , -0.07304938, -1.6892858 , -0.06962445]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(P)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor可以与数值比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if 1999 > eui_square:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看tensor矩阵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _EagerTensorBase.eval of <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
      "array([[ 2.1364086 ,  0.35504657, -1.355262  , -1.1461542 ,  0.01607939],\n",
      "       [-0.9848871 , -1.6975371 , -0.0937865 ,  1.4891666 ,  0.7412058 ],\n",
      "       [-0.09960404, -1.5323157 , -0.07304938, -1.6892858 , -0.06962445]],\n",
      "      dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(P.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求向量的范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function norm_v2 in module tensorflow.python.ops.linalg_ops:\n",
      "\n",
      "norm_v2(tensor, ord='euclidean', axis=None, keepdims=None, name=None)\n",
      "    Computes the norm of vectors, matrices, and tensors.\n",
      "    \n",
      "    This function can compute several different vector norms (the 1-norm, the\n",
      "    Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\n",
      "    matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n",
      "    \n",
      "    Args:\n",
      "      tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n",
      "      ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n",
      "        `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n",
      "        p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n",
      "        `tensor` is a matrix and equivalent to 2-norm for vectors.\n",
      "        Some restrictions apply:\n",
      "          a) The Frobenius norm `'fro'` is not defined for vectors,\n",
      "          b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n",
      "             `2`, `np.inf` are supported.\n",
      "        See the description of `axis` on how to compute norms for a batch of\n",
      "        vectors or matrices stored in a tensor.\n",
      "      axis: If `axis` is `None` (the default), the input is considered a vector\n",
      "        and a single vector norm is computed over the entire set of values in the\n",
      "        tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n",
      "        `norm(reshape(tensor, [-1]), ord=ord)`.\n",
      "        If `axis` is a Python integer, the input is considered a batch of vectors,\n",
      "        and `axis` determines the axis in `tensor` over which to compute vector\n",
      "        norms.\n",
      "        If `axis` is a 2-tuple of Python integers it is considered a batch of\n",
      "        matrices and `axis` determines the axes in `tensor` over which to compute\n",
      "        a matrix norm.\n",
      "        Negative indices are supported. Example: If you are passing a tensor that\n",
      "        can be either a matrix or a batch of matrices at runtime, pass\n",
      "        `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n",
      "        computed.\n",
      "      keepdims: If True, the axis indicated in `axis` are kept with size 1.\n",
      "        Otherwise, the dimensions in `axis` are removed from the output shape.\n",
      "      name: The name of the op.\n",
      "    \n",
      "    Returns:\n",
      "      output: A `Tensor` of the same type as tensor, containing the vector or\n",
      "        matrix norms. If `keepdims` is True then the rank of output is equal to\n",
      "        the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n",
      "        if `axis` is an integer, the rank of `output` is one less than the rank\n",
      "        of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n",
      "        than the rank of `tensor`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `ord` or `axis` is invalid.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Mostly equivalent to numpy.linalg.norm.\n",
      "    Not supported: ord <= 0, 2-norm for matrices, nuclear norm.\n",
      "    Other differences:\n",
      "      a) If axis is `None`, treats the flattened `tensor` as a vector\n",
      "       regardless of rank.\n",
      "      b) Explicitly supports 'euclidean' norm as the default, including for\n",
      "       higher order tensors.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4. 0. 1. 0. 5.]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.4807405, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算的是向量的。\n",
    "row_vector = tf.reshape(R_tf[0, :], [1, -1])\n",
    "print(row_vector)\n",
    "\n",
    "L1 = tf.norm(row_vector, ord=1)\n",
    "print(L1)\n",
    "\n",
    "L2 = tf.norm(row_vector, ord=2)\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([16. 14. 11. 10. 16.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([6.4807405 6.3245554 7.141428  6.4807405 6.4807405 5.477226 ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵所有行或者所有列的范数。\n",
    "\n",
    "# 按列求L1范数\n",
    "L1_matrix = tf.norm(R_tf, ord=1, axis=0)\n",
    "print(L1_matrix)\n",
    "\n",
    "# 按行求L2范数\n",
    "L2_matrix = tf.norm(R_tf, ord=2, axis=1)\n",
    "print(L2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matrix_inverse in module tensorflow.python.ops.gen_linalg_ops:\n",
      "\n",
      "matrix_inverse(input, adjoint=False, name=None)\n",
      "    Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\n",
      "    \n",
      "    \n",
      "    The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\n",
      "    form square matrices. The output is a tensor of the same shape as the input\n",
      "    containing the inverse for all input submatrices `[..., :, :]`.\n",
      "    \n",
      "    The op uses LU decomposition with partial pivoting to compute the inverses.\n",
      "    \n",
      "    If a matrix is not invertible there is no guarantee what the op does. It\n",
      "    may detect the condition and raise an exception or it may simply return a\n",
      "    garbage result.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.\n",
      "        Shape is `[..., M, M]`.\n",
      "      adjoint: An optional `bool`. Defaults to `False`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.linalg.inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8799998  -0.8399998  -0.92        1.7199999   1.12      ]\n",
      " [ 0.39999998  0.19999999  0.6000001  -0.6        -0.6       ]\n",
      " [ 0.99999976  0.99999976  0.99999994 -1.9999998  -0.99999994]\n",
      " [-1.4799998  -0.6399999  -1.3200002   2.12        1.5200001 ]\n",
      " [ 0.7039999   0.47199994  0.536      -0.9759999  -0.696     ]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.linalg.inv(R_Square_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
      "    Transposes `a`, where `a` is a Tensor.\n",
      "    \n",
      "    Permutes the dimensions according to the value of `perm`.\n",
      "    \n",
      "    The returned tensor's dimension `i` will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
      "    of the input tensor. Hence by default, this operation performs a regular\n",
      "    matrix transpose on 2-D input Tensors.\n",
      "    \n",
      "    If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
      "    then the values of `a` are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.transpose(x)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    \n",
      "    Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
      "    \n",
      "    If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
      "    \n",
      "    >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "    ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    >>> tf.transpose(x, conjugate=True)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
      "    array([[1.-1.j, 4.-4.j],\n",
      "           [2.-2.j, 5.-5.j],\n",
      "           [3.-3.j, 6.-6.j]])>\n",
      "    \n",
      "    'perm' is more useful for n-dimensional tensors where n > 2:\n",
      "    \n",
      "    >>> x = tf.constant([[[ 1,  2,  3],\n",
      "    ...                   [ 4,  5,  6]],\n",
      "    ...                  [[ 7,  8,  9],\n",
      "    ...                   [10, 11, 12]]])\n",
      "    \n",
      "    As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
      "    \n",
      "    To take the transpose of the matrices in dimension-0 (such as when you are\n",
      "    transposing matrices where 0 is the batch dimension), you would set\n",
      "    `perm=[0,2,1]`.\n",
      "    \n",
      "    >>> tf.transpose(x, perm=[0, 2, 1])\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[ 1,  4],\n",
      "            [ 2,  5],\n",
      "            [ 3,  6]],\n",
      "            [[ 7, 10],\n",
      "            [ 8, 11],\n",
      "            [ 9, 12]]], dtype=int32)>\n",
      "    \n",
      "    Note: This has a shorthand `linalg.matrix_transpose`):\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 1. 4. 2. 5. 0.]\n",
      " [0. 2. 5. 3. 1. 3.]\n",
      " [1. 1. 3. 0. 4. 2.]\n",
      " [0. 3. 1. 2. 0. 4.]\n",
      " [5. 5. 0. 5. 0. 1.]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.transpose(R_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求导和求偏导\n",
    "\n",
    "1. 在对标量求梯度时就相当于在求导了。\n",
    "2. 在对向量求梯度时就相当于在求偏导了。对多元函数求偏导，这个部分还没有理解如何通过向量或者矩阵来表示多元函数。**或者说向量中每一个元素就时多元函数中的多元**。\n",
    "3. 但是对于矩阵求导还没有理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对一元方程求导。参考<https://blog.csdn.net/AwesomeP/article/details/123787448>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里定义了一个变量x，使用tf.variable()声明，与普通张量一样，该变量拥有形状、类型和值这3种属性。变量与普通张量的一个重要区别是，它默认能够被Tensorflow的自动求导机制求导，因此常用于定义机器模型的参数。\n",
    "\n",
    "补充知识:tf.Variable () 将变量标记为“可训练”，**被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # tf为2.*版本\n",
    "x = tf.Variable(initial_value = 3.0) # 定义变量x，初始化为3\n",
    "with tf.GradientTape() as tape: # 在tf.GradientTape()的上下文中，所有的计算步骤都会被记录，用以求导\n",
    "    y = tf.square(x) # y = x的平方\n",
    "y_grad = tape.gradient(y,x) # 计算y关于x的导数\n",
    "print(y)      # 输出3的平方  tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "print(y_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中使用了tf.Variable()定义x，下面我们展示使用tf.constant()定义x，注意两者之间求导时的不同，需要tape.watch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.constant(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    tape.watch(x) \n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 求偏导\n",
    "\n",
    "按照教程里面的例子手动来一遍。\\\n",
    "给定两个向量$\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{1 \\times n}$，它们的点积（dot product）表示为$\\boldsymbol{x} \\boldsymbol{y}^T \\in \\mathbb{R}^{1 \\times 1}$。\\\n",
    "tf.GradientTape().gradient()计算的是梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 2. 3.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "print(x)\n",
    "x = tf.Variable(x)\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。接下来，**我们通过调用反向传播函数来自动计算y关于x每个分量的梯度**。\n",
    "\n",
    "所以下面计算出来的不是一个值，而是一个向量。梯度是对向量上每个元素求偏导数。\\\n",
    "也就是说完成了偏导和求梯度两个步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这段代码说明了对矩阵求偏导和函数求偏导差别很大。\\\n",
    "矩阵求导还不在线性代数最常用的10个公式里面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[0.14131767, 0.6090764 , 0.5801397 ],\n",
      "       [0.8204821 , 0.3736083 , 0.7771608 ],\n",
      "       [0.5528199 , 0.34281173, 0.94885653]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[5.6903067 6.9717417 6.7182155]\n",
      " [5.3120604 6.5934954 6.339969 ]\n",
      " [7.2733817 8.554817  8.3012905]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "matrix_will_grad = tf.Variable(np.random.rand(3, 3), dtype=float)\n",
    "print(matrix_will_grad)\n",
    "with tf.GradientTape() as t:\n",
    "    # y = 2 * tf.matmul(matrix_will_grad, matrix_will_grad)\n",
    "    y = 2 * tf.tensordot(matrix_will_grad, matrix_will_grad, axes=1)\n",
    "x_grad = t.gradient(y, matrix_will_grad)\n",
    "print(x_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False]])>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * matrix_will_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 实现链式法则\n",
    "\n",
    "Dive into deep learning 2.5.3中文字描述如下：\n",
    "“有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，我们希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "在这里，我们可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gradient(y, x) == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 求二阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于python流的梯度计算\n",
    "\n",
    "好处：**可以计算分段函数的导数或者梯度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    print(\"1 b\", b)\n",
    "    # 计算的是向量的L1范数，也就是求向量元素绝对值之和。\n",
    "    # 注意tf.norm只是求了b的L1范数，但是没有改变b的值。b的值是在b = b * 2中改变的。\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    print(\"2 b\", b)\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    print(\"3 c\", c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 1.5448906 ,  0.96358085,  0.14866216],\n",
      "       [ 0.5324363 ,  2.7335956 , -0.17174071],\n",
      "       [-0.64002603, -1.462468  ,  0.68632823]], dtype=float32)>\n",
      "(3, 3)\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "1 b tf.Tensor(\n",
      "[[ 3.0897813   1.9271617   0.29732433]\n",
      " [ 1.0648726   5.467191   -0.34348142]\n",
      " [-1.2800521  -2.924936    1.3726565 ]], shape=(3, 3), dtype=float32)\n",
      "2 b tf.Tensor(\n",
      "[[ 790.984     493.3534     76.11503 ]\n",
      " [ 272.6074   1399.601     -87.931244]\n",
      " [-327.69333  -748.7836    351.40005 ]], shape=(3, 3), dtype=float32)\n",
      "3 c tf.Tensor(\n",
      "[[ 790.984     493.3534     76.11503 ]\n",
      " [ 272.6074   1399.601     -87.931244]\n",
      " [-327.69333  -748.7836    351.40005 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[512. 512. 512.]\n",
      " [512. 512. 512.]\n",
      " [512. 512. 512.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# a的这种定义形式还是第一次看到。shape=()表示的含义还不清楚。\n",
    "# a = tf.Variable(tf.random.normal(shape=()))\n",
    "a = tf.Variable(tf.random.normal(shape=(3,3)))\n",
    "# a = tf.Variable(1)\n",
    "# a = tf.range(4, dtype=tf.float32)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(type(a))\n",
    "\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "print(d_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[512. 512. 512.]\n",
      " [512. 512. 512.]\n",
      " [512. 512. 512.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(d/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里d/a表示的就是由a到d所乘以的常数C的值。\n",
    "d_grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "理解：当两个向量的形状不符合向量（矩阵的试验不成功）计算的规则时\n",
    "1. 将两个向量分别按照二元计算的两个向量的形状进行扩展，变成为相同的形状。\n",
    "2. 然后将两个向量再进行二元计算。\n",
    "\n",
    "如下面的例子所示，将A只有一列，B只有一行。A按照B的行数进行复制，B按照A的列数进行复制，然后都形成$3 \\times 2$的矩阵在进行加法计算。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>中的说明如下：我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]])>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(3), (3, 1))\n",
    "B = tf.reshape(tf.range(2), (1, 2))\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.ones([2,2])\n",
    "D = tf.ones([3,3])\n",
    "C, D\n",
    "\n",
    "# 这个时候C和D不能使用广播机制来进行计算。\n",
    "# C + D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "这部分看到了代码但是还不理解作用。\n",
    "现在的理解是：定义一个变量，python给它分配了一块内存。经过一个运算之后，再次给这个变量赋值时，变量指向了另外一块地址。而原来定义变量时分配的内容没有释放。而tensorflow直接解决了这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. python的例子。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>说明如下：在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。**这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow的例子。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "Variables是TensorFlow中的可变容器，它们提供了一种存储模型参数的方法。 我们可以通过assign将一个操作的结果分配给一个Variable。 为了说明这一点，我们创建了一个与另一个张量Y相同的形状的Z， 使用zeros_like来分配一个全0的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1469066056\n",
      "id(Z): 1469066056\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n",
    "\n",
    "由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。\n",
    "\n",
    "但是，TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[-3.9564424 , -2.9073195 ,  1.201944  ,  4.5663815 , -0.6437767 ],\n",
       "       [-3.6116128 , 12.968735  ,  3.3742406 ,  7.800539  ,  0.6106877 ],\n",
       "       [ 9.814019  ,  1.5240061 ,  2.2290041 ,  0.62262857,  5.092563  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n",
    "    A = X + Y  # 当不再需要时，分配将被复用\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变为其他类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "A = tf.ones([3,5])\n",
    "O = A.numpy()\n",
    "print(type(O), O)\n",
    "C = tf.constant(A)\n",
    "print(type(C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**：要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。\n",
    "对constant和variable都需要先转为numpy在进行提取标量的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.6], dtype=float32), 4.599999904632568, 4.599999904632568, 4)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([4.6]).numpy()\n",
    "B, B.item(), float(B), int(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4], dtype=float32), 4.400000095367432, 4.400000095367432, 4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = tf.Variable([4.4]).numpy()\n",
    "V, V.item(), float(V), int(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "降维是通过按行或者按列进行求和/平均值等方式来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-0.55764234, -1.3727949 , -0.12950353, -0.85175216,  0.6165435 ],\n",
       "        [ 0.62362367,  0.39802516,  0.27504653,  0.5750814 ,  1.9288772 ],\n",
       "        [ 0.370116  ,  0.0739801 , -0.8985078 , -0.44087556,  0.7204611 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=0表示按照列来进行求和。\n",
    "axis=1按照行来进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 3., 3., 3., 3.], dtype=float32)>,\n",
       " TensorShape([5]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = tf.reduce_sum(Y, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>,\n",
       " TensorShape([3]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = tf.reduce_sum(Y, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，可以指定降维的坐标轴。这里是二维的所以轴只有0和1。对于更高维的可以指定更多的坐标轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(Y, axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非降维求和\n",
    "\n",
    "使用的参数是keepdims=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = tf.reduce_sum(Y, axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按指定轴进行累积求和\n",
    "\n",
    "这种操作的详细说明：第一行的值是自己。第二行是第一行的值加上第二行的值。第三行是更新后第二行的值再加上第三行的值。以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵与向量之间的乘法\n",
    "\n",
    "要求$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$和$\\boldsymbol{x} \\in \\mathbb{R}^n$。注意A的列数和x的元素个数相同。得到的结果$\\boldsymbol{Ax} \\in \\mathbb{R}^{m \\times 1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 3]), TensorShape([1, 3]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# B = tf.constant([[1],[2],[3]])\n",
    "B = tf.constant([[1,2,3]])\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[14, 32, 50]])>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "[相关的数学定义和说明详见](../../mathematics/LinearAlgebra.md)\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算所需要的向量和矩阵\n",
    "import tensorflow as tf\n",
    "x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "A = tf.random.normal([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对向量计算$L_1$范数，$L_1$为$||x||_1 = \\sum\\limits_{i=1}^n |x_i|$。也就是向量元素绝对值之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对向量计算$L_2$范数，$L_2$为$||x||_2 = \\sqrt{\\sum\\limits_{i=1}^n x_i^2}$。也就是向量元素平方之和再开根号。\n",
    "\n",
    "必须要求向量中元素不是int类型，也就是必须是float类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.4161983>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对矩阵计算$L_F$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.703996>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量之间点积（Dot Product）\n",
    "\n",
    "给定两个向量$\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^d$，它们的点积（dot product）$\\boldsymbol{x}^T \\boldsymbol{y}$（或$<\\boldsymbol{x}, \\boldsymbol{y}>$）是相同位置的按元素乘积的和：$\\sum\\limits_{i=1}^dx_iy_i$。\n",
    "\n",
    "感觉这个地方是不是这样表述更准确一些。$\\boldsymbol{x}^T \\in \\mathbb{R}^{n \\times 1}$，所以$\\boldsymbol{x}^T \\boldsymbol{y} \\in \\mathbb{R}^{n \\times n}$。它们的点积是不是应该表示为$\\boldsymbol{x} \\boldsymbol{y}^T \\in \\mathbb{R}^{1 \\times 1}$。这样是不是符合直觉一点呢？\n",
    "\n",
    "tf.tensordot有点特别，使用x_1和y_1计算时，与tf.reduce_sum(x_1 * y_1)的计算结果不一样。应该是和tf.tensordot的定义有关。而且特别的和axes这个参数有关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.3468587], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_1 = tf.random.normal([5,1])\n",
    "tf.tensordot(x_1, y_1, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.6711707>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_1 * y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([[1.0], [1.0], [1.0], [1.0]])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.tensordot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=24.0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_2 * y_2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
