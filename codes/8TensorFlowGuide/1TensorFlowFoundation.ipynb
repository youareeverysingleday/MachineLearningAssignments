{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow基本操作\n",
    "\n",
    "1. 使用的tensorflow版本是2.8.0 \n",
    "2. 参考视频<https://www.bilibili.com/video/BV1Ub4y1e7P3?p=4>\n",
    "3. **重点-tensorflow几个最基本的操作参考**<https://zhuanlan.zhihu.com/p/377280469>\n",
    "4. **重点-tensorflow关于导数和线性代数的参考**<https://zh-v2.d2l.ai/>中第2章的内容。而且这部分内容非常简单易学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 6 * 5矩阵\n",
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "\n",
    "# 方阵\n",
    "R_Square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建指定大小的矩阵\n",
    "\n",
    "推荐使用第二种和第三种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建全零的tensor矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =5\n",
    "K = 4\n",
    "n = 7\n",
    "# 方法一\n",
    "P = tf.zeros([m, K], dtype=float)\n",
    "Q = tf.zeros([K, n], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建指定大小的tensor矩阵。**推荐**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 4) dtype=float32, numpy=\n",
      "array([[0.01349655, 0.58297825, 0.01978612, 0.66976625],\n",
      "       [0.39606136, 0.31068736, 0.08329462, 0.53200436],\n",
      "       [0.8634768 , 0.67366105, 0.55502975, 0.45528156],\n",
      "       [0.1360803 , 0.98812103, 0.78562754, 0.8057012 ],\n",
      "       [0.6441219 , 0.94557893, 0.77365655, 0.83368546]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **推荐**。使用tensorflow自己的函数创建随机产生的矩阵。而且矩阵中数值的分布满足每个元素都从均值为0、标准差为1的标准高斯分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.56630117  0.5605701   2.0584826   0.35033432]\n",
      " [-0.11664142  0.27289727 -0.60019696 -1.2242912 ]\n",
      " [ 0.20981804  0.80932397  3.0300972   0.30548906]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 4])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 创建全1矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE = tf.ones(shape=[3,4])\n",
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建单位矩阵\n",
    "\n",
    "对角矩阵是方阵，一般记为$diag(a_1,a_2,\\cdots,a_n)$。\\\n",
    "单位矩阵是对角矩阵的特例，所有对角线上的元素全为1。\\\n",
    "tf可以创建不是方阵的在$i=j$的位置上的类似对角矩阵的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = tf.eye(3)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_other = tf.eye(3,5)\n",
    "E_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将np.array转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "R_Square_tf = tf.convert_to_tensor(R_Square, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的长度\n",
    "\n",
    "通过os.len()实际上获取的是矩阵或者向量第一个维度的值。\\\n",
    "\\\n",
    "对于向量获取的是元素个数。\\\n",
    "对于2维矩阵获取的是行数。\\\n",
    "对于3维矩阵获取的也是行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 0. 1. 0. 5.]\n",
      " [1. 2. 1. 3. 5.]\n",
      " [4. 5. 3. 1. 0.]\n",
      " [2. 3. 0. 2. 5.]\n",
      " [5. 1. 4. 0. 0.]\n",
      " [0. 3. 2. 4. 1.]], shape=(6, 5), dtype=float32)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(R_tf)\n",
    "print(len(R_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]], shape=(2, 3, 4), dtype=float32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "T = tf.ones([2,3,4])\n",
    "print(T)\n",
    "print(len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(R_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取tensor矩阵指定位置的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 2., 1., 3., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 1., 3., 0., 4., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor的形状\n",
    "\n",
    "1. 若有一个维度为-1，那么tensorflow会自动推导。\n",
    "2. tf.reshape(tensor, shape, name=None)\n",
    "    1. tensor：要改变维度（形状）的张量；\n",
    "    2. shape：希望变成什么维度；\n",
    "    3. name：操作的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "P = tf.reshape(R_tf[1, :], [1, -1])\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q = tf.reshape(R_tf[:,2], [-1, 1])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[ 2. ],\n",
       "       [-3.4]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O = tf.reshape([2, -3.4], (-1, 1))\n",
    "O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor之间的运算有两种\n",
    "1. 按元素的运算\n",
    "2. 按线性代数之间的预算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32) tf.Tensor(\n",
      "[[ 1.1369019  -0.22495075  0.15675488 -1.7778722 ]\n",
      " [-0.10768849  0.2974442   1.5016825   0.21784396]\n",
      " [-1.9667642   1.2029815  -1.8759325  -1.5181731 ]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.ones([3, 4], dtype=float)\n",
    "Q = tf.random.normal(shape=[3, 4])\n",
    "print(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 2.1369019 ,  0.77504927,  1.1567549 , -0.7778722 ],\n",
       "       [ 0.8923115 ,  1.2974442 ,  2.5016825 ,  1.217844  ],\n",
       "       [-0.9667642 ,  2.2029815 , -0.87593246, -0.5181731 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P + Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.13690186,  1.2249508 ,  0.84324515,  2.777872  ],\n",
       "       [ 1.1076884 ,  0.7025558 , -0.5016825 ,  0.78215605],\n",
       "       [ 2.9667642 , -0.20298147,  2.8759325 ,  2.5181732 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 1.1369019 , -0.22495075,  0.15675488, -1.7778722 ],\n",
       "       [-0.10768849,  0.2974442 ,  1.5016825 ,  0.21784396],\n",
       "       [-1.9667642 ,  1.2029815 , -1.8759325 , -1.5181731 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.8795834 , -4.4454174 ,  6.379387  , -0.56247014],\n",
       "       [-9.286044  ,  3.3619752 ,  0.6659197 ,  4.5904417 ],\n",
       "       [-0.5084494 ,  0.831268  , -0.53306824, -0.6586864 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P / Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素求幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[3.1170962 , 0.79855555, 1.1697088 , 0.16899736],\n",
       "       [0.89790726, 1.3464133 , 4.489236  , 1.2433931 ],\n",
       "       [0.13990884, 3.3300304 , 0.15321204, 0.21911182]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素判断tensor比较大小、判断是否相等，返回的是每个位置的boolen型值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P == Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[ True, False, False, False],\n",
       "       [False, False,  True, False],\n",
       "       [False,  True, False, False]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P < Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False,  True,  True,  True],\n",
       "       [ True,  True, False,  True],\n",
       "       [ True, False,  True,  True]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P > Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素对tensor所有元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[19.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui = tf.matmul(tf.reshape(R_Square_tf[2, :], [1, -1]),tf.reshape(R_Square_tf[:, 3], [-1, 1])) - R_Square_tf[2,3]\n",
    "print(eui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间数乘\n",
    "\n",
    "1. 计算的是矩阵每个对应位置元素相乘。\n",
    "2. 需要两个矩阵形状一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 8., 15.,  0.,  2.,  0.], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(R_Square_tf[2, :], R_Square_tf[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数乘以矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12.  0.  3.  0. 15.]\n",
      " [ 3.  6.  3.  9. 15.]\n",
      " [12. 15.  9.  3.  0.]\n",
      " [ 6.  9.  0.  6. 15.]\n",
      " [15.  3. 12.  0.  0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "print(a * R_Square_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor类型判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(eui, tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(eui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的类型转换\n",
    "\n",
    "1. 参考网页<https://wenku.baidu.com/view/9a3b8439fc00bed5b9f3f90f76c66137ef064f55.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些都是错误的，在tf 2.0之后都不支持了。\n",
    "\n",
    "# 将字符串转化为tf.float32（默认）和tf.int32\n",
    "# tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "\n",
    "# 转化为tf.float64\n",
    "# tf.to_double(eui, name='ToDouble')\n",
    "\n",
    "# 转化为tf.float32\n",
    "# tf.to_float(eui, name='ToFloat')\n",
    "\n",
    "# 转化为tf.int32\n",
    "# tf.to_int32(eui, name='ToInt32')\n",
    "\n",
    "# 转化为tf.int64\n",
    "# tf.to_int64(eui, name='ToInt64')\n",
    "\n",
    "# 转化为dtype指定的类型\n",
    "# tf.cast(x, dtype, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0. 1. 2. 3. 4.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.arange(5)\n",
    "print(x_0)\n",
    "x_1 = tf.convert_to_tensor(x_0, dtype=tf.int32)\n",
    "print(x_1)\n",
    "x_2 = tf.cast(x_1, dtype=tf.float32)\n",
    "print(x_2)\n",
    "x_3 = tf.cast(x_2, dtype=tf.int32)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵计算平方\n",
    "\n",
    "1. 计算的是每个元素的平方值\n",
    "2. tf.math还有这个类，下面应该有很多数学方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[361.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui_square = tf.square(eui)\n",
    "print(eui_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1.  4.  1.  9. 25.]\n",
      " [16. 25.  9.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "R_tf_square = tf.square(R_tf)\n",
    "print(R_tf_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算tensor所有行或者所有列的和\n",
    "\n",
    "1. 使用tf.reduce_sum来完成\n",
    "2. 建议都需要填写keepdims=True参数，用于保证tensor的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=67.0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(R_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    This is the reduction operation for the elementwise `tf.math.add` op.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    of the entries in `axis`, which must be unique. If `keepdims` is true, the\n",
      "    reduced dimensions are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "      >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "      >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "      >>> x.numpy()\n",
      "      array([[1, 1, 1],\n",
      "             [1, 1, 1]], dtype=int32)\n",
      "      >>> # sum all the elements\n",
      "      >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "      >>> tf.reduce_sum(x).numpy()\n",
      "      6\n",
      "      >>> # reduce along the first dimension\n",
      "      >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> tf.reduce_sum(x, 0).numpy()\n",
      "      array([2, 2, 2], dtype=int32)\n",
      "      >>> # reduce along the second dimension\n",
      "      >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "      >>> tf.reduce_sum(x, 1).numpy()\n",
      "      array([3, 3], dtype=int32)\n",
      "      >>> # keep the original dimensions\n",
      "      >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "      array([[3],\n",
      "             [3]], dtype=int32)\n",
      "      >>> # reduce along both dimensions\n",
      "      >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "      >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "      >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> # 2 + 2 + 2 = 6\n",
      "      >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "      6\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10.]\n",
      " [12.]\n",
      " [13.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]], shape=(6, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按行求和\n",
    "row_sum = tf.reduce_sum(R_tf, 1, keepdims=True)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16. 14. 11. 10. 16.]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按列求和\n",
    "column_sum = tf.reduce_sum(R_tf, 0, keepdims=True)\n",
    "print(column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(67.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对所有元素求总和\n",
    "\n",
    "total_sum = tf.reduce_sum(tf.reduce_sum(R_tf, 0))\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor指定位置的值\n",
    "\n",
    "1. 重点-参考网址<https://blog.csdn.net/qq_34418352/article/details/106399327>\n",
    "2. tensor本身不能直接修改指定位置的值，需要转化为Variable之后，配合assign（动词，分配的意思）一起来完成这个操作。\n",
    "3. 使用的是tf.Variable.assign和tf.Variable.assign_add来进行修改。也就是说支队tensorflow variable类型才能使用。<https://www.jianshu.com/p/efcb86940896>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6, 5) dtype=float32, numpy=\n",
      "array([[16.,  0.,  1.,  0., 25.],\n",
      "       [ 1., 99.,  1.,  9., 25.],\n",
      "       [16., 25.,  9.,  1.,  0.],\n",
      "       [ 4.,  9.,  0.,  4., 25.],\n",
      "       [25.,  1., 16.,  0.,  0.],\n",
      "       [ 0.,  9.,  4., 16.,  1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "R_tf_square_variable = tf.Variable(R_tf_square)\n",
    "R_tf_square_variable[1, 1].assign(99)\n",
    "print(R_tf_square_variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第二种方法\n",
    "\n",
    "1. 这种方法也太不直接了。不建议使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1. 99.  1.  9. 25.]\n",
      " [16. 25. 44.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def ModifyTensor(input_tensor, position=None, value=None):\n",
    "    input_tensor = input_tensor.numpy()\n",
    "    input_tensor[tuple(position)] = value\n",
    "    return input_tensor\n",
    "# new_tensor\n",
    "R_tf_square_variable = tf.py_function(ModifyTensor, inp=[R_tf_square_variable, [2,2], 44], \n",
    "                            Tout=R_tf_square_variable.dtype)\n",
    "print(type(R_tf_square_variable))\n",
    "print(R_tf_square_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变量的定义及使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a + 2) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义矩阵变量\n",
    "3. 修改特定位置的值\n",
    "4. 修改一行值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.21978787, 0.23595503, 0.25877693, 0.77385706, 0.18389128],\n",
      "       [0.50294673, 0.5100356 , 0.6401777 , 0.95943785, 0.68273026],\n",
      "       [0.46769127, 0.15093099, 0.15645316, 0.15426117, 0.6435276 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.21978787, 0.23595503, 0.25877693, 0.77385706, 0.18389128],\n",
      "       [0.50294673, 5.        , 0.6401777 , 0.95943785, 0.68273026],\n",
      "       [0.46769127, 0.15093099, 0.15645316, 0.15426117, 0.6435276 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.21978787, 0.23595503, 0.25877693, 0.77385706, 0.18389128],\n",
      "       [0.50294673, 5.        , 0.6401777 , 0.95943785, 0.68273026],\n",
      "       [1.        , 2.        , 3.        , 4.        , 5.        ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)\n",
    "print(a)\n",
    "a[1, 1].assign(5)\n",
    "print(a)\n",
    "a[2, :].assign(tf.constant([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. tf.Variable.assign_add的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a.assign_add(2)) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 循环访问tensor变量中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display P[i, :].shape=(5,) and P[i, :]=[0.27632573 0.5896827  0.5645084  0.550275   0.3080767 ]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.5184518  0.526369   0.6338102  0.954697   0.05905344]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.12789334 0.56777567 0.83133197 0.6412353  0.62076956]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.27632573 0.5184518  0.12789334]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.5896827  0.526369   0.56777567]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.5645084  0.6338102  0.83133197]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.550275  0.954697  0.6412353]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.3080767  0.05905344 0.62076956]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "K = 5\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"Display P[i, :].shape={} and P[i, :]={}\".format(P[i, :].shape, P[i, :]))\n",
    "\n",
    "for j in range(K):\n",
    "    print(\"Display P[:, j].shape={} and P[:, j]={}\".format(P[i, :].shape, P[:, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 修改指定行，不能用=来进行赋值。必须用assign来赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.27632573, 0.5896827 , 0.5645084 , 0.550275  , 0.3080767 ],\n",
       "       [0.7947775 , 1.1160517 , 1.1983186 , 1.504972  , 0.36713016],\n",
       "       [0.12789334, 0.56777567, 0.83133197, 0.6412353 , 0.62076956]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,:].assign(P[1,:] + P[0,:])\n",
    "# P[1,:] = P[1,:] + P[0,:]\n",
    "# print(P[1,:])\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 修改指定列\n",
    "这个地方要与第9点进行比较，并没有出现将列拿出来单独计算时的问题！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.27632573, 0.5896827 , 0.5645084 , 0.550275  , 0.3080767 ],\n",
      "       [0.7947775 , 1.1160517 , 1.1983186 , 1.504972  , 0.36713016],\n",
      "       [0.12789334, 0.56777567, 0.83133197, 0.6412353 , 0.62076956]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.8660084 , 0.5896827 , 0.5645084 , 0.550275  , 0.3080767 ],\n",
      "       [1.9108292 , 1.1160517 , 1.1983186 , 1.504972  , 0.36713016],\n",
      "       [0.695669  , 0.56777567, 0.83133197, 0.6412353 , 0.62076956]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "\n",
    "P[:, 0].assign(P[:, 0] + P[:, 1])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **读取tensor矩阵中的一列时，直接读取出来的维度不对！需要使用reshape来调整维度**。\n",
    "\n",
    "在读取一个维度的时候，读取出来的是3个标量，而不是1*3的向量！！！在做乘法的时候表现出来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.28730702 0.8811223  0.63508797], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.28730702]\n",
      " [0.8811223 ]\n",
      " [0.63508797]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "P = tf.Variable(np.random.rand(m, n), dtype=float)\n",
    "\n",
    "# 注意两者的维度不同！！！\n",
    "print(P[:,1])\n",
    "print(tf.reshape(P[:,1], [3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！！ c是一个2维的矩阵，只不过维度是[1,1]。这个和直接P[1,1]有区别！！！而且这两个值不能直接进行计算！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.2622585]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.26225853]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.matmul(tf.reshape(P[:,1], [1,m]), tf.reshape(P[:,1], [m,1]))\n",
    "print(c)\n",
    "\n",
    "d = c - tf.Variable(1.0)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这个报错！就不能将一个二维矩阵赋值给一个标量。\n",
    "但是按一般的理解P[1,1]取出来的值也应该是一个二维矩阵，但tensorflow做了严格的区分。\n",
    "只能通过c[0,0]来转换一下再进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9784996 , 0.28730702, 0.3683818 , 0.5162962 , 0.29625553],\n",
      "       [0.83347034, 0.8811223 , 0.8406531 , 0.74810976, 0.8145793 ],\n",
      "       [0.8865295 , 0.63508797, 0.73778003, 0.5122837 , 0.546354  ]],\n",
      "      dtype=float32)> tf.Tensor([[1.2622585]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor(0.8811223, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P, c)\n",
    "print(P[1,1])\n",
    "# P[1,1].assign(c)\n",
    "# print(P, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将一个值赋值给矩阵中指定的位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9784996 , 0.28730702, 0.3683818 , 0.5162962 , 0.29625553],\n",
      "       [0.83347034, 0.8811223 , 0.8406531 , 0.74810976, 0.8145793 ],\n",
      "       [0.8865295 , 0.63508797, 0.73778003, 0.5122837 , 0.546354  ]],\n",
      "      dtype=float32)> 1.4\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.9784996 , 0.28730702, 0.3683818 , 0.5162962 , 0.29625553],\n",
      "       [0.83347034, 1.4       , 0.8406531 , 0.74810976, 0.8145793 ],\n",
      "       [0.8865295 , 0.63508797, 0.73778003, 0.5122837 , 0.546354  ]],\n",
      "      dtype=float32)> 1.4\n"
     ]
    }
   ],
   "source": [
    "d = 1.4\n",
    "print(P, d)\n",
    "P[1,1].assign(d)\n",
    "print(P, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.9784996 , 0.28730702, 0.3683818 , 0.5162962 , 0.29625553],\n",
       "       [0.83347034, 1.2622585 , 0.8406531 , 0.74810976, 0.8145793 ],\n",
       "       [0.8865295 , 0.63508797, 0.73778003, 0.5122837 , 0.546354  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,1].assign(c[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. tensor之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. tensor与数值之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'float'>\n",
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = 1.0\n",
    "\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. tf.slice的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.726005   0.63669276 0.8521151  0.6914843  0.65082204]], shape=(1, 5), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "sr = tf.slice(a, [0,0], [1,5])\n",
    "print(sr)\n",
    "print(type(sr))\n",
    "print(sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.726005  ]\n",
      " [0.38516757]\n",
      " [0.01539873]], shape=(3, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = tf.slice(a, [0,0], [3,1])\n",
    "print(sc)\n",
    "print(type(sc))\n",
    "print(sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片了之后无法赋值！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.726005]], shape=(1, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 1)\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.726005  , 0.63669276, 0.8521151 , 0.6914843 , 0.65082204],\n",
      "       [0.38516757, 0.53012574, 0.8761317 , 0.15448685, 0.6265644 ],\n",
      "       [0.01539873, 0.90016234, 0.80248106, 0.87793916, 0.44606802]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "so = tf.slice(a, [0,0], [1,1])\n",
    "print(so)\n",
    "print(type(so))\n",
    "print(so.shape)\n",
    "\n",
    "so = 44\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片和索引\n",
    "\n",
    "tensorflow对tensor的处理可以向numpy一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.4972901  -1.6616298   0.70684797 -1.8260847   2.0507994 ]\n",
      " [-1.1673863  -1.5195048   1.2709242  -0.04335115  0.6397501 ]\n",
      " [-0.80664235  0.2813131  -0.8387753  -0.49146476  0.08622979]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 5])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([-0.80664235,  0.2813131 , -0.8387753 , -0.49146476,  0.08622979],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[-1.1673863 , -1.5195048 ,  1.2709242 , -0.04335115,  0.6397501 ],\n",
       "        [-0.80664235,  0.2813131 , -0.8387753 , -0.49146476,  0.08622979]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[-1], P[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对矩阵中的多个元素进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [-0.80664235,  0.2813131 , -0.8387753 , -0.49146476,  0.08622979]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(P)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor可以与数值比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if 1999 > eui_square:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看tensor矩阵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _EagerTensorBase.eval of <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
      "array([[ 1.4972901 , -1.6616298 ,  0.70684797, -1.8260847 ,  2.0507994 ],\n",
      "       [-1.1673863 , -1.5195048 ,  1.2709242 , -0.04335115,  0.6397501 ],\n",
      "       [-0.80664235,  0.2813131 , -0.8387753 , -0.49146476,  0.08622979]],\n",
      "      dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(P.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求向量的范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function norm_v2 in module tensorflow.python.ops.linalg_ops:\n",
      "\n",
      "norm_v2(tensor, ord='euclidean', axis=None, keepdims=None, name=None)\n",
      "    Computes the norm of vectors, matrices, and tensors.\n",
      "    \n",
      "    This function can compute several different vector norms (the 1-norm, the\n",
      "    Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\n",
      "    matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n",
      "    \n",
      "    Args:\n",
      "      tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n",
      "      ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n",
      "        `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n",
      "        p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n",
      "        `tensor` is a matrix and equivalent to 2-norm for vectors.\n",
      "        Some restrictions apply:\n",
      "          a) The Frobenius norm `'fro'` is not defined for vectors,\n",
      "          b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n",
      "             `2`, `np.inf` are supported.\n",
      "        See the description of `axis` on how to compute norms for a batch of\n",
      "        vectors or matrices stored in a tensor.\n",
      "      axis: If `axis` is `None` (the default), the input is considered a vector\n",
      "        and a single vector norm is computed over the entire set of values in the\n",
      "        tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n",
      "        `norm(reshape(tensor, [-1]), ord=ord)`.\n",
      "        If `axis` is a Python integer, the input is considered a batch of vectors,\n",
      "        and `axis` determines the axis in `tensor` over which to compute vector\n",
      "        norms.\n",
      "        If `axis` is a 2-tuple of Python integers it is considered a batch of\n",
      "        matrices and `axis` determines the axes in `tensor` over which to compute\n",
      "        a matrix norm.\n",
      "        Negative indices are supported. Example: If you are passing a tensor that\n",
      "        can be either a matrix or a batch of matrices at runtime, pass\n",
      "        `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n",
      "        computed.\n",
      "      keepdims: If True, the axis indicated in `axis` are kept with size 1.\n",
      "        Otherwise, the dimensions in `axis` are removed from the output shape.\n",
      "      name: The name of the op.\n",
      "    \n",
      "    Returns:\n",
      "      output: A `Tensor` of the same type as tensor, containing the vector or\n",
      "        matrix norms. If `keepdims` is True then the rank of output is equal to\n",
      "        the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n",
      "        if `axis` is an integer, the rank of `output` is one less than the rank\n",
      "        of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n",
      "        than the rank of `tensor`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `ord` or `axis` is invalid.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Mostly equivalent to numpy.linalg.norm.\n",
      "    Not supported: ord <= 0, 2-norm for matrices, nuclear norm.\n",
      "    Other differences:\n",
      "      a) If axis is `None`, treats the flattened `tensor` as a vector\n",
      "       regardless of rank.\n",
      "      b) Explicitly supports 'euclidean' norm as the default, including for\n",
      "       higher order tensors.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4. 0. 1. 0. 5.]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.4807405, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算的是向量的。\n",
    "row_vector = tf.reshape(R_tf[0, :], [1, -1])\n",
    "print(row_vector)\n",
    "\n",
    "L1 = tf.norm(row_vector, ord=1)\n",
    "print(L1)\n",
    "\n",
    "L2 = tf.norm(row_vector, ord=2)\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([16. 14. 11. 10. 16.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([6.4807405 6.3245554 7.141428  6.4807405 6.4807405 5.477226 ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵所有行或者所有列的范数。\n",
    "\n",
    "# 按列求L1范数\n",
    "L1_matrix = tf.norm(R_tf, ord=1, axis=0)\n",
    "print(L1_matrix)\n",
    "\n",
    "# 按行求L2范数\n",
    "L2_matrix = tf.norm(R_tf, ord=2, axis=1)\n",
    "print(L2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matrix_inverse in module tensorflow.python.ops.gen_linalg_ops:\n",
      "\n",
      "matrix_inverse(input, adjoint=False, name=None)\n",
      "    Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\n",
      "    \n",
      "    \n",
      "    The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\n",
      "    form square matrices. The output is a tensor of the same shape as the input\n",
      "    containing the inverse for all input submatrices `[..., :, :]`.\n",
      "    \n",
      "    The op uses LU decomposition with partial pivoting to compute the inverses.\n",
      "    \n",
      "    If a matrix is not invertible there is no guarantee what the op does. It\n",
      "    may detect the condition and raise an exception or it may simply return a\n",
      "    garbage result.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.\n",
      "        Shape is `[..., M, M]`.\n",
      "      adjoint: An optional `bool`. Defaults to `False`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.linalg.inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8799998  -0.8399998  -0.92        1.7199999   1.12      ]\n",
      " [ 0.39999998  0.19999999  0.6000001  -0.6        -0.6       ]\n",
      " [ 0.99999976  0.99999976  0.99999994 -1.9999998  -0.99999994]\n",
      " [-1.4799998  -0.6399999  -1.3200002   2.12        1.5200001 ]\n",
      " [ 0.7039999   0.47199994  0.536      -0.9759999  -0.696     ]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.linalg.inv(R_Square_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
      "    Transposes `a`, where `a` is a Tensor.\n",
      "    \n",
      "    Permutes the dimensions according to the value of `perm`.\n",
      "    \n",
      "    The returned tensor's dimension `i` will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
      "    of the input tensor. Hence by default, this operation performs a regular\n",
      "    matrix transpose on 2-D input Tensors.\n",
      "    \n",
      "    If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
      "    then the values of `a` are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.transpose(x)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    \n",
      "    Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
      "    \n",
      "    If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
      "    \n",
      "    >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "    ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    >>> tf.transpose(x, conjugate=True)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
      "    array([[1.-1.j, 4.-4.j],\n",
      "           [2.-2.j, 5.-5.j],\n",
      "           [3.-3.j, 6.-6.j]])>\n",
      "    \n",
      "    'perm' is more useful for n-dimensional tensors where n > 2:\n",
      "    \n",
      "    >>> x = tf.constant([[[ 1,  2,  3],\n",
      "    ...                   [ 4,  5,  6]],\n",
      "    ...                  [[ 7,  8,  9],\n",
      "    ...                   [10, 11, 12]]])\n",
      "    \n",
      "    As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
      "    \n",
      "    To take the transpose of the matrices in dimension-0 (such as when you are\n",
      "    transposing matrices where 0 is the batch dimension), you would set\n",
      "    `perm=[0,2,1]`.\n",
      "    \n",
      "    >>> tf.transpose(x, perm=[0, 2, 1])\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[ 1,  4],\n",
      "            [ 2,  5],\n",
      "            [ 3,  6]],\n",
      "            [[ 7, 10],\n",
      "            [ 8, 11],\n",
      "            [ 9, 12]]], dtype=int32)>\n",
      "    \n",
      "    Note: This has a shorthand `linalg.matrix_transpose`):\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 1. 4. 2. 5. 0.]\n",
      " [0. 2. 5. 3. 1. 3.]\n",
      " [1. 1. 3. 0. 4. 2.]\n",
      " [0. 3. 1. 2. 0. 4.]\n",
      " [5. 5. 0. 5. 0. 1.]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.transpose(R_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求导和求偏导\n",
    "\n",
    "1. 在对标量求梯度时就相当于在求导了。\n",
    "2. 在对向量求梯度时就相当于在求偏导了。对多元函数求偏导，这个部分还没有理解如何通过向量或者矩阵来表示多元函数。**或者说向量中每一个元素就时多元函数中的多元**。\n",
    "3. 但是对于矩阵求导还没有理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对一元方程求导。参考<https://blog.csdn.net/AwesomeP/article/details/123787448>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里定义了一个变量x，使用tf.variable()声明，与普通张量一样，该变量拥有形状、类型和值这3种属性。变量与普通张量的一个重要区别是，它默认能够被Tensorflow的自动求导机制求导，因此常用于定义机器模型的参数。\n",
    "\n",
    "补充知识:tf.Variable () 将变量标记为“可训练”，**被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # tf为2.*版本\n",
    "x = tf.Variable(initial_value = 3.0) # 定义变量x，初始化为3\n",
    "with tf.GradientTape() as tape: # 在tf.GradientTape()的上下文中，所有的计算步骤都会被记录，用以求导\n",
    "    y = tf.square(x) # y = x的平方\n",
    "y_grad = tape.gradient(y,x) # 计算y关于x的导数\n",
    "print(y)      # 输出3的平方  tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "print(y_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中使用了tf.Variable()定义x，下面我们展示使用tf.constant()定义x，注意两者之间求导时的不同，需要tape.watch(x)\n",
    "\n",
    "默认情况下GradientTape的资源在调用gradient函数后就被释放，再次调用就无法计算了。**所以如果需要多次计算梯度**，需要开启persistent=True属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.constant(3.0)\n",
    "# 注意使用persistent=True。需要多次计算梯度的时候就需要使用这个参数。\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x) \n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.Variable(3.0)\n",
    "# 注意使用persistent=True。需要多次计算梯度的时候就需要使用这个参数。\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 求偏导\n",
    "\n",
    "按照教程里面的例子手动来一遍。\\\n",
    "给定两个向量$\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{1 \\times n}$，它们的点积（dot product）表示为$\\boldsymbol{x} \\boldsymbol{y}^T \\in \\mathbb{R}^{1 \\times 1}$。\n",
    "\n",
    "也就是$\\boldsymbol{x} = \\{x_1,x_2,\\cdots, x_n\\}, \\boldsymbol{y}= \\{y_1,y_2,\\cdots, y_n\\}, x\\cdot y \\coloneqq x_1y_1 + x_2y_2 + \\cdots + x_ny_n$\n",
    "\n",
    "tf.GradientTape().gradient()计算的是梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 2. 3.], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "print(x)\n",
    "x = tf.Variable(x)\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。接下来，**我们通过调用反向传播函数来自动计算y关于x每个分量的梯度**。\n",
    "\n",
    "所以下面计算出来的不是一个值，而是一个向量。梯度是对向量上每个元素求偏导数。\\\n",
    "也就是说完成了偏导和求梯度两个步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这段代码说明了对矩阵求偏导和函数求偏导差别很大。\n",
    "\n",
    "矩阵求导还不在线性代数最常用的10个公式里面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[0.71304387, 0.6407354 , 0.37049818],\n",
      "       [0.5417524 , 0.6280546 , 0.17211482],\n",
      "       [0.65342075, 0.40862086, 0.29642776]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[7.264989  6.5002775 6.533373 ]\n",
      " [6.8033767 6.0386653 6.07176  ]\n",
      " [5.1266365 4.361925  4.3950205]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "matrix_will_grad = tf.Variable(np.random.rand(3, 3), dtype=float)\n",
    "print(matrix_will_grad)\n",
    "with tf.GradientTape() as t:\n",
    "    # y = 2 * tf.matmul(matrix_will_grad, matrix_will_grad)\n",
    "    y = 2 * tf.tensordot(matrix_will_grad, matrix_will_grad, axes=1)\n",
    "x_grad = t.gradient(y, matrix_will_grad)\n",
    "print(x_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False]])>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * matrix_will_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 实现链式法则\n",
    "\n",
    "Dive into deep learning 2.5.3中文字描述如下：\n",
    "“有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，我们希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "在这里，我们可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gradient(y, x) == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 对两个参数分别求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(150.0, shape=(), dtype=float32)\n",
      "tf.Tensor(90.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(3.0)\n",
    "b = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    c = a * a * b * b\n",
    "\n",
    "da, db = tape.gradient(c, [a,b])\n",
    "print(da)\n",
    "print(db)\n",
    "# print(c_grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 求二阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于python流的梯度计算\n",
    "\n",
    "好处：**可以计算分段函数的导数或者梯度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    print(\"1 b\", b)\n",
    "    # 计算的是向量的L1范数，也就是求向量元素绝对值之和。\n",
    "    # 注意tf.norm只是求了b的L1范数，但是没有改变b的值。b的值是在b = b * 2中改变的。\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    print(\"2 b\", b)\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    print(\"3 c\", c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 0.30392563, -0.03773735, -1.0085843 ],\n",
      "       [ 0.1575343 ,  0.9578781 , -1.9102858 ],\n",
      "       [-0.02082451,  0.96601367, -1.08276   ]], dtype=float32)>\n",
      "(3, 3)\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "1 b tf.Tensor(\n",
      "[[ 0.60785127 -0.07547469 -2.0171685 ]\n",
      " [ 0.3150686   1.9157562  -3.8205717 ]\n",
      " [-0.04164901  1.9320273  -2.16552   ]], shape=(3, 3), dtype=float32)\n",
      "2 b tf.Tensor(\n",
      "[[ 155.60992    -19.321522  -516.39514  ]\n",
      " [  80.65756    490.4336    -978.06635  ]\n",
      " [ -10.6621475  494.599     -554.3731   ]], shape=(3, 3), dtype=float32)\n",
      "3 c tf.Tensor(\n",
      "[[ 15560.992   -1932.1522 -51639.516 ]\n",
      " [  8065.7563  49043.36   -97806.63  ]\n",
      " [ -1066.2147  49459.9    -55437.312 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[51200. 51200. 51200.]\n",
      " [51200. 51200. 51200.]\n",
      " [51200. 51200. 51200.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# a的这种定义形式还是第一次看到。shape=()表示的含义还不清楚。\n",
    "# a = tf.Variable(tf.random.normal(shape=()))\n",
    "a = tf.Variable(tf.random.normal(shape=(3,3)))\n",
    "# a = tf.Variable(1)\n",
    "# a = tf.range(4, dtype=tf.float32)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(type(a))\n",
    "\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "print(d_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[51200. 51200. 51200.]\n",
      " [51200. 51200. 51200.]\n",
      " [51200. 51200. 51200.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(d/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里d/a表示的就是由a到d所乘以的常数C的值。\n",
    "d_grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "理解：当两个向量的形状不符合向量（矩阵的试验不成功）计算的规则时\n",
    "1. 将两个向量分别按照二元计算的两个向量的形状进行扩展，变成为相同的形状。\n",
    "2. 然后将两个向量再进行二元计算。\n",
    "\n",
    "如下面的例子所示，将A只有一列，B只有一行。A按照B的行数进行复制，B按照A的列数进行复制，然后都形成$3 \\times 2$的矩阵在进行加法计算。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>中的说明如下：我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]])>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(3), (3, 1))\n",
    "B = tf.reshape(tf.range(2), (1, 2))\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.ones([2,2])\n",
    "D = tf.ones([3,3])\n",
    "C, D\n",
    "\n",
    "# 这个时候C和D不能使用广播机制来进行计算。\n",
    "# C + D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "这部分看到了代码但是还不理解作用。\n",
    "现在的理解是：定义一个变量，python给它分配了一块内存。经过一个运算之后，再次给这个变量赋值时，变量指向了另外一块地址。而原来定义变量时分配的内容没有释放。而tensorflow直接解决了这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. python的例子。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>说明如下：在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。**这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow的例子。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "Variables是TensorFlow中的可变容器，它们提供了一种存储模型参数的方法。 我们可以通过assign将一个操作的结果分配给一个Variable。 为了说明这一点，我们创建了一个与另一个张量Y相同的形状的Z， 使用zeros_like来分配一个全0的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 809336392\n",
      "id(Z): 809336392\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n",
    "\n",
    "由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。\n",
    "\n",
    "但是，TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[10.902623  ,  5.584292  ,  8.424958  ,  5.02748   ,  9.17971   ],\n",
       "       [ 3.289022  ,  8.806149  ,  0.42324775,  7.458191  ,  1.2676685 ],\n",
       "       [ 6.4035788 ,  7.8937836 ,  6.2977185 ,  3.3101096 , -0.9947074 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n",
    "    A = X + Y  # 当不再需要时，分配将被复用\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变为其他类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "A = tf.ones([3,5])\n",
    "O = A.numpy()\n",
    "print(type(O), O)\n",
    "C = tf.constant(A)\n",
    "print(type(C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**：要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。\n",
    "对constant和variable都需要先转为numpy在进行提取标量的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.6], dtype=float32), 4.599999904632568, 4.599999904632568, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([4.6]).numpy()\n",
    "B, B.item(), float(B), int(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4], dtype=float32), 4.400000095367432, 4.400000095367432, 4)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = tf.Variable([4.4]).numpy()\n",
    "V, V.item(), float(V), int(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "降维是通过按行或者按列进行求和/平均值等方式来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-0.07129372,  0.8245831 ,  1.361083  ,  1.7577128 ,  0.04817514],\n",
       "        [ 0.4768322 , -0.8659228 ,  0.9600666 , -0.5569902 ,  0.8157658 ],\n",
       "        [ 0.16324878,  0.46411008, -1.562415  , -0.48711687, -0.41980916]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=0表示按照列来进行求和。\n",
    "axis=1按照行来进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 3., 3., 3., 3.], dtype=float32)>,\n",
       " TensorShape([5]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = tf.reduce_sum(Y, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>,\n",
       " TensorShape([3]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = tf.reduce_sum(Y, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，可以指定降维的坐标轴。这里是二维的所以轴只有0和1。对于更高维的可以指定更多的坐标轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(Y, axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非降维求和\n",
    "\n",
    "使用的参数是keepdims=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = tf.reduce_sum(Y, axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按指定轴进行累积求和\n",
    "\n",
    "这种操作的详细说明：第一行的值是自己。第二行是第一行的值加上第二行的值。第三行是更新后第二行的值再加上第三行的值。以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵与向量之间的乘法\n",
    "\n",
    "要求$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$和$\\boldsymbol{x} \\in \\mathbb{R}^n$。注意A的列数和x的元素个数相同。得到的结果$\\boldsymbol{Ax} \\in \\mathbb{R}^{m \\times 1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 3]), TensorShape([1, 3]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# B = tf.constant([[1],[2],[3]])\n",
    "B = tf.constant([[1,2,3]])\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[14, 32, 50]])>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "[相关的数学定义和说明详见](../../mathematics/LinearAlgebra.md)\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算所需要的向量和矩阵\n",
    "import tensorflow as tf\n",
    "x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "A = tf.random.normal([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对向量计算$L_1$范数，$L_1$为$||x||_1 = \\sum\\limits_{i=1}^n |x_i|$。也就是向量元素绝对值之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对向量计算$L_2$范数，$L_2$为$||x||_2 = \\sqrt{\\sum\\limits_{i=1}^n x_i^2}$。也就是向量元素平方之和再开根号。\n",
    "\n",
    "必须要求向量中元素不是int类型，也就是必须是float类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.4161983>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对矩阵计算$L_F$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.869356>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量之间点积（Dot Product）\n",
    "\n",
    "给定两个向量$\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^d$，它们的点积（dot product）$\\boldsymbol{x}^T \\boldsymbol{y}$（或$<\\boldsymbol{x}, \\boldsymbol{y}>$）是相同位置的按元素乘积的和：$\\sum\\limits_{i=1}^dx_iy_i$。\n",
    "\n",
    "感觉这个地方是不是这样表述更准确一些。$\\boldsymbol{x}^T \\in \\mathbb{R}^{n \\times 1}$，所以$\\boldsymbol{x}^T \\boldsymbol{y} \\in \\mathbb{R}^{n \\times n}$。它们的点积是不是应该表示为$\\boldsymbol{x} \\boldsymbol{y}^T \\in \\mathbb{R}^{1 \\times 1}$。这样是不是符合直觉一点呢？\n",
    "\n",
    "tf.tensordot有点特别，使用x_1和y_1计算时，与tf.reduce_sum(x_1 * y_1)的计算结果不一样。应该是和tf.tensordot的定义有关。而且特别的和axes这个参数有关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([11.65424], dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_1 = tf.random.normal([5,1])\n",
    "tf.tensordot(x_1, y_1, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=40.03689>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_1 * y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([[1.0], [1.0], [1.0], [1.0]])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.tensordot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=24.0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_2 * y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.gather用法\n",
    "\n",
    "```python\n",
    "tf.gather\n",
    "(\n",
    "    params,\n",
    "    indices,\n",
    "    validate_indices=None,\n",
    "    name=None,\n",
    "    axis=0\n",
    ")\n",
    "```\n",
    "\n",
    "这个函数的作用就是为了分批量提取样本的。具体而言：\n",
    "\n",
    "params是样本空间（也就是所有样本数据）。\n",
    "\n",
    "indices是想要提取的样本编号（index），可以是一个列表类似的数据类型。默认是按照第一个维度提取。二维数据就是按行提取。\n",
    "\n",
    "axis=0 已经有默认值了。默认值就是第一个维度，二维数据默认值就是取行上的数据。\n",
    "\n",
    "还有两个参数用得非常少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 1 0 5]\n",
      " [1 2 1 3 5]\n",
      " [4 5 3 1 0]\n",
      " [2 3 0 2 5]\n",
      " [5 1 4 0 0]\n",
      " [0 3 2 4 1]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(R)\n",
    "print(R.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在dive into deep learning中使用了这种用法。\n",
    "# 为什么这样用，不知道。\n",
    "ii =tf.constant([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=int32, numpy=\n",
       "array([[4, 0, 1, 0, 5],\n",
       "       [1, 2, 1, 3, 5],\n",
       "       [4, 5, 3, 1, 0]])>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(R, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=int32, numpy=\n",
       "array([[4, 0, 1, 0, 5],\n",
       "       [1, 2, 1, 3, 5],\n",
       "       [4, 5, 3, 1, 0]])>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(R, [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=int32, numpy=\n",
       "array([[4, 0, 1],\n",
       "       [1, 2, 1],\n",
       "       [4, 5, 3],\n",
       "       [2, 3, 0],\n",
       "       [5, 1, 4],\n",
       "       [0, 3, 2]])>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(R, [0,1,2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow中assign_sub()函数的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([ 1., -3.], dtype=float32)>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([3.0, 4.0])\n",
    "y = tf.Variable([2.0, 7.0])\n",
    "z = x.assign_sub(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method assign_sub in module tensorflow.python.ops.resource_variable_ops:\n",
      "\n",
      "assign_sub(delta, use_locking=None, name=None, read_value=True) method of tensorflow.python.ops.resource_variable_ops.ResourceVariable instance\n",
      "    Subtracts a value from this variable.\n",
      "    \n",
      "    Args:\n",
      "      delta: A `Tensor`. The value to subtract from this variable.\n",
      "      use_locking: If `True`, use locking during the operation.\n",
      "      name: The name to use for the operation.\n",
      "      read_value: A `bool`. Whether to read and return the new value of the\n",
      "        variable or not.\n",
      "    \n",
      "    Returns:\n",
      "      If `read_value` is `True`, this method will return the new value of the\n",
      "      variable after the assignment has completed. Otherwise, when in graph mode\n",
      "      it will return the `Operation` that does the assignment, and when in eager\n",
      "      mode it will return `None`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(x.assign_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "1. 参考\n",
    "   1. <https://zhuanlan.zhihu.com/p/380141130> \n",
    "   2. <https://blog.csdn.net/weixin_44493841/article/details/121147868>\n",
    "2. 重点：\n",
    "   一般我们常使用tf.data.Dataset.from_tensor_slices方法加载数据。同时，Dataset提供了repeat()和batch()方法方便我们建立循环的数据，repeat参数给定一个整型值就可以使数据重复几份，而batch则是将数据以多少条进行批处理，也就是按照batch参数大小切割数据。\n",
    "   \n",
    "   注意，repeat和batch的先后顺序不一样 ，结果是不同的，先repeat再batch会把数据先复制N份变成一个大数据，然后batch是根据这个大的数据来做的。例如，上面我们构造了10个数据，先repeat10份就有100个，再假设batch设置为6，那么最终数据是100/6+1=17份，那么也就是循环17次，如果先batch设置为6，那么数据先变成了10/6+1=2份，再repeat10次就有了20份数据了，循环要20次。这个一定要注意。\n",
    "3. 步骤\n",
    "   \n",
    "   Step0: 准备要加载的numpy数据\n",
    "\n",
    "   Step1: 使用 tf.data.Dataset.from_tensor_slices() 函数进行加载\n",
    "\n",
    "   Step2: 使用 shuffle() 打乱数据\n",
    "\n",
    "   Step3: 使用 map() 函数进行预处理\n",
    "\n",
    "   Step4: 使用 batch() 函数设置 batch size 值\n",
    "\n",
    "   Step5: 根据需要 使用 repeat() 设置是否循环迭代数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,0,1],\n",
    "     [1,2,1],\n",
    "     [4,5,3]])\n",
    "\n",
    "B = np.array([[1],\n",
    "     [1],\n",
    "     [0]])\n",
    "\n",
    "C = np.array([[2,3,6],\n",
    "     [7,1,2],\n",
    "     [1,2,0]])\n",
    "\n",
    "D = np.array([[0],\n",
    "     [0],\n",
    "     [1]])\n",
    "\n",
    "x_train = tf.convert_to_tensor(A, dtype=float)\n",
    "y_train = tf.convert_to_tensor(B, dtype=float)\n",
    "\n",
    "x_test = tf.convert_to_tensor(C, dtype=float)\n",
    "y_test = tf.convert_to_tensor(D, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先人工生成一组数据\n",
    "# m = 10\n",
    "# n = 5\n",
    "\n",
    "# m_test = 15\n",
    "\n",
    "# x_train = tf.zeros([m, n], dtype=float)\n",
    "# y_train = tf.ones([m, 1], dtype=float)\n",
    "\n",
    "# x_test = tf.ones([m_test, n], dtype=float)\n",
    "# y_test = tf.ones([m_test, 1], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 1]\n",
      " [1 2 1]\n",
      " [4 5 3]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n",
      "(3, 3)\n",
      "[[2 3 6]\n",
      " [7 1 2]\n",
      " [1 2 0]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "tf.print(x_train)\n",
    "tf.print(y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "tf.print(x_test)\n",
    "tf.print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(3,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 1.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 2., 1.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 5., 3.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for data in db_train:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset element_spec=(TensorSpec(shape=(3,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = db_train.repeat(2) # 把dataset重复2次\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf831573a0ff294614842876d2763885d6da16fb80bd95fae4076843946dd1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
