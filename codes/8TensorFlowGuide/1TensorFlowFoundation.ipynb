{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow基本操作\n",
    "\n",
    "1. 使用的tensorflow版本是2.8.0 \n",
    "2. 参考视频<https://www.bilibili.com/video/BV1Ub4y1e7P3?p=4>\n",
    "3. **重点-参考网页**<https://zhuanlan.zhihu.com/p/377280469>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 6 * 5矩阵\n",
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "\n",
    "# 方阵\n",
    "R_Square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建指定大小的矩阵\n",
    "\n",
    "推荐使用第二种和第三种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建全零的tensor矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m =5\n",
    "K = 4\n",
    "n = 7\n",
    "# 方法一\n",
    "P = tf.zeros([m, K], dtype=float)\n",
    "Q = tf.zeros([K, n], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建指定大小的tensor矩阵。**推荐**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 4) dtype=float32, numpy=\n",
      "array([[0.43144086, 0.85016817, 0.50062466, 0.14131236],\n",
      "       [0.77002394, 0.2379001 , 0.83680534, 0.8187988 ],\n",
      "       [0.44617015, 0.19982924, 0.8027723 , 0.79939735],\n",
      "       [0.09975889, 0.40954486, 0.16605115, 0.19585085],\n",
      "       [0.48539668, 0.2297538 , 0.28419298, 0.73151916]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **推荐**。使用tensorflow自己的函数创建随机产生的矩阵。而且矩阵中数值的分布满足每个元素都从均值为0、标准差为1的标准高斯分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7.9705125e-01 -9.1456337e-04  7.6800370e-01 -1.0291841e+00]\n",
      " [ 4.8757440e-01  2.0260191e-02  2.3996034e+00  7.4235994e-01]\n",
      " [-1.7365299e-01  5.6161481e-01  1.0555596e-01 -1.4153365e+00]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 4])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 创建全1矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE = tf.ones(shape=[3,4])\n",
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建单位矩阵\n",
    "\n",
    "对角矩阵是方阵，一般记为$diag(a_1,a_2,\\cdots,a_n)$。\\\n",
    "单位矩阵是对角矩阵的特例，所有对角线上的元素全为1。\\\n",
    "tf可以创建不是方阵的在$i=j$的位置上的类似对角矩阵的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = tf.eye(3)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_other = tf.eye(3,5)\n",
    "E_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将np.array转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "R_Square_tf = tf.convert_to_tensor(R_Square, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor的长度\n",
    "\n",
    "通过os.len()实际上获取的是矩阵或者向量第一个维度的值。\\\n",
    "\\\n",
    "对于向量获取的是元素个数。\\\n",
    "对于2维矩阵获取的是行数。\\\n",
    "对于3维矩阵获取的也是行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 0. 1. 0. 5.]\n",
      " [1. 2. 1. 3. 5.]\n",
      " [4. 5. 3. 1. 0.]\n",
      " [2. 3. 0. 2. 5.]\n",
      " [5. 1. 4. 0. 0.]\n",
      " [0. 3. 2. 4. 1.]], shape=(6, 5), dtype=float32)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(R_tf)\n",
    "print(len(R_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]], shape=(2, 3, 4), dtype=float32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "T = tf.ones([2,3,4])\n",
    "print(T)\n",
    "print(len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取tensor中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(R_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取tensor矩阵指定位置的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 2., 1., 3., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 1., 3., 0., 4., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor的形状\n",
    "\n",
    "1. 若有一个维度为-1，那么tensorflow会自动推导。\n",
    "2. tf.reshape(tensor, shape, name=None)\n",
    "    1. tensor：要改变维度（形状）的张量；\n",
    "    2. shape：希望变成什么维度；\n",
    "    3. name：操作的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "P = tf.reshape(R_tf[1, :], [1, -1])\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q = tf.reshape(R_tf[:,2], [-1, 1])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor之间的运算有两种\n",
    "1. 按元素的运算\n",
    "2. 按线性代数之间的预算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32) tf.Tensor(\n",
      "[[-0.375439   -0.7055323   0.8676633  -0.86653954]\n",
      " [-2.541676    0.9290615   0.8484637  -0.8516716 ]\n",
      " [ 1.0396664   0.10289904 -1.3224237  -0.5521795 ]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.ones([3, 4], dtype=float)\n",
    "Q = tf.random.normal(shape=[3, 4])\n",
    "print(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.624561  ,  0.2944677 ,  1.8676634 ,  0.13346046],\n",
       "       [-1.541676  ,  1.9290614 ,  1.8484638 ,  0.14832842],\n",
       "       [ 2.0396664 ,  1.1028991 , -0.3224237 ,  0.44782048]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P + Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 1.3754389 ,  1.7055323 ,  0.13233668,  1.8665395 ],\n",
       "       [ 3.541676  ,  0.07093853,  0.15153629,  1.8516716 ],\n",
       "       [-0.03966641,  0.8971009 ,  2.3224237 ,  1.5521796 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P - Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.375439  , -0.7055323 ,  0.8676633 , -0.86653954],\n",
       "       [-2.541676  ,  0.9290615 ,  0.8484637 , -0.8516716 ],\n",
       "       [ 1.0396664 ,  0.10289904, -1.3224237 , -0.5521795 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-2.6635487 , -1.4173695 ,  1.1525208 , -1.1540154 ],\n",
       "       [-0.39344117,  1.0763551 ,  1.1786008 , -1.1741616 ],\n",
       "       [ 0.961847  ,  9.718264  , -0.7561873 , -1.8110052 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P / Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素求幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0.68698764, 0.4938456 , 2.38134   , 0.42040384],\n",
       "       [0.07873433, 2.5321314 , 2.3360553 , 0.42670107],\n",
       "       [2.8282733 , 1.1083795 , 0.26648864, 0.57569367]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素判断tensor比较大小、判断是否相等，返回的是每个位置的boolen型值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P == Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [ True, False, False, False]])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P < Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [False,  True,  True,  True]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P > Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素对tensor所有元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[19.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui = tf.matmul(tf.reshape(R_Square_tf[2, :], [1, -1]),tf.reshape(R_Square_tf[:, 3], [-1, 1])) - R_Square_tf[2,3]\n",
    "print(eui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵之间数乘\n",
    "\n",
    "1. 计算的是矩阵每个对应位置元素相乘。\n",
    "2. 需要两个矩阵形状一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 8., 15.,  0.,  2.,  0.], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(R_Square_tf[2, :], R_Square_tf[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数乘以矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12.  0.  3.  0. 15.]\n",
      " [ 3.  6.  3.  9. 15.]\n",
      " [12. 15.  9.  3.  0.]\n",
      " [ 6.  9.  0.  6. 15.]\n",
      " [15.  3. 12.  0.  0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "print(a * R_Square_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor类型判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(eui, tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(eui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eui.dtype == tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的类型转换\n",
    "\n",
    "1. 参考网页<https://wenku.baidu.com/view/9a3b8439fc00bed5b9f3f90f76c66137ef064f55.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些都是错误的，在tf 2.0之后都不支持了。\n",
    "\n",
    "# 将字符串转化为tf.float32（默认）和tf.int32\n",
    "# tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "\n",
    "# 转化为tf.float64\n",
    "# tf.to_double(eui, name='ToDouble')\n",
    "\n",
    "# 转化为tf.float32\n",
    "# tf.to_float(eui, name='ToFloat')\n",
    "\n",
    "# 转化为tf.int32\n",
    "# tf.to_int32(eui, name='ToInt32')\n",
    "\n",
    "# 转化为tf.int64\n",
    "# tf.to_int64(eui, name='ToInt64')\n",
    "\n",
    "# 转化为dtype指定的类型\n",
    "# tf.cast(x, dtype, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0. 1. 2. 3. 4.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.arange(5)\n",
    "print(x_0)\n",
    "x_1 = tf.convert_to_tensor(x_0, dtype=tf.int32)\n",
    "print(x_1)\n",
    "x_2 = tf.cast(x_1, dtype=tf.float32)\n",
    "print(x_2)\n",
    "x_3 = tf.cast(x_2, dtype=tf.int32)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵计算平方\n",
    "\n",
    "1. 计算的是每个元素的平方值\n",
    "2. tf.math还有这个类，下面应该有很多数学方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[361.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "eui_square = tf.square(eui)\n",
    "print(eui_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1.  4.  1.  9. 25.]\n",
      " [16. 25.  9.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "R_tf_square = tf.square(R_tf)\n",
    "print(R_tf_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算tensor所有行或者所有列的和\n",
    "\n",
    "1. 使用tf.reduce_sum来完成\n",
    "2. 建议都需要填写keepdims=True参数，用于保证tensor的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    This is the reduction operation for the elementwise `tf.math.add` op.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    of the entries in `axis`, which must be unique. If `keepdims` is true, the\n",
      "    reduced dimensions are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "      >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "      >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "      >>> x.numpy()\n",
      "      array([[1, 1, 1],\n",
      "             [1, 1, 1]], dtype=int32)\n",
      "      >>> # sum all the elements\n",
      "      >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "      >>> tf.reduce_sum(x).numpy()\n",
      "      6\n",
      "      >>> # reduce along the first dimension\n",
      "      >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> tf.reduce_sum(x, 0).numpy()\n",
      "      array([2, 2, 2], dtype=int32)\n",
      "      >>> # reduce along the second dimension\n",
      "      >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "      >>> tf.reduce_sum(x, 1).numpy()\n",
      "      array([3, 3], dtype=int32)\n",
      "      >>> # keep the original dimensions\n",
      "      >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "      array([[3],\n",
      "             [3]], dtype=int32)\n",
      "      >>> # reduce along both dimensions\n",
      "      >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "      >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "      >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "      >>> # 2 + 2 + 2 = 6\n",
      "      >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "      6\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10.]\n",
      " [12.]\n",
      " [13.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]], shape=(6, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按行求和\n",
    "row_sum = tf.reduce_sum(R_tf, 1, keepdims=True)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16. 14. 11. 10. 16.]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 按列求和\n",
    "column_sum = tf.reduce_sum(R_tf, 0, keepdims=True)\n",
    "print(column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(67.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对所有元素求总和\n",
    "\n",
    "total_sum = tf.reduce_sum(tf.reduce_sum(R_tf, 0))\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改tensor指定位置的值\n",
    "\n",
    "1. 重点-参考网址<https://blog.csdn.net/qq_34418352/article/details/106399327>\n",
    "2. tensor本身不能直接修改指定位置的值，需要转化为Variable之后，配合assign（动词，分配的意思）一起来完成这个操作。\n",
    "3. 使用的是tf.Variable.assign和tf.Variable.assign_add来进行修改。也就是说支队tensorflow variable类型才能使用。<https://www.jianshu.com/p/efcb86940896>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(6, 5) dtype=float32, numpy=\n",
      "array([[16.,  0.,  1.,  0., 25.],\n",
      "       [ 1., 99.,  1.,  9., 25.],\n",
      "       [16., 25.,  9.,  1.,  0.],\n",
      "       [ 4.,  9.,  0.,  4., 25.],\n",
      "       [25.,  1., 16.,  0.,  0.],\n",
      "       [ 0.,  9.,  4., 16.,  1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "R_tf_square_variable = tf.Variable(R_tf_square)\n",
    "R_tf_square_variable[1, 1].assign(99)\n",
    "print(R_tf_square_variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改指定位置的值第二种方法\n",
    "\n",
    "1. 这种方法也太不直接了。不建议使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[16.  0.  1.  0. 25.]\n",
      " [ 1. 99.  1.  9. 25.]\n",
      " [16. 25. 44.  1.  0.]\n",
      " [ 4.  9.  0.  4. 25.]\n",
      " [25.  1. 16.  0.  0.]\n",
      " [ 0.  9.  4. 16.  1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def ModifyTensor(input_tensor, position=None, value=None):\n",
    "    input_tensor = input_tensor.numpy()\n",
    "    input_tensor[tuple(position)] = value\n",
    "    return input_tensor\n",
    "# new_tensor\n",
    "R_tf_square_variable = tf.py_function(ModifyTensor, inp=[R_tf_square_variable, [2,2], 44], \n",
    "                            Tout=R_tf_square_variable.dtype)\n",
    "print(type(R_tf_square_variable))\n",
    "print(R_tf_square_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变量的定义及使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a + 2) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义矩阵变量\n",
    "3. 修改特定位置的值\n",
    "4. 修改一行值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.26801476, 0.1535667 , 0.0712911 , 0.04843451, 0.6471752 ],\n",
      "       [0.44910797, 0.95876855, 0.32208735, 0.13650386, 0.4854029 ],\n",
      "       [0.4881721 , 0.51031166, 0.6748007 , 0.11212479, 0.94872445]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.26801476, 0.1535667 , 0.0712911 , 0.04843451, 0.6471752 ],\n",
      "       [0.44910797, 5.        , 0.32208735, 0.13650386, 0.4854029 ],\n",
      "       [0.4881721 , 0.51031166, 0.6748007 , 0.11212479, 0.94872445]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.26801476, 0.1535667 , 0.0712911 , 0.04843451, 0.6471752 ],\n",
      "       [0.44910797, 5.        , 0.32208735, 0.13650386, 0.4854029 ],\n",
      "       [1.        , 2.        , 3.        , 4.        , 5.        ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)\n",
    "print(a)\n",
    "a[1, 1].assign(5)\n",
    "print(a)\n",
    "a[2, :].assign(tf.constant([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. tf.Variable.assign_add的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = (a.assign_add(2)) * 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 循环访问tensor变量中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display P[i, :].shape=(5,) and P[i, :]=[0.91882783 0.0759024  0.92322785 0.02995001 0.8881097 ]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.182659   0.95950633 0.79451627 0.5247518  0.01154005]\n",
      "Display P[i, :].shape=(5,) and P[i, :]=[0.6942809  0.34874028 0.45080623 0.28575063 0.06420353]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.91882783 0.182659   0.6942809 ]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.0759024  0.95950633 0.34874028]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.92322785 0.79451627 0.45080623]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.02995001 0.5247518  0.28575063]\n",
      "Display P[:, j].shape=(5,) and P[:, j]=[0.8881097  0.01154005 0.06420353]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "K = 5\n",
    "P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"Display P[i, :].shape={} and P[i, :]={}\".format(P[i, :].shape, P[i, :]))\n",
    "\n",
    "for j in range(K):\n",
    "    print(\"Display P[:, j].shape={} and P[:, j]={}\".format(P[i, :].shape, P[:, j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 修改指定行，不能用=来进行赋值。必须用assign来赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.91882783, 0.0759024 , 0.92322785, 0.02995001, 0.8881097 ],\n",
       "       [1.1014868 , 1.0354087 , 1.7177441 , 0.5547018 , 0.89964974],\n",
       "       [0.6942809 , 0.34874028, 0.45080623, 0.28575063, 0.06420353]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,:].assign(P[1,:] + P[0,:])\n",
    "# P[1,:] = P[1,:] + P[0,:]\n",
    "# print(P[1,:])\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 修改指定列\n",
    "这个地方要与第9点进行比较，并没有出现将列拿出来单独计算时的问题！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.91882783, 0.0759024 , 0.92322785, 0.02995001, 0.8881097 ],\n",
      "       [1.1014868 , 1.0354087 , 1.7177441 , 0.5547018 , 0.89964974],\n",
      "       [0.6942809 , 0.34874028, 0.45080623, 0.28575063, 0.06420353]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.99473023, 0.0759024 , 0.92322785, 0.02995001, 0.8881097 ],\n",
      "       [2.1368957 , 1.0354087 , 1.7177441 , 0.5547018 , 0.89964974],\n",
      "       [1.0430212 , 0.34874028, 0.45080623, 0.28575063, 0.06420353]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "\n",
    "P[:, 0].assign(P[:, 0] + P[:, 1])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **读取tensor矩阵中的一列时，直接读取出来的维度不对！需要使用reshape来调整维度**。\n",
    "\n",
    "在读取一个维度的时候，读取出来的是3个标量，而不是1*3的向量！！！在做乘法的时候表现出来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.57025856 0.4053715  0.2647269 ], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.57025856]\n",
      " [0.4053715 ]\n",
      " [0.2647269 ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "P = tf.Variable(np.random.rand(m, n), dtype=float)\n",
    "\n",
    "# 注意两者的维度不同！！！\n",
    "print(P[:,1])\n",
    "print(tf.reshape(P[:,1], [3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！！ c是一个2维的矩阵，只不过维度是[1,1]。这个和直接P[1,1]有区别！！！而且这两个值不能直接进行计算！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5596012]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[-0.4403988]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.matmul(tf.reshape(P[:,1], [1,m]), tf.reshape(P[:,1], [m,1]))\n",
    "print(c)\n",
    "\n",
    "d = c - tf.Variable(1.0)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这个报错！就不能将一个二维矩阵赋值给一个标量。\n",
    "但是按一般的理解P[1,1]取出来的值也应该是一个二维矩阵，但tensorflow做了严格的区分。\n",
    "只能通过c[0,0]来转换一下再进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.6211364 , 0.57025856, 0.85768   , 0.16790171, 0.76152724],\n",
      "       [0.5395178 , 0.4053715 , 0.8894003 , 0.63968676, 0.3847038 ],\n",
      "       [0.8028148 , 0.2647269 , 0.5983037 , 0.63252807, 0.6088181 ]],\n",
      "      dtype=float32)> tf.Tensor([[0.5596012]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor(0.4053715, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P, c)\n",
    "print(P[1,1])\n",
    "# P[1,1].assign(c)\n",
    "# print(P, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将一个值赋值给矩阵中指定的位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.6211364 , 0.57025856, 0.85768   , 0.16790171, 0.76152724],\n",
      "       [0.5395178 , 0.4053715 , 0.8894003 , 0.63968676, 0.3847038 ],\n",
      "       [0.8028148 , 0.2647269 , 0.5983037 , 0.63252807, 0.6088181 ]],\n",
      "      dtype=float32)> 1.4\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.6211364 , 0.57025856, 0.85768   , 0.16790171, 0.76152724],\n",
      "       [0.5395178 , 1.4       , 0.8894003 , 0.63968676, 0.3847038 ],\n",
      "       [0.8028148 , 0.2647269 , 0.5983037 , 0.63252807, 0.6088181 ]],\n",
      "      dtype=float32)> 1.4\n"
     ]
    }
   ],
   "source": [
    "d = 1.4\n",
    "print(P, d)\n",
    "P[1,1].assign(d)\n",
    "print(P, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[0.6211364 , 0.57025856, 0.85768   , 0.16790171, 0.76152724],\n",
       "       [0.5395178 , 0.5596012 , 0.8894003 , 0.63968676, 0.3847038 ],\n",
       "       [0.8028148 , 0.2647269 , 0.5983037 , 0.63252807, 0.6088181 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1,1].assign(c[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. tensor之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. tensor与数值之间比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'float'>\n",
      "a > b\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2.0)\n",
    "b = 1.0\n",
    "\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "\n",
    "if a > b:\n",
    "    print(\"a > b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. tf.slice的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(np.random.rand(3, 5), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.4955216  0.55671823 0.9319378  0.9532316  0.1546873 ]], shape=(1, 5), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "sr = tf.slice(a, [0,0], [1,5])\n",
    "print(sr)\n",
    "print(type(sr))\n",
    "print(sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.4955216 ]\n",
      " [0.44926876]\n",
      " [0.26596218]], shape=(3, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = tf.slice(a, [0,0], [3,1])\n",
    "print(sc)\n",
    "print(type(sc))\n",
    "print(sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片了之后无法赋值！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.4955216]], shape=(1, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 1)\n",
      "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[0.4955216 , 0.55671823, 0.9319378 , 0.9532316 , 0.1546873 ],\n",
      "       [0.44926876, 0.68685144, 0.30435973, 0.96370775, 0.95880693],\n",
      "       [0.26596218, 0.20282783, 0.06525303, 0.6754675 , 0.5663379 ]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "so = tf.slice(a, [0,0], [1,1])\n",
    "print(so)\n",
    "print(type(so))\n",
    "print(so.shape)\n",
    "\n",
    "so = 44\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切片和索引\n",
    "\n",
    "tensorflow对tensor的处理可以向numpy一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.138094    0.39117828 -0.10791022  1.555232    0.7438391 ]\n",
      " [-0.38289082 -0.49445543  0.4139752  -0.73416686  0.4170466 ]\n",
      " [-1.0501405   0.03855523  0.75220084 -0.43975762 -0.55680084]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "P = tf.random.normal(shape=[3, 5])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       " array([-1.0501405 ,  0.03855523,  0.75220084, -0.43975762, -0.55680084],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[-0.38289082, -0.49445543,  0.4139752 , -0.73416686,  0.4170466 ],\n",
       "        [-1.0501405 ,  0.03855523,  0.75220084, -0.43975762, -0.55680084]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[-1], P[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对矩阵中的多个元素进行赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       "array([[12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [12.        , 12.        , 12.        , 12.        , 12.        ],\n",
       "       [-1.0501405 ,  0.03855523,  0.75220084, -0.43975762, -0.55680084]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(P)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor可以与数值比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if 1999 > eui_square:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看tensor矩阵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _EagerTensorBase.eval of <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
      "array([[ 1.138094  ,  0.39117828, -0.10791022,  1.555232  ,  0.7438391 ],\n",
      "       [-0.38289082, -0.49445543,  0.4139752 , -0.73416686,  0.4170466 ],\n",
      "       [-1.0501405 ,  0.03855523,  0.75220084, -0.43975762, -0.55680084]],\n",
      "      dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(P.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求向量的范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function norm_v2 in module tensorflow.python.ops.linalg_ops:\n",
      "\n",
      "norm_v2(tensor, ord='euclidean', axis=None, keepdims=None, name=None)\n",
      "    Computes the norm of vectors, matrices, and tensors.\n",
      "    \n",
      "    This function can compute several different vector norms (the 1-norm, the\n",
      "    Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\n",
      "    matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n",
      "    \n",
      "    Args:\n",
      "      tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n",
      "      ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n",
      "        `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n",
      "        p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n",
      "        `tensor` is a matrix and equivalent to 2-norm for vectors.\n",
      "        Some restrictions apply:\n",
      "          a) The Frobenius norm `'fro'` is not defined for vectors,\n",
      "          b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n",
      "             `2`, `np.inf` are supported.\n",
      "        See the description of `axis` on how to compute norms for a batch of\n",
      "        vectors or matrices stored in a tensor.\n",
      "      axis: If `axis` is `None` (the default), the input is considered a vector\n",
      "        and a single vector norm is computed over the entire set of values in the\n",
      "        tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n",
      "        `norm(reshape(tensor, [-1]), ord=ord)`.\n",
      "        If `axis` is a Python integer, the input is considered a batch of vectors,\n",
      "        and `axis` determines the axis in `tensor` over which to compute vector\n",
      "        norms.\n",
      "        If `axis` is a 2-tuple of Python integers it is considered a batch of\n",
      "        matrices and `axis` determines the axes in `tensor` over which to compute\n",
      "        a matrix norm.\n",
      "        Negative indices are supported. Example: If you are passing a tensor that\n",
      "        can be either a matrix or a batch of matrices at runtime, pass\n",
      "        `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n",
      "        computed.\n",
      "      keepdims: If True, the axis indicated in `axis` are kept with size 1.\n",
      "        Otherwise, the dimensions in `axis` are removed from the output shape.\n",
      "      name: The name of the op.\n",
      "    \n",
      "    Returns:\n",
      "      output: A `Tensor` of the same type as tensor, containing the vector or\n",
      "        matrix norms. If `keepdims` is True then the rank of output is equal to\n",
      "        the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n",
      "        if `axis` is an integer, the rank of `output` is one less than the rank\n",
      "        of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n",
      "        than the rank of `tensor`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `ord` or `axis` is invalid.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Mostly equivalent to numpy.linalg.norm.\n",
      "    Not supported: ord <= 0, 2-norm for matrices, nuclear norm.\n",
      "    Other differences:\n",
      "      a) If axis is `None`, treats the flattened `tensor` as a vector\n",
      "       regardless of rank.\n",
      "      b) Explicitly supports 'euclidean' norm as the default, including for\n",
      "       higher order tensors.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[4., 0., 1., 0., 5.],\n",
       "       [1., 2., 1., 3., 5.],\n",
       "       [4., 5., 3., 1., 0.],\n",
       "       [2., 3., 0., 2., 5.],\n",
       "       [5., 1., 4., 0., 0.],\n",
       "       [0., 3., 2., 4., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4. 0. 1. 0. 5.]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.4807405, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算的是向量的。\n",
    "row_vector = tf.reshape(R_tf[0, :], [1, -1])\n",
    "print(row_vector)\n",
    "\n",
    "L1 = tf.norm(row_vector, ord=1)\n",
    "print(L1)\n",
    "\n",
    "L2 = tf.norm(row_vector, ord=2)\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([16. 14. 11. 10. 16.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([6.4807405 6.3245554 7.141428  6.4807405 6.4807405 5.477226 ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵所有行或者所有列的范数。\n",
    "\n",
    "# 按列求L1范数\n",
    "L1_matrix = tf.norm(R_tf, ord=1, axis=0)\n",
    "print(L1_matrix)\n",
    "\n",
    "# 按行求L2范数\n",
    "L2_matrix = tf.norm(R_tf, ord=2, axis=1)\n",
    "print(L2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matrix_inverse in module tensorflow.python.ops.gen_linalg_ops:\n",
      "\n",
      "matrix_inverse(input, adjoint=False, name=None)\n",
      "    Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\n",
      "    \n",
      "    \n",
      "    The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\n",
      "    form square matrices. The output is a tensor of the same shape as the input\n",
      "    containing the inverse for all input submatrices `[..., :, :]`.\n",
      "    \n",
      "    The op uses LU decomposition with partial pivoting to compute the inverses.\n",
      "    \n",
      "    If a matrix is not invertible there is no guarantee what the op does. It\n",
      "    may detect the condition and raise an exception or it may simply return a\n",
      "    garbage result.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.\n",
      "        Shape is `[..., M, M]`.\n",
      "      adjoint: An optional `bool`. Defaults to `False`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.linalg.inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8799998  -0.8399998  -0.92        1.7199999   1.12      ]\n",
      " [ 0.39999998  0.19999999  0.6000001  -0.6        -0.6       ]\n",
      " [ 0.99999976  0.99999976  0.99999994 -1.9999998  -0.99999994]\n",
      " [-1.4799998  -0.6399999  -1.3200002   2.12        1.5200001 ]\n",
      " [ 0.7039999   0.47199994  0.536      -0.9759999  -0.696     ]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.linalg.inv(R_Square_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求矩阵的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
      "    Transposes `a`, where `a` is a Tensor.\n",
      "    \n",
      "    Permutes the dimensions according to the value of `perm`.\n",
      "    \n",
      "    The returned tensor's dimension `i` will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
      "    of the input tensor. Hence by default, this operation performs a regular\n",
      "    matrix transpose on 2-D input Tensors.\n",
      "    \n",
      "    If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
      "    then the values of `a` are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.transpose(x)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    \n",
      "    Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
      "    \n",
      "    If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
      "    \n",
      "    >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "    ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    >>> tf.transpose(x, conjugate=True)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
      "    array([[1.-1.j, 4.-4.j],\n",
      "           [2.-2.j, 5.-5.j],\n",
      "           [3.-3.j, 6.-6.j]])>\n",
      "    \n",
      "    'perm' is more useful for n-dimensional tensors where n > 2:\n",
      "    \n",
      "    >>> x = tf.constant([[[ 1,  2,  3],\n",
      "    ...                   [ 4,  5,  6]],\n",
      "    ...                  [[ 7,  8,  9],\n",
      "    ...                   [10, 11, 12]]])\n",
      "    \n",
      "    As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
      "    \n",
      "    To take the transpose of the matrices in dimension-0 (such as when you are\n",
      "    transposing matrices where 0 is the batch dimension), you would set\n",
      "    `perm=[0,2,1]`.\n",
      "    \n",
      "    >>> tf.transpose(x, perm=[0, 2, 1])\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[ 1,  4],\n",
      "            [ 2,  5],\n",
      "            [ 3,  6]],\n",
      "            [[ 7, 10],\n",
      "            [ 8, 11],\n",
      "            [ 9, 12]]], dtype=int32)>\n",
      "    \n",
      "    Note: This has a shorthand `linalg.matrix_transpose`):\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 1. 4. 2. 5. 0.]\n",
      " [0. 2. 5. 3. 1. 3.]\n",
      " [1. 1. 3. 0. 4. 2.]\n",
      " [0. 3. 1. 2. 0. 4.]\n",
      " [5. 5. 0. 5. 0. 1.]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.transpose(R_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对一元方程求导。参考<https://blog.csdn.net/AwesomeP/article/details/123787448>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里定义了一个变量x，使用tf.variable()声明，与普通张量一样，该变量拥有形状、类型和值这3种属性。变量与普通张量的一个重要区别是，它默认能够被Tensorflow的自动求导机制求导，因此常用于定义机器模型的参数。\n",
    "\n",
    "补充知识:tf.Variable () 将变量标记为“可训练”，**被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # tf为2.*版本\n",
    "x = tf.Variable(initial_value = 3.0) # 定义变量x，初始化为3\n",
    "with tf.GradientTape() as tape: # 在tf.GradientTape()的上下文中，所有的计算步骤都会被记录，用以求导\n",
    "    y = tf.square(x) # y = x的平方\n",
    "y_grad = tape.gradient(y,x) # 计算y关于x的导数\n",
    "print(y)      # 输出3的平方  tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "print(y_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中使用了tf.Variable()定义x，下面我们展示使用tf.constant()定义x，注意两者之间求导时的不同，需要tape.watch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.constant(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    tape.watch(x) \n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n",
      "81.0\n",
      "108.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape(persistent=True) as tape:  # 注意使用persistent=True\n",
    "    y = tf.square(x)    # y = x的平方\n",
    "    z = tf.pow(x,4)     # z = x的4次方\n",
    "y_grad = tape.gradient(y,x) \n",
    "Z_grad = tape.gradient(z,x)\n",
    "print(y.numpy())       # 9.0， 我们将tensor类型转换为numpy类型\n",
    "print(y_grad.numpy())  # 6.0\n",
    "\n",
    "print(z.numpy())       # 81.0\n",
    "print(Z_grad.numpy())  # 108.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对多元函数求偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_will_grad = tf.Variable(np.random.rand(3, 5), dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对矩阵求偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "理解：当两个向量的形状不符合向量（矩阵的试验不成功）计算的规则时\n",
    "1. 将两个向量分别按照二元计算的两个向量的形状进行扩展，变成为相同的形状。\n",
    "2. 然后将两个向量再进行二元计算。\n",
    "\n",
    "如下面的例子所示，将A只有一列，B只有一行。A按照B的行数进行复制，B按照A的列数进行复制，然后都形成$3 \\times 2$的矩阵在进行加法计算。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>中的说明如下：我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]])>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.reshape(tf.range(3), (3, 1))\n",
    "B = tf.reshape(tf.range(2), (1, 2))\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.ones([2,2])\n",
    "D = tf.ones([3,3])\n",
    "C, D\n",
    "\n",
    "# 这个时候C和D不能使用广播机制来进行计算。\n",
    "# C + D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "这部分看到了代码但是还不理解作用。\n",
    "现在的理解是：定义一个变量，python给它分配了一块内存。经过一个运算之后，再次给这个变量赋值时，变量指向了另外一块地址。而原来定义变量时分配的内容没有释放。而tensorflow直接解决了这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. python的例子。\n",
    "\n",
    "<https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html>说明如下：在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。**这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow的例子。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "Variables是TensorFlow中的可变容器，它们提供了一种存储模型参数的方法。 我们可以通过assign将一个操作的结果分配给一个Variable。 为了说明这一点，我们创建了一个与另一个张量Y相同的形状的Z， 使用zeros_like来分配一个全0的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1622265288\n",
      "id(Z): 1622265288\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n",
    "\n",
    "由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。\n",
    "\n",
    "但是，TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[10.488976  ,  4.153824  ,  6.9177847 , -0.20857757,  4.556662  ],\n",
       "       [ 7.6709642 ,  3.0006697 ,  2.2487464 ,  1.7624911 ,  7.7171993 ],\n",
       "       [ 6.1307464 ,  1.0036516 , -0.02064353,  5.4354377 ,  2.7916303 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n",
    "    A = X + Y  # 当不再需要时，分配将被复用\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor变为其他类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "A = tf.ones([3,5])\n",
    "O = A.numpy()\n",
    "print(type(O), O)\n",
    "C = tf.constant(A)\n",
    "print(type(C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**：要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。\n",
    "对constant和variable都需要先转为numpy在进行提取标量的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.6], dtype=float32), 4.599999904632568, 4.599999904632568, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([4.6]).numpy()\n",
    "B, B.item(), float(B), int(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4], dtype=float32), 4.400000095367432, 4.400000095367432, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = tf.Variable([4.4]).numpy()\n",
    "V, V.item(), float(V), int(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "降维是通过按行或者按列进行求和/平均值等方式来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-1.7603735 , -0.3856228 , -0.17601512,  0.8873603 , -1.0727581 ],\n",
       "        [-1.8786201 , -1.5494004 ,  1.3354777 , -1.0904859 , -0.98799276],\n",
       "        [ 0.88038915, -1.6967478 ,  0.84404236, -1.2167649 , -1.7032427 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.normal(shape=[3,5])\n",
    "Y = tf.ones([3,5])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=0表示按照列来进行求和。\n",
    "axis=1按照行来进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([3., 3., 3., 3., 3.], dtype=float32)>,\n",
       " TensorShape([5]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = tf.reduce_sum(Y, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>,\n",
       " TensorShape([3]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = tf.reduce_sum(Y, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，可以指定降维的坐标轴。这里是二维的所以轴只有0和1。对于更高维的可以指定更多的坐标轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(Y, axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非降维求和\n",
    "\n",
    "使用的参数是keepdims=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[5.],\n",
       "       [5.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = tf.reduce_sum(Y, axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按指定轴进行累积求和\n",
    "\n",
    "这种操作的详细说明：第一行的值是自己。第二行是第一行的值加上第二行的值。第三行是更新后第二行的值再加上第三行的值。以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵与向量之间的乘法\n",
    "\n",
    "要求$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$和$\\boldsymbol{x} \\in \\mathbb{R}^n$。注意A的列数和x的元素个数相同。得到的结果$\\boldsymbol{Ax} \\in \\mathbb{R}^{m \\times 1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 3]), TensorShape([1, 3]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# B = tf.constant([[1],[2],[3]])\n",
    "B = tf.constant([[1,2,3]])\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[14, 32, 50]])>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.matvec(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "[相关的数学定义和说明详见](../../mathematics/LinearAlgebra.md)\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算所需要的向量和矩阵\n",
    "import tensorflow as tf\n",
    "x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "A = tf.random.normal([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对向量计算$L_1$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对向量计算$L_2$范数\n",
    "\n",
    "必须要求向量中元素不是int类型，也就是必须是float类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.4161983>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对矩阵计算$L_F$范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.7382605>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量之间点积（Dot Product）\n",
    "\n",
    "给定两个向量$\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^d$，它们的点积（dot product）$\\boldsymbol{x}^T \\boldsymbol{y}$（或$<\\boldsymbol{x}, \\boldsymbol{y}>$）是相同位置的按元素乘积的和：$\\sum\\limits_{i=1}^dx_iy_i$\n",
    "\n",
    "tf.tensordot有点特别，使用x_1和y_1计算时，与tf.reduce_sum(x_1 * y_1)的计算结果不一样。应该是和tf.tensordot的定义有关。而且特别的和axes这个参数有关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.8162303], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_1 = tf.random.normal([5,1])\n",
    "tf.tensordot(x_1, y_1, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-7.6646547>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_1 * y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tf.constant([0, 1.0, 2.0, 3.0])\n",
    "y_2 = tf.constant([[1.0], [1.0], [1.0], [1.0]])\n",
    "tf.tensordot(x_2, y_2, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.tensordot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=24.0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x_2 * y_2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
