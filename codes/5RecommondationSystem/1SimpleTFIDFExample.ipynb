{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单TF-IDF示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义数据和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m没有使用标点符号。\n",
    "docA = \"The cat sat on my bed\"\n",
    "docB = \"The dog sat on my kness\"\n",
    "\n",
    "# 先分词。生成词袋。\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "\n",
    "# 构建完整词库，取并集。\n",
    "wordSet = set(bowA).union(set(bowB)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 进行词数统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>dog</th>\n",
       "      <th>sat</th>\n",
       "      <th>The</th>\n",
       "      <th>kness</th>\n",
       "      <th>on</th>\n",
       "      <th>my</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bed  dog  sat  The  kness  on  my  cat\n",
       "0    1    0    1    1      0   1   1    1\n",
       "1    0    1    1    1      1   1   1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用统计字典来保存词出现的次数。\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "# 遍历文档，统计词数。\n",
    "for word in bowA:\n",
    "    wordDictA[word] += 1\n",
    "for word in bowB:\n",
    "    wordDictB[word] += 1\n",
    "\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 计算TF\n",
    "\n",
    "$$\\text{词频TF}=\\frac{\\text{某个词在一篇文章中出现的次数}}{\\text{一篇文章的总词数}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bed': 0.16666666666666666,\n",
       " 'dog': 0.0,\n",
       " 'sat': 0.16666666666666666,\n",
       " 'The': 0.16666666666666666,\n",
       " 'kness': 0.0,\n",
       " 'on': 0.16666666666666666,\n",
       " 'my': 0.16666666666666666,\n",
       " 'cat': 0.16666666666666666}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    # 用一个字典对象记录TF\n",
    "    tfDict = {}\n",
    "    nbowCount = len(bow)\n",
    "    \n",
    "    for word,count in wordDict.items():\n",
    "        # print(word, count,)\n",
    "        tfDict[word] = count / nbowCount\n",
    "    \n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(wordDictA, bowA)\n",
    "tfB = computeTF(wordDictB, bowB)\n",
    "\n",
    "tfA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 计算逆文档频率\n",
    "\n",
    "$$\\text{逆文档频率IDF}=\\log_{10}\\frac{\\text{语料库的文档总数}}{\\text{包含该词的文档数}+1}$$\n",
    "1. “语料库的文档总数”是固定值。\n",
    "2. 只需要求“包含该词的文档数”即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bed': 1, 'dog': 1, 'sat': 2, 'The': 2, 'kness': 1, 'on': 2, 'my': 2, 'cat': 1}\n",
      "2 1\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "2 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bed': 0.17609125905568124,\n",
       " 'dog': 0.17609125905568124,\n",
       " 'sat': 0.0,\n",
       " 'The': 0.0,\n",
       " 'kness': 0.17609125905568124,\n",
       " 'on': 0.0,\n",
       " 'my': 0.0,\n",
       " 'cat': 0.17609125905568124}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(wordDictList):\n",
    "    # 用一个字典对象来保存idf结果，每个词作为key，初始值为0。\n",
    "    idfDict = dict.fromkeys(wordDictList[0], 0)\n",
    "    N = len(wordDictList)\n",
    "    \n",
    "    for wordDict in wordDictList:\n",
    "        # 遍历字典中的每个词汇，统计Ni\n",
    "        for word,count in wordDict.items():\n",
    "            if count > 0 :\n",
    "                # 先把Ni增加1，存入到idfDict\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    print(idfDict)\n",
    "    # 已经得到所有词汇i对应的Ni，现在更加公式把它替换为idf值。\n",
    "    for word, ni in idfDict.items():\n",
    "        idfDict[word] = math.log10((N+1)/(ni+1))\n",
    "        print(N, ni)\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 计算TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>dog</th>\n",
       "      <th>sat</th>\n",
       "      <th>The</th>\n",
       "      <th>kness</th>\n",
       "      <th>on</th>\n",
       "      <th>my</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       dog  sat  The     kness   on   my       cat\n",
       "0  0.029349  0.000000  0.0  0.0  0.000000  0.0  0.0  0.029349\n",
       "1  0.000000  0.029349  0.0  0.0  0.029349  0.0  0.0  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tf, idfs):\n",
    "    tfidf = {}\n",
    "    for word, tfval in tf.items():\n",
    "        tfidf[word] = tfval * idfs[word]\n",
    "        \n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个相对复杂的例子\n",
    "\n",
    "1. 几个文件PDF导出成txt的时候格式还都不一样。wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于第一个字符如果是特殊字符的行不进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipCharList = ['\\n', '\\r', '\\t', ' ', '.', ':', 'a', 'b', 'c', 'd', \n",
    "                'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "                'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 设置总词库\n",
    "   1. 列名是单词。\n",
    "   2. 行名是不同的文章的名称或者对应的index。\n",
    "   3. 目前是四篇paper，创建的词袋是4行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [P1, P2, P3, P4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordBag = pd.DataFrame([[],[],[],[]])\n",
    "# 设置行名称。\n",
    "WordBag.index = pd.Series(['P1', 'P2', 'P3', 'P4'])\n",
    "WordBag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义将词添加入总词袋中，并且对出现的次数进行记录的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddWordToWordBag(paperName, word):\n",
    "    # WordList = WordBag.columns.to_list()\n",
    "    if word in skipCharList:\n",
    "        return\n",
    "    if word in WordBag.columns.to_list():\n",
    "        WordBag.loc[paperName, word] += 1\n",
    "    else:\n",
    "        WordBag[word] = 0\n",
    "        WordBag.loc[paperName, word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "\n",
    "# words = ['attention', 'NaN', 'all', 'is', 'you', 'need', 'NaN', 'NaN']\n",
    "\n",
    "# for word in words:\n",
    "#     AddWordToWordBag(1, word)\n",
    "    \n",
    "# print(WordBag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 定义文本路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4个文本的路径。\n",
    "PaperPathList = [\"../../data/TFIDF/1.txt\", \n",
    "                 \"../../data/TFIDF/2.txt\", \n",
    "                 \"../../data/TFIDF/3.txt\", \n",
    "                 \"../../data/TFIDF/4.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第一个文档进行处理。\n",
    "\n",
    "1. 在PDF导出为txt的时候就将所有的文字分为了逐个的单词，而且每个单词占一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python37\\lib\\site-packages\\ipykernel_launcher.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neural  collaborative  filtering  xiangnan  helizi  liao  hanwang  zhang  \\\n",
      "P1      44             41          4         1       1     1        1      8   \n",
      "P2       0              0          0         0       0     0        0      0   \n",
      "P3       0              0          0         0       0     0        0      0   \n",
      "P4       0              0          0         0       0     0        0      0   \n",
      "\n",
      "    national  university  ...  shen  luan  discrete  start  identifying  \\\n",
      "P1         4           6  ...     1     2         1      1            1   \n",
      "P2         0           0  ...     0     0         0      0            0   \n",
      "P3         0           0  ...     0     0         0      0            0   \n",
      "P4         0           0  ...     0     0         0      0            0   \n",
      "\n",
      "    naming  attributes  tang  ding  zhou  \n",
      "P1       1           1     1     1     1  \n",
      "P2       0           0     0     0     0  \n",
      "P3       0           0     0     0     0  \n",
      "P4       0           0     0     0     0  \n",
      "\n",
      "[4 rows x 1654 columns]\n"
     ]
    }
   ],
   "source": [
    "# 对第一篇论文进行处理\n",
    "\n",
    "\n",
    "# 逐行读取\n",
    "with open(PaperPathList[0]) as f1:\n",
    "    line = f1.readline()\n",
    "    line = line\n",
    "    \n",
    "    while line:\n",
    "        if line in skipCharList:\n",
    "            line = f1.readline()  #读取一行文件，包括换行符\n",
    "            line = line\n",
    "            continue\n",
    "\n",
    "        AddWordToWordBag('P1', re.sub(r'[^a-zA-Z]', '', line).casefold())\n",
    "        \n",
    "        # 读取下一行。\n",
    "        line = f1.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # print(\"-----------\")\n",
    "        # print(Paper1WordList)\n",
    "        # print(\"-----------------------------------\")\n",
    "        \n",
    "print(WordBag)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第二个文本进行处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python37\\lib\\site-packages\\ipykernel_launcher.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neural  collaborative  filtering  xiangnan  helizi  liao  hanwang  zhang  \\\n",
      "P1      44             41          4         1       1     1        1      8   \n",
      "P2       0              0          0         0       0     0        0      1   \n",
      "P3       0              0          0         0       0     0        0      0   \n",
      "P4       0              0          0         0       0     0        0      0   \n",
      "\n",
      "    national  university  ...  forms  constructing  tegration  exten  \\\n",
      "P1         4           6  ...      0             0          0      0   \n",
      "P2         1           2  ...      1             1          1      1   \n",
      "P3         0           0  ...      0             0          0      0   \n",
      "P4         0           0  ...      0             0          0      0   \n",
      "\n",
      "    sibility  shed  light  possibly  acknowledgments  foun  \n",
      "P1         0     0      0         0                0     0  \n",
      "P2         1     1      1         1                1     1  \n",
      "P3         0     0      0         0                0     0  \n",
      "P4         0     0      0         0                0     0  \n",
      "\n",
      "[4 rows x 2448 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(PaperPathList[1]) as f2:\n",
    "    line = f2.readline()\n",
    "    line = line\n",
    "    while line:        \n",
    "        # 如果存在“|”字符，那么使用“|”对字符串进行分词。不然就用\".\"对字符串进行分词。\n",
    "        if \"|\" in line:\n",
    "            lineWords = line.split(\"|\")\n",
    "        else:\n",
    "            lineWords = line.split(\".\")\n",
    "        # print(lineWords)\n",
    "        \n",
    "        # if line in skipCharList:\n",
    "        #     line = f2.readline()  #读取一行文件，包括换行符\n",
    "        #     line = line\n",
    "        #     continue\n",
    "        \n",
    "        for word in lineWords:\n",
    "            AddWordToWordBag('P2', re.sub(r'[^a-zA-Z]', '', word).casefold())\n",
    "\n",
    "        line = f2.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(WordBag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第三个文本进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python37\\lib\\site-packages\\ipykernel_launcher.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neural  collaborative  filtering  xiangnan  helizi  liao  hanwang  zhang  \\\n",
      "P1      44             41          4         1       1     1        1      8   \n",
      "P2       0              0          0         0       0     0        0      1   \n",
      "P3      27              0          0         0       0     0        0      3   \n",
      "P4       0              0          0         0       0     0        0      0   \n",
      "\n",
      "    national  university  ...  colors  viewed  color  \\\n",
      "P1         4           6  ...       0       0      0   \n",
      "P2         1           2  ...       0       0      0   \n",
      "P3         0           1  ...       1       1      1   \n",
      "P4         0           0  ...       0       0      0   \n",
      "\n",
      "    thelawwillneverbeperfectbutitsapplicationshouldbejustthisiswhatwearemissinginmyopinioneospad  \\\n",
      "P1                                                  0                                              \n",
      "P2                                                  0                                              \n",
      "P3                                                 12                                              \n",
      "P4                                                  0                                              \n",
      "\n",
      "    apparently  anaphora  isolated  sharp  behaviour  give  \n",
      "P1           0         0         0      0          0     0  \n",
      "P2           0         0         0      0          0     0  \n",
      "P3           1         1         1      1          1     1  \n",
      "P4           0         0         0      0          0     0  \n",
      "\n",
      "[4 rows x 3290 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(PaperPathList[2]) as f3:\n",
    "    line = f3.readline()\n",
    "    line = line\n",
    "    while line: \n",
    "        # 用\" \"对字符串进行分词。\n",
    "        lineWords = line.split(\" \")\n",
    "        \n",
    "        # if line in skipCharList:\n",
    "        #     line = f3.readline()  #读取一行文件，包括换行符\n",
    "        #     line = line\n",
    "        #     continue\n",
    "            \n",
    "        # print(lineWords)\n",
    "        for word in lineWords:\n",
    "            AddWordToWordBag('P3', re.sub(r'[^a-zA-Z]', '', word).casefold())\n",
    "\n",
    "        line = f3.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # line = line[:-1]\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(WordBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zz = Paper3WordList.columns\n",
    "# print(type(zz))\n",
    "# print(zz.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第四个文本进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python37\\lib\\site-packages\\ipykernel_launcher.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neural  collaborative  filtering  xiangnan  helizi  liao  hanwang  zhang  \\\n",
      "P1      44             41          4         1       1     1        1      8   \n",
      "P2       0              0          0         0       0     0        0      1   \n",
      "P3      27              0          0         0       0     0        0      3   \n",
      "P4       2              6          5         0       0     0        0      4   \n",
      "\n",
      "    national  university  ...  locationcontentaware  nguyen  adapting  drift  \\\n",
      "P1         4           6  ...                     0       0         0      0   \n",
      "P2         1           2  ...                     0       0         0      0   \n",
      "P3         0           1  ...                     0       0         0      0   \n",
      "P4         1           2  ...                     1       1         1      1   \n",
      "\n",
      "    pp  shao  thalmann  jd  cy  sigspatial  \n",
      "P1   0     0         0   0   0           0  \n",
      "P2   0     0         0   0   0           0  \n",
      "P3   0     0         0   0   0           0  \n",
      "P4   1     1         1   1   1           1  \n",
      "\n",
      "[4 rows x 4028 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(PaperPathList[3]) as f4:\n",
    "    line = f4.readline()\n",
    "    line = line\n",
    "    while line: \n",
    "        # 用\" \"对字符串进行分词。\n",
    "        lineWords = line.split(\" \")\n",
    "        \n",
    "        # if line in skipCharList:\n",
    "        #     line = f4.readline()  #读取一行文件，包括换行符\n",
    "        #     line = line\n",
    "        #     continue\n",
    "            \n",
    "        # print(lineWords)\n",
    "        for word in lineWords:\n",
    "            AddWordToWordBag('P4', re.sub(r'[^a-zA-Z]', '', word).casefold())\n",
    "\n",
    "        line = f4.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # line = line[:-1]\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(WordBag)\n",
    "# Paper4WordList.to_csv(\"../../data/TFIDF/4t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordBag['TotalCount'] = WordBag.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordBag['max_value']=WordBag.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordBag.to_csv(\"../../data/TFIDF/WordBag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义IFIDF存储结构。\n",
    "\n",
    "1. 需要将WordBag的列名赋值给IFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neural</th>\n",
       "      <th>collaborative</th>\n",
       "      <th>filtering</th>\n",
       "      <th>xiangnan</th>\n",
       "      <th>helizi</th>\n",
       "      <th>liao</th>\n",
       "      <th>hanwang</th>\n",
       "      <th>zhang</th>\n",
       "      <th>national</th>\n",
       "      <th>university</th>\n",
       "      <th>...</th>\n",
       "      <th>nguyen</th>\n",
       "      <th>adapting</th>\n",
       "      <th>drift</th>\n",
       "      <th>pp</th>\n",
       "      <th>shao</th>\n",
       "      <th>thalmann</th>\n",
       "      <th>jd</th>\n",
       "      <th>cy</th>\n",
       "      <th>sigspatial</th>\n",
       "      <th>TotalCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TFIDF1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 4029 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neural collaborative filtering xiangnan helizi liao hanwang zhang  \\\n",
       "TFIDF1    NaN           NaN       NaN      NaN    NaN  NaN     NaN   NaN   \n",
       "TFIDF2    NaN           NaN       NaN      NaN    NaN  NaN     NaN   NaN   \n",
       "TFIDF3    NaN           NaN       NaN      NaN    NaN  NaN     NaN   NaN   \n",
       "TFIDF4    NaN           NaN       NaN      NaN    NaN  NaN     NaN   NaN   \n",
       "\n",
       "       national university  ... nguyen adapting drift   pp shao thalmann   jd  \\\n",
       "TFIDF1      NaN        NaN  ...    NaN      NaN   NaN  NaN  NaN      NaN  NaN   \n",
       "TFIDF2      NaN        NaN  ...    NaN      NaN   NaN  NaN  NaN      NaN  NaN   \n",
       "TFIDF3      NaN        NaN  ...    NaN      NaN   NaN  NaN  NaN      NaN  NaN   \n",
       "TFIDF4      NaN        NaN  ...    NaN      NaN   NaN  NaN  NaN      NaN  NaN   \n",
       "\n",
       "         cy sigspatial TotalCount  \n",
       "TFIDF1  NaN        NaN        NaN  \n",
       "TFIDF2  NaN        NaN        NaN  \n",
       "TFIDF3  NaN        NaN        NaN  \n",
       "TFIDF4  NaN        NaN        NaN  \n",
       "\n",
       "[4 rows x 4029 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordBagColumnsNameList = WordBag.columns.to_list()\n",
    "# print(WordBagColumnsNameList.remove('TotalCount'))\n",
    "# IFIDFColumnsName =  WordBag.columns.to_list().remove('TotalCount')\n",
    "# print(IFIDFColumnsName)\n",
    "IFIDF = pd.DataFrame(index=['TFIDF1', 'TFIDF2', 'TFIDF3', 'TFIDF4'], \n",
    "                     columns=WordBag.columns.to_list())\n",
    "IFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算TF\n",
    "\n",
    "1. TF为单词出现的个数除以总的单词类别数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       neural collaborative filtering  xiangnan    helizi      liao   hanwang  \\\n",
      "TFIDF1    0.0      0.000615   0.00006  0.000036  0.000036  0.000036  0.000036   \n",
      "TFIDF2    0.0           0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "TFIDF3    0.0           0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "TFIDF4    0.0      0.000078  0.000065       0.0       0.0       0.0       0.0   \n",
      "\n",
      "           zhang national university  ...    nguyen  adapting     drift  \\\n",
      "TFIDF1 -0.000093      0.0   -0.00007  ...       0.0       0.0       0.0   \n",
      "TFIDF2 -0.000018      0.0  -0.000035  ...       0.0       0.0       0.0   \n",
      "TFIDF3 -0.000043      0.0  -0.000014  ...       0.0       0.0       0.0   \n",
      "TFIDF4  -0.00004      0.0   -0.00002  ...  0.000031  0.000031  0.000031   \n",
      "\n",
      "              pp      shao  thalmann        jd        cy sigspatial TotalCount  \n",
      "TFIDF1       0.0       0.0       0.0       0.0       0.0        0.0   -0.09691  \n",
      "TFIDF2       0.0       0.0       0.0       0.0       0.0        0.0   -0.09691  \n",
      "TFIDF3       0.0       0.0       0.0       0.0       0.0        0.0   -0.09691  \n",
      "TFIDF4  0.000031  0.000031  0.000031  0.000031  0.000031   0.000031   -0.09691  \n",
      "\n",
      "[4 rows x 4029 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "documentCount = 4\n",
    "\n",
    "for columnName in WordBag.columns.to_list():\n",
    "    # IFIDF.loc['IDF', columnName] = math.log10(documentCount/(WordBag[WordBag[columnName]>0].shape[0] + 1))\n",
    "    IDF = math.log10(documentCount/(WordBag[WordBag[columnName]>0].shape[0] + 1))\n",
    "    IFIDF.loc['TFIDF1', columnName] = (WordBag.loc['P1', columnName] / WordBag.loc['P1', 'TotalCount']) * IDF\n",
    "    IFIDF.loc['TFIDF2', columnName] = (WordBag.loc['P2', columnName] / WordBag.loc['P2', 'TotalCount']) * IDF\n",
    "    IFIDF.loc['TFIDF3', columnName] = (WordBag.loc['P3', columnName] / WordBag.loc['P3', 'TotalCount']) * IDF\n",
    "    IFIDF.loc['TFIDF4', columnName] = (WordBag.loc['P4', columnName] / WordBag.loc['P4', 'TotalCount']) * IDF\n",
    "    \n",
    "print(IFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFIDF.sort_values(by='TFIDF1', ascending=False, axis=1, inplace=True)\n",
    "IFIDF.loc['TFIDF1'].T.to_csv(\"../../data/TFIDF/P1_IFIDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFIDF.sort_values(by='TFIDF2', ascending=False, axis=1, inplace=True)\n",
    "IFIDF.loc['TFIDF2'].T.to_csv(\"../../data/TFIDF/P2_IFIDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFIDF.sort_values(by='TFIDF3', ascending=False, axis=1, inplace=True)\n",
    "IFIDF.loc['TFIDF3'].T.to_csv(\"../../data/TFIDF/P3_IFIDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFIDF.sort_values(by='TFIDF4', ascending=False, axis=1, inplace=True)\n",
    "IFIDF.loc['TFIDF4'].T.to_csv(\"../../data/TFIDF/P4_IFIDF.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
