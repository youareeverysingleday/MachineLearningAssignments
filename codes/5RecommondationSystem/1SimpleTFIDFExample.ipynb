{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单TF-IDF示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义数据和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m没有使用标点符号。\n",
    "docA = \"The cat sat on my bed\"\n",
    "docB = \"The dog sat on my kness\"\n",
    "\n",
    "# 先分词。生成词袋。\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "\n",
    "# 构建完整词库，取并集。\n",
    "wordSet = set(bowA).union(set(bowB)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 进行词数统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>sat</th>\n",
       "      <th>dog</th>\n",
       "      <th>kness</th>\n",
       "      <th>The</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  sat  dog  kness  The  my  on  bed\n",
       "0    1    1    0      0    1   1   1    1\n",
       "1    0    1    1      1    1   1   1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用统计字典来保存词出现的次数。\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "# 遍历文档，统计词数。\n",
    "for word in bowA:\n",
    "    wordDictA[word] += 1\n",
    "for word in bowB:\n",
    "    wordDictB[word] += 1\n",
    "\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 计算TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.16666666666666666,\n",
       " 'sat': 0.16666666666666666,\n",
       " 'dog': 0.0,\n",
       " 'kness': 0.0,\n",
       " 'The': 0.16666666666666666,\n",
       " 'my': 0.16666666666666666,\n",
       " 'on': 0.16666666666666666,\n",
       " 'bed': 0.16666666666666666}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    # 用一个字典对象记录TF\n",
    "    tfDict = {}\n",
    "    nbowCount = len(bow)\n",
    "    \n",
    "    for word,count in wordDict.items():\n",
    "        # print(word, count,)\n",
    "        tfDict[word] = count / nbowCount\n",
    "    \n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(wordDictA, bowA)\n",
    "tfB = computeTF(wordDictB, bowB)\n",
    "\n",
    "tfA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 计算逆文档频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.17609125905568124,\n",
       " 'sat': 0.0,\n",
       " 'dog': 0.17609125905568124,\n",
       " 'kness': 0.17609125905568124,\n",
       " 'The': 0.0,\n",
       " 'my': 0.0,\n",
       " 'on': 0.0,\n",
       " 'bed': 0.17609125905568124}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(wordDictList):\n",
    "    # 用一个字典对象来保存idf结果，每个词作为key，初始值为0。\n",
    "    idfDict = dict.fromkeys(wordDictList[0], 0)\n",
    "    N = len(wordDictList)\n",
    "    \n",
    "    for wordDict in wordDictList:\n",
    "        # 遍历字典中的每个词汇，统计Ni\n",
    "        for word,count in wordDict.items():\n",
    "            if count > 0 :\n",
    "                # 先把Ni增加1，存入到idfDict\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    # 已经得到所有词汇i对应的Ni，现在更加公式把它替换为idf值。\n",
    "    for word, ni in idfDict.items():\n",
    "        idfDict[word] = math.log10((N+1)/(ni+1))\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 计算TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>sat</th>\n",
       "      <th>dog</th>\n",
       "      <th>kness</th>\n",
       "      <th>The</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat  sat       dog     kness  The   my   on       bed\n",
       "0  0.029349  0.0  0.000000  0.000000  0.0  0.0  0.0  0.029349\n",
       "1  0.000000  0.0  0.029349  0.029349  0.0  0.0  0.0  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tf, idfs):\n",
    "    tfidf = {}\n",
    "    for word, tfval in tf.items():\n",
    "        tfidf[word] = tfval * idfs[word]\n",
    "        \n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个相对复杂的例子\n",
    "\n",
    "1. 几个文件PDF导出成txt的时候格式还都不一样。wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于第一个字符如果是特殊字符的行不进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipCharList = ['\\n', '\\r', '\\t', ' ', '.', ':']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 设置总词库\n",
    "   1. 列名是单词。\n",
    "   2. 行名是不同的文章的名称或者对应的index。\n",
    "   3. 目前是四篇paper，创建的词袋是4行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordBag = pd.DataFrame([[],[],[],[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordBag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 将词添加如总词袋中，并且对出现的次数进行记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddWordToWordBag(paperName, word):\n",
    "    # WordList = WordBag.columns.to_list()\n",
    "    if word in WordBag.columns.to_list():\n",
    "        WordBag.loc[paperName, word] += 1\n",
    "    else:\n",
    "        WordBag[word] = 0\n",
    "        WordBag.loc[paperName, word] = 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   attention  NaN  all  is  you  need\n",
      "0          0    0    0   0    0     0\n",
      "1          1    3    1   1    1     1\n",
      "2          0    0    0   0    0     0\n",
      "3          0    0    0   0    0     0\n"
     ]
    }
   ],
   "source": [
    "# 测试函数\n",
    "\n",
    "# words = ['attention', 'NaN', 'all', 'is', 'you', 'need', 'NaN', 'NaN']\n",
    "\n",
    "# for word in words:\n",
    "#     AddWordToWordBag(1, word)\n",
    "    \n",
    "# print(WordBag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第一个文档进行处理。\n",
    "\n",
    "1. 在PDF导出为txt的时候就将所有的文字分为了逐个的单词，而且每个单词占一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5632/3434580699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# 对记录缓存进行遍历。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPaper1WordList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-zA-Z]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-zA-Z]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;31m# 修改对应单词的计数。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5487\u001b[0m         \u001b[1;31m# the same attribute.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5489\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5490\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 对第一篇论文进行处理\n",
    "\n",
    "Paper1WordList = pd.DataFrame(columns=[\"word\", \"count\", \"TF\", \"IDF\"])\n",
    "Paper1WordList['word'] = Paper1WordList['word'].astype(np.str)\n",
    "Paper1WordList['count'] = Paper1WordList['count'].astype(np.int)\n",
    "Paper1WordList['TF'] = Paper1WordList['TF'].astype(np.float)\n",
    "Paper1WordList['IDF'] = Paper1WordList['IDF'].astype(np.float)\n",
    "\n",
    "# 4个文本的路径。\n",
    "PaperPathList = [\"../../data/TFIDF/1.txt\", \n",
    "                 \"../../data/TFIDF/2.txt\", \n",
    "                 \"../../data/TFIDF/3.txt\", \n",
    "                 \"../../data/TFIDF/4.txt\"]\n",
    "# 逐行读取\n",
    "with open(\"../../data/TFIDF/1.txt\") as f1:\n",
    "    line = f1.readline()\n",
    "    line = line\n",
    "    \n",
    "    while line:\n",
    "        # 用于记录暂存下来查找过程中的循环次数。\n",
    "        i = 0\n",
    "        # 是否找到的标签。\n",
    "        flag = False\n",
    "        # 存储作为是否遍历完成的判断数。\n",
    "        rowNumber = Paper1WordList.shape[0]\n",
    "        # print(\"rowNumber: {}\".format(rowNumber))\n",
    "        \n",
    "        if line in skipCharList:\n",
    "            line = f1.readline()  #读取一行文件，包括换行符\n",
    "            line = line\n",
    "            continue\n",
    "\n",
    "        # 对记录缓存进行遍历。\n",
    "        for index,row in Paper1WordList.iterrows():\n",
    "            if re.sub(r'[^a-zA-Z]', '', row['word']).casefold() == re.sub(r'[^a-zA-Z]', '', line).casefold():\n",
    "                # 修改对应单词的计数。\n",
    "                Paper1WordList.loc[index,'count'] += 1\n",
    "                # 找了结束遍历，并且将标识设置为True。\n",
    "                flag = True\n",
    "                break\n",
    "            # 统计循环次数。\n",
    "            i += 1\n",
    "        \n",
    "        # print(\"i: {0} rowNumber: {1}\".format(i, rowNumber))\n",
    "        # 由循环次数是否等于总的缓存行数和是否没有找到作为判断条件来添加新的单词。\n",
    "        if i == rowNumber and flag == False:\n",
    "            Paper1WordList = Paper1WordList.append({'word':re.sub(r'[^a-zA-Z]', '', line).casefold(), 'count':1}, ignore_index=True)\n",
    "            # print(\"Append\")\n",
    "        # 对标识使用完毕并初始化。\n",
    "        flag = False\n",
    "        \n",
    "        # 读取下一行。\n",
    "        line = f1.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # print(\"-----------\")\n",
    "        # print(Paper1WordList)\n",
    "        # print(\"-----------------------------------\")\n",
    "        \n",
    "print(Paper1WordList)\n",
    "# Paper1.to_csv(\"1.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第二个文本进行处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word  count  TF  IDF\n",
      "0     contextualized      1 NaN  NaN\n",
      "1    pointofinterest      2 NaN  NaN\n",
      "2     recommendation      6 NaN  NaN\n",
      "3                        41 NaN  NaN\n",
      "4               peng      1 NaN  NaN\n",
      "..               ...    ...  ..  ...\n",
      "198             time      1 NaN  NaN\n",
      "199            equal      1 NaN  NaN\n",
      "200     contribution      1 NaN  NaN\n",
      "201   ycorresponding      1 NaN  NaN\n",
      "202           author      1 NaN  NaN\n",
      "\n",
      "[203 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Paper2WordList = pd.DataFrame(columns=[\"word\", \"count\", \"TF\", \"IDF\"])\n",
    "# Paper2WordList['word'] = Paper2WordList['word'].astype(np.str)\n",
    "# Paper2WordList['count'] = Paper2WordList['count'].astype(np.int)\n",
    "# Paper2WordList['TF'] = Paper2WordList['TF'].astype(np.float)\n",
    "# Paper2WordList['IDF'] = Paper2WordList['IDF'].astype(np.float)\n",
    "\n",
    "# with open(\"../../data/TFIDF/2t.txt\") as f2:\n",
    "#     line = f2.readline()\n",
    "#     line = line\n",
    "#     while line:        \n",
    "#         # 如果存在“|”字符，那么使用“|”对字符串进行分词。不然就用\".\"对字符串进行分词。\n",
    "#         if \"|\" in line:\n",
    "#             lineWords = line.split(\"|\")\n",
    "#         else:\n",
    "#             lineWords = line.split(\".\")\n",
    "#         # print(lineWords)\n",
    "        \n",
    "#         if line in skipCharList:\n",
    "#             line = f2.readline()  #读取一行文件，包括换行符\n",
    "#             line = line\n",
    "#             continue\n",
    "        \n",
    "#         for word in lineWords:\n",
    "#             # 用于记录暂存下来查找过程中的循环次数。\n",
    "#             i = 0\n",
    "#             # 是否找到的标签。\n",
    "#             flag = False\n",
    "#             # 存储作为是否遍历完成的判断数。\n",
    "#             rowNumber = Paper2WordList.shape[0]\n",
    "#             for index,row in Paper2WordList.iterrows():\n",
    "#                 if re.sub(r'[^a-zA-Z]', '', row['word']).casefold() == re.sub(r'[^a-zA-Z]', '', word).casefold():\n",
    "#                     Paper2WordList.loc[index,'count'] += 1\n",
    "#                     # 找了结束遍历，并且将标识设置为True。\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#                 # 统计循环次数。\n",
    "#                 i += 1\n",
    "#             # print(i, rowNumber)\n",
    "#             if i == rowNumber and flag == False:\n",
    "#                 # print(word)\n",
    "#                 Paper2WordList = Paper2WordList.append({'word':re.sub(r'[^a-zA-Z]', '', word).casefold(), 'count':1}, ignore_index=True)\n",
    "#             # print(\"Append\")\n",
    "#             # 对标识使用完毕并初始化。\n",
    "#             flag = False\n",
    "#             i = 0\n",
    "\n",
    "#         line = f2.readline()  #读取一行文件，包括换行符\n",
    "#         line = line\n",
    "#         # print(\"--------------------------------------\")\n",
    "\n",
    "# print(Paper2WordList)\n",
    "# # Paper2WordList.to_csv(\"../../data/TFIDF/2t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/TFIDF/2t.txt\") as f2:\n",
    "    line = f2.readline()\n",
    "    line = line\n",
    "    while line:        \n",
    "        # 如果存在“|”字符，那么使用“|”对字符串进行分词。不然就用\".\"对字符串进行分词。\n",
    "        if \"|\" in line:\n",
    "            lineWords = line.split(\"|\")\n",
    "        else:\n",
    "            lineWords = line.split(\".\")\n",
    "        # print(lineWords)\n",
    "        \n",
    "        if line in skipCharList:\n",
    "            line = f2.readline()  #读取一行文件，包括换行符\n",
    "            line = line\n",
    "            continue\n",
    "        \n",
    "        for word in lineWords:\n",
    "            AddWordToWordBag(3-1, re.sub(r'[^a-zA-Z]', '', word).casefold())\n",
    "\n",
    "        line = f2.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(WordBag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第三个文本进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper3WordList = pd.DataFrame(columns=[\"word\", \"count\", \"TF\", \"IDF\"])\n",
    "# Paper3WordList['word'] = Paper3WordList['word'].astype(np.str)\n",
    "# Paper3WordList['count'] = Paper3WordList['count'].astype(np.int)\n",
    "# Paper3WordList['TF'] = Paper3WordList['TF'].astype(np.float)\n",
    "# Paper3WordList['IDF'] = Paper3WordList['IDF'].astype(np.float)\n",
    "\n",
    "# with open(\"../../data/TFIDF/3t.txt\") as f3:\n",
    "#     line = f3.readline()\n",
    "#     line = line\n",
    "#     while line: \n",
    "#         # 用\" \"对字符串进行分词。\n",
    "#         lineWords = line.split(\" \")\n",
    "        \n",
    "#         if line in skipCharList:\n",
    "#             line = f3.readline()  #读取一行文件，包括换行符\n",
    "#             line = line\n",
    "#             continue\n",
    "            \n",
    "#         # print(lineWords)\n",
    "#         for word in lineWords:\n",
    "#             # 用于记录暂存下来查找过程中的循环次数。\n",
    "#             i = 0\n",
    "#             # 是否找到的标签。\n",
    "#             flag = False\n",
    "#             # 存储作为是否遍历完成的判断数。\n",
    "#             rowNumber = Paper3WordList.shape[0]\n",
    "            \n",
    "#             AddWordToWordBag(3, re.sub(r'[^a-zA-Z]', '', row['word']).casefold())\n",
    "#             for index,row in Paper3WordList.iterrows():\n",
    "#                 if re.sub(r'[^a-zA-Z]', '', row['word']).casefold() == re.sub(r'[^a-zA-Z]', '', word).casefold():\n",
    "#                     Paper3WordList.loc[index,'count'] += 1\n",
    "#                     # 找了结束遍历，并且将标识设置为True。\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#                 # 统计循环次数。\n",
    "#                 i += 1\n",
    "#             # print(i, rowNumber)\n",
    "#             if i == rowNumber and flag == False:\n",
    "#                 # print(word)\n",
    "#                 Paper3WordList = Paper3WordList.append({'word':re.sub(r'[^a-zA-Z]', '', word).casefold(), 'count':1}, ignore_index=True)\n",
    "#             # print(\"Append\")\n",
    "#             # 对标识使用完毕并初始化。\n",
    "#             flag = False\n",
    "#             i = 0\n",
    "\n",
    "#         line = f3.readline()  #读取一行文件，包括换行符\n",
    "#         line = line\n",
    "#         # line = line[:-1]\n",
    "#         # print(\"--------------------------------------\")\n",
    "\n",
    "# print(Paper3WordList)\n",
    "# # Paper3WordList.to_csv(\"../../data/TFIDF/3t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   attention  is  all  you  need      arxivv  cscl  dec  ashish  ...  zwork  \\\n",
      "0          0   0    0    0     0   0       0     0    0       0  ...      0   \n",
      "1          0   0    0    0     0   0       0     0    0       0  ...      0   \n",
      "2          0   0    0    0     0   0       0     0    0       0  ...      0   \n",
      "3          5   2    1    1     1  69       1     1    1       2  ...      1   \n",
      "\n",
      "   st  conference  information  processing  systems  nips  beach  ca  usa  \n",
      "0   0           0            0           0        0     0      0   0    0  \n",
      "1   0           0            0           0        0     0      0   0    0  \n",
      "2   0           0            0           0        0     0      0   0    0  \n",
      "3   1           1            1           1        1     1      1   1    1  \n",
      "\n",
      "[4 rows x 232 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/TFIDF/3t.txt\") as f3:\n",
    "    line = f3.readline()\n",
    "    line = line\n",
    "    while line: \n",
    "        # 用\" \"对字符串进行分词。\n",
    "        lineWords = line.split(\" \")\n",
    "        \n",
    "        if line in skipCharList:\n",
    "            line = f3.readline()  #读取一行文件，包括换行符\n",
    "            line = line\n",
    "            continue\n",
    "            \n",
    "        # print(lineWords)\n",
    "        for word in lineWords:\n",
    "            AddWordToWordBag(3-1, re.sub(r'[^a-zA-Z]', '', word).casefold())\n",
    "\n",
    "        line = f3.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # line = line[:-1]\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(WordBag)\n",
    "# WordBag.to_csv(\"../../data/TFIDF/wb3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.base.Index'>\n",
      "['word', 'count', 'TF', 'IDF']\n"
     ]
    }
   ],
   "source": [
    "zz = Paper3WordList.columns\n",
    "print(type(zz))\n",
    "print(zz.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对第四个文本进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word  count  TF  IDF\n",
      "0           learning      2 NaN  NaN\n",
      "1                        69 NaN  NaN\n",
      "2         graphbased      2 NaN  NaN\n",
      "3                poi      1 NaN  NaN\n",
      "4          embedding      5 NaN  NaN\n",
      "..               ...    ...  ..  ...\n",
      "366               li      1 NaN  NaN\n",
      "367             lore      1 NaN  NaN\n",
      "368       exploiting      1 NaN  NaN\n",
      "369  recommendations      1 NaN  NaN\n",
      "370       sigspatial      1 NaN  NaN\n",
      "\n",
      "[371 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "Paper4WordList = pd.DataFrame(columns=[\"word\", \"count\", \"TF\", \"IDF\"])\n",
    "Paper4WordList['word'] = Paper4WordList['word'].astype(np.str)\n",
    "Paper4WordList['count'] = Paper4WordList['count'].astype(np.int)\n",
    "Paper4WordList['TF'] = Paper4WordList['TF'].astype(np.float)\n",
    "Paper4WordList['IDF'] = Paper4WordList['IDF'].astype(np.float)\n",
    "\n",
    "with open(\"../../data/TFIDF/4t.txt\") as f4:\n",
    "    line = f4.readline()\n",
    "    line = line\n",
    "    while line: \n",
    "        # 用\" \"对字符串进行分词。\n",
    "        lineWords = line.split(\" \")\n",
    "        \n",
    "        if line in skipCharList:\n",
    "            line = f4.readline()  #读取一行文件，包括换行符\n",
    "            line = line\n",
    "            continue\n",
    "            \n",
    "        # print(lineWords)\n",
    "        for word in lineWords:\n",
    "            # 用于记录暂存下来查找过程中的循环次数。\n",
    "            i = 0\n",
    "            # 是否找到的标签。\n",
    "            flag = False\n",
    "            # 存储作为是否遍历完成的判断数。\n",
    "            rowNumber = Paper4WordList.shape[0]\n",
    "            for index,row in Paper4WordList.iterrows():\n",
    "                if re.sub(r'[^a-zA-Z]', '', row['word']).casefold() == re.sub(r'[^a-zA-Z]', '', word).casefold():\n",
    "                    Paper4WordList.loc[index,'count'] += 1\n",
    "                    # 找了结束遍历，并且将标识设置为True。\n",
    "                    flag = True\n",
    "                    break\n",
    "                # 统计循环次数。\n",
    "                i += 1\n",
    "            # print(i, rowNumber)\n",
    "            if i == rowNumber and flag == False:\n",
    "                # print(word)\n",
    "                Paper4WordList = Paper4WordList.append({'word':re.sub(r'[^a-zA-Z]', '', word).casefold(), 'count':1}, ignore_index=True)\n",
    "            # print(\"Append\")\n",
    "            # 对标识使用完毕并初始化。\n",
    "            flag = False\n",
    "            i = 0\n",
    "\n",
    "        line = f4.readline()  #读取一行文件，包括换行符\n",
    "        line = line\n",
    "        # line = line[:-1]\n",
    "        # print(\"--------------------------------------\")\n",
    "\n",
    "print(Paper4WordList)\n",
    "Paper4WordList.to_csv(\"../../data/TFIDF/4t.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算TF\n",
    "\n",
    "1. TF为单词出现的个数除以总的单词类别数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateTF(ariseCount, totalWordCategoryCount):\n",
    "    return ariseCount/totalWordCategoryCount\n",
    "\n",
    "def CalculateIDF():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
