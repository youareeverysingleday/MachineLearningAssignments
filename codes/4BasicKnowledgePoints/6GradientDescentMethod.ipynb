{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement Gradient Descent Method\n",
    "\n",
    "1. the focus is achieving random gradient descent method.\n",
    "2. 在矩阵因式分解的过程中，为了是得目标函数求得极值，需要将其导数等于0，来找到极值点。\n",
    "3. 为了解决在矩阵分解中两个（或者多个矩阵）同时变化的情况，这里使用的是交替最小二乘法来完成梯度下降的计算。交替最小二乘法ALS的核心思想是：\n",
    "   1. 从多个变量，选定一个变量；\n",
    "   2. 其他的变量都固定住（可以初始化一个值，这个初始化值非常有技术含量，初始化值可以通过多次随机来避免陷入局部最优解的情况），\n",
    "   3. 然后对选定的变量的进行最小二乘法；\n",
    "   4. 然后将该选定的变量进行最小二乘法之后的结果固定住；\n",
    "   5. 再从余下的变量中再选一个变量；\n",
    "   6. 重复2-5步骤，直到达到目标函数的要求。\n",
    "4. 这里为什么要用梯度下降来求矩阵分解？是因为在3中需要求矩阵的逆。一般认为矩阵的逆是不好计算的。所以通过梯度下降来迭代求出矩阵的分解因子。这样就不需要目标函数的导数等于0了，只需要求偏导数乘以步长，来进行迭代即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "# 为0的地方并不是评分为0，而是用户并没有对该物品进行评价。没有评分的地方并不用考虑它的误差。\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "输入\n",
    "R 是m*n的评分矩阵\n",
    "K 隐性特征向量维度。注意这个特征维度是人为定义的。但是按道理这不应该有人为定义。\n",
    "steps/max_iter 最大迭代步长\n",
    "alpha 步长\n",
    "lamda 正则化系数\n",
    "\n",
    "输出\n",
    "分解之后的P和Q\n",
    "P 初始化用户特征矩阵m*k\n",
    "Q 初始化物品特征矩阵k*n\n",
    "\"\"\"\n",
    "\n",
    "# 对超参数进行赋值\n",
    "K=2\n",
    "max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "alpha = 0.0002\n",
    "lamda = 0.004\n",
    "\n",
    "def grad(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "    m = len(R)\n",
    "    n = len(R[0])\n",
    "    \n",
    "    P = np.random.rand(m, K)\n",
    "    Q = np.random.rand(K, n)\n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        # 对所有的用户u和物品i做遍历。对对应的Pu和Qi向量进行梯度下降。\n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                # 对于每一个大于0的评分，求出评分误差。\n",
    "                if R[u][i] > 0:\n",
    "                    eui = np.dot(P[u, :],Q[:, i]) - R[u,i]\n",
    "                    \n",
    "                    # 带入梯度下降的公式，按照梯度下降算法更新当前的Pu和Qi。也就是按照K个隐藏维度来更新。\n",
    "                    for k in range(K):\n",
    "                        # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "                        # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "                        # 用再求和了。\n",
    "                        P[u][k] = P[u][k] - alpha * (2 * eui * Q[k][i] - 2 * lamda * P[u][k])\n",
    "                        # 同样的\n",
    "                        Q[k][i] = Q[k][i] - alpha * (2 * eui * P[u][k] - 2 * lamda * Q[k][i])\n",
    "                \n",
    "        # u和i遍历完成。所有特征向量都更新完成。可以计算预测评分矩阵。\n",
    "        # predictR = np.dot(P, Q)\n",
    "        # 计算当前的损失函数。\n",
    "        cost = 0\n",
    "        \n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                # 在评分矩阵R中为0的不计算损失函数，原因依然是为0的评分可能是用户没有评分。\n",
    "                if R[u][i] > 0:\n",
    "                    cost += (np.dot(P[u, :],Q[:, i]) - R[u,i]) ** 2\n",
    "                    for k in range(K):\n",
    "                        cost += lamda * (P[u][k] ** 2 + Q[k][i] ** 2)\n",
    "        # 当损失函数小于某一个特定阈值时退出。\n",
    "        if cost < cost_threshold:\n",
    "            break\n",
    "    return P, Q, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin R is [[4 0 1 0 5]\n",
      " [1 2 1 3 5]\n",
      " [4 5 3 1 0]\n",
      " [2 3 0 2 5]\n",
      " [5 1 4 0 0]\n",
      " [0 3 2 4 1]], \n",
      "\n",
      " predict Matrix is [[ 3.08530702  1.92869752  2.11718031  3.54010476  5.13398751]\n",
      " [ 2.09603817  0.87096131  1.44174417  2.88328844  4.719205  ]\n",
      " [ 3.86158274  5.07022909  2.62923045  1.53899963 -1.01951784]\n",
      " [ 2.84000453  1.96195705  1.94740031  3.05549172  4.20276956]\n",
      " [ 4.67550674  1.81305316  3.21702057  6.57283231 10.89051844]\n",
      " [ 3.33665781  3.41411086  2.27933898  2.38243145  1.82918127]], \n",
      "\n",
      "  User matrix is [[ 1.61932674  0.59191702]\n",
      " [ 1.43891851  0.13441167]\n",
      " [-0.02179166  2.35952158]\n",
      " [ 1.34666796  0.65856847]\n",
      " [ 3.30977281  0.2207581 ]\n",
      " [ 0.72685412  1.44957194]], \n",
      "\n",
      " Item matrix is [[ 1.30267523  0.40421375  0.89710065  1.94118608  3.31718897]\n",
      " [ 1.64862666  2.15257094  1.12259188  0.67017878 -0.40145036]], \n",
      "\n",
      " Cost is 14.632448538433511\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P, Q, cost = grad(R, K, max_iter, alpha, lamda)\n",
    "predictR = np.dot(P, Q)\n",
    "print(\"origin R is {}, \\n\\n predict Matrix is {}, \\n\\n  User matrix is {}, \\n\\n Item matrix is {}, \\n\\n Cost is {}\\n\\n\".format(R, predictR, P, Q, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin R is [[4 0 1 0 5]\n",
      " [1 2 1 3 5]\n",
      " [4 5 3 1 0]\n",
      " [2 3 0 2 5]\n",
      " [5 1 4 0 0]\n",
      " [0 3 2 4 1]], \n",
      "\n",
      " predict Matrix is [[3.98129334 2.88772337 1.03185416 2.2871385  5.00714545]\n",
      " [1.00423263 2.00907678 0.98400247 3.00982762 5.00561081]\n",
      " [4.02301129 4.99966975 2.99214759 0.99586013 8.17229013]\n",
      " [1.98796753 3.02288686 2.15911716 2.00039133 5.00294678]\n",
      " [5.01446274 1.00394671 3.99722158 3.87058576 1.44459909]\n",
      " [2.49623367 2.98782827 2.00730408 4.0123347  1.0034288 ]], \n",
      "\n",
      "  User matrix is [[ 0.77263191  0.39894275  0.44141781  0.83615125  1.64913806]\n",
      " [ 1.87732493  0.30978616  0.10762536  0.35761834  0.20940714]\n",
      " [ 0.76522684  1.97498457 -0.14383774  1.6699223   0.87418461]\n",
      " [ 1.08084512  1.22845156  0.21445352  0.66658305  0.17889472]\n",
      " [ 0.10747017  0.5456395   2.46119379  0.78410461  0.32887995]\n",
      " [ 0.70083521  1.35408608  1.35217766 -0.8015405   0.66165732]], \n",
      "\n",
      " Item matrix is [[-0.00741578  0.6596376   0.17590694  1.51469259  2.00592624]\n",
      " [ 0.54959653  1.66837407  0.97702877  0.18853392  1.10260378]\n",
      " [ 1.36069905 -0.18171945  1.17516206  1.53887665 -0.55154933]\n",
      " [ 1.19028518  0.18216839  0.8996725  -0.41273327  2.16174404]\n",
      " [ 1.31697296  0.99468595 -0.46377891  0.42897822  0.88127599]], \n",
      "\n",
      " Cost is 0.9973872907468129\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过上面的结果观察，评分矩阵和实际的评分直接误差有点大。\n",
    "# 这个时候思考可能分解的维度可能太低了，测试提高分解维度来观察结果。\n",
    "K= 5\n",
    "P, Q, cost = grad(R, K, max_iter, alpha, lamda)\n",
    "predictR = np.dot(P, Q)\n",
    "print(\"origin R is {}, \\n\\n predict Matrix is {}, \\n\\n  User matrix is {}, \\n\\n Item matrix is {}, \\n\\n Cost is {}\\n\\n\".format(R, predictR, P, Q, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tensorflow来实现特征值分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "R_square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新用矩阵的思维来写特征值分解。感觉以前写的有点问题。\n",
    "\n",
    "\n",
    "用tensor来实现。用tensorflow来实现完成。但是还不是用的矩阵思维。还是有问题。主要是对算法理解还有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress is 0.0\n",
      "Progress is 1.0\n",
      "Progress is 2.0\n",
      "Progress is 3.0\n",
      "Progress is 4.0\n",
      "Progress is 5.0\n",
      "Progress is 6.0\n",
      "Progress is 7.0\n",
      "Progress is 8.0\n",
      "Progress is 9.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "K=2\n",
    "max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "alpha = 0.0002\n",
    "lamda = 0.004\n",
    "\n",
    "def GDM_Tensor(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "    m = len(R)\n",
    "    n = len(R[0])\n",
    "\n",
    "    R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "    P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "    Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "    # print(\"Display P.shape={} and  Q.shape={}\".format(P.shape, Q.shape))\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        if step%100 == 0:\n",
    "            print(\"Progress is {}\".format(step/100 + 1))\n",
    "        # 按逻辑对于用户必须逐行更新，对于item必须逐列更新。\n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                if R[u][i] > 0:\n",
    "                    # print(\"Display P[u, :].shape={} and  Q[:, i].shape={}\".format(P[u, :].shape, Q[:, i].shape))\n",
    "                    # eui = tf.matmul(P[u, :], Q[:, i]) - R[u,i]\n",
    "                    eui = (tf.matmul(tf.reshape(P[u, :], [1,K]), tf.reshape(Q[:, i], [K,1])) - R[u,i])[0,0]\n",
    "                    # print(eui)\n",
    "                    # print(type(eui))\n",
    "                    for k in range(K):\n",
    "                        # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "                        # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "                        # 用再求和了。\n",
    "                        P[u, k].assign(P[u, k] - alpha * (2 * eui * Q[k, i] - 2 * lamda * P[u, k]))\n",
    "                        Q[k, i].assign(Q[k, i] - alpha * (2 * eui * P[u, k] - 2 * lamda * Q[k, i]))\n",
    "                        break\n",
    "                    \n",
    "        # print(\"Cycle end.\")\n",
    "        # 这个是收敛条件之一。\n",
    "        cost = tf.Variable(0.0)\n",
    "        Eui_matrix = tf.matmul(P, Q) - R_tf\n",
    "        cost = tf.reduce_sum(tf.reduce_sum(tf.square(Eui_matrix))) + lamda * (tf.reduce_sum(tf.reduce_sum(tf.square(P), 0)) + tf.reduce_sum(tf.reduce_sum(tf.square(Q), 0)))\n",
    "        \n",
    "        if cost <= cost_threshold:\n",
    "            break\n",
    "    \n",
    "    return P, Q, cost\n",
    "\n",
    "P, Q, cost = GDM_Tensor(R_square, K=2, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 2) dtype=float32, numpy=\n",
      "array([[1.5650222 , 0.4134315 ],\n",
      "       [1.3195595 , 0.64713115],\n",
      "       [2.619709  , 0.37340465],\n",
      "       [1.4728941 , 0.84225214],\n",
      "       [2.181799  , 0.8092877 ]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 5) dtype=float32, numpy=\n",
      "array([[1.5793083 , 1.3907872 , 1.1425058 , 0.84974104, 3.2946165 ],\n",
      "       [0.6499688 , 0.14561683, 0.34822193, 0.11311472, 0.2997057 ]],\n",
      "      dtype=float32)>\n",
      "tf.Tensor(167.67068, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "print(Q)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用tensorflow来实现这个函数.\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def ModifyTensor(input_tensor, position=None, value=None):\n",
    "#     input_tensor = input_tensor.numpy()\n",
    "#     input_tensor[tuple(position)] = value\n",
    "#     return input_tensor\n",
    "\n",
    "# K=2\n",
    "# max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "# alpha = 0.0002\n",
    "# lamda = 0.004\n",
    "\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "# def grad_tf(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "#     R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "#     m = len(R)\n",
    "#     n = len(R[0])\n",
    "    \n",
    "#     # P = np.random.rand(m, K)\n",
    "#     # Q = np.random.rand(K, n)\n",
    "#     P = tf.Variable(tf.zeros([m, K], dtype=float))\n",
    "#     Q = tf.Variable(tf.zeros([K, n], dtype=float))\n",
    "    \n",
    "#     for step in range(max_iter):\n",
    "#         # 对所有的用户u和物品i做遍历。对对应的Pu和Qi向量进行梯度下降。\n",
    "#         for u in range(m):\n",
    "#             for i in range(n):\n",
    "#                 # 对于每一个大于0的评分，求出评分误差。\n",
    "#                 if R_tf[u][i] > 0:\n",
    "#                     eui = tf.matmul(tf.reshape(P[u, :], [1, -1]),tf.reshape(Q[:, i], [-1, 1])) - R_tf[u,i]\n",
    "                    \n",
    "#                     # 带入梯度下降的公式，按照梯度下降算法更新当前的Pu和Qi。也就是按照K个隐藏维度来更新。\n",
    "#                     for k in range(K):\n",
    "#                         # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "#                         # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "#                         # 用再求和了。\n",
    "#                         P = tf.py_function(ModifyTensor, \n",
    "#                                            inp=[P, [u, k], P[u][k] - alpha * (2 * eui * Q[k][i] - 2 * lamda * P[u][k])], \n",
    "#                                            Tout=P.dtype)\n",
    "#                         # 同样的\n",
    "#                         Q = tf.py_function(ModifyTensor, \n",
    "#                                            inp=[Q, [k, i], Q[k][i] - alpha * (2 * eui * P[u][k] - 2 * lamda * Q[k][i])], \n",
    "#                                            Tout=Q.dtype)\n",
    "                \n",
    "#         # u和i遍历完成。所有特征向量都更新完成。可以计算预测评分矩阵。\n",
    "#         # predictR = np.dot(P, Q)\n",
    "#         # 计算当前的损失函数。\n",
    "#         cost = 0\n",
    "        \n",
    "#         for u in range(m):\n",
    "#             for i in range(n):\n",
    "#                 # 在评分矩阵R_tf中为0的不计算损失函数，原因依然是为0的评分可能是用户没有评分。\n",
    "#                 if R_tf[u][i] > 0:\n",
    "#                     cost += (tf.matmul(tf.reshape(P[u, :], [1, -1]),tf.reshape(Q[:, i], [-1, 1])) - R_tf[u,i]) ** 2\n",
    "#                     for k in range(K):\n",
    "#                         cost += lamda * (tf.square(P[u][k]) + tf.square(Q[k][i]))\n",
    "#         # 当损失函数小于某一个特定阈值时退出。\n",
    "#         if cost < cost_threshold:\n",
    "#             break\n",
    "#     return P, Q, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_6888/2097229772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# P, Q, cost = grad_tf(R, K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Eig in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# R_tf = tf.convert_to_tensor(R_square, dtype=float)\n",
    "# # tensorflow进行eig必须是方阵。\n",
    "# # \n",
    "# M, N = tf.eig(R_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.5489885-1.8442735e-08j -1.7649044-1.9651887e+00j\n",
      " -1.7649044+1.9651892e+00j  2.9616973-3.0399860e-08j\n",
      " 11.019123 +6.4537744e-09j], shape=(5,), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.50880915+0.00434304j  0.28622308-0.30314255j -0.38530356+0.15924889j\n",
      "  -0.56747574+0.20958522j  0.35919613-0.00087918j]\n",
      " [ 0.20886296-0.00178275j  0.05577823-0.29754078j -0.29786214-0.05403553j\n",
      "   0.42365906-0.15646952j  0.4714589 -0.00115396j]\n",
      " [ 0.61511016-0.0052503j  -0.1284342 +0.6106717j   0.61630976+0.09786545j\n",
      "   0.6015964 -0.2221869j   0.530147  -0.0012976j ]\n",
      " [-0.5167199 +0.00441057j  0.38827828+0.04287843j -0.09845588+0.37802777j\n",
      "   0.12856518-0.04748289j  0.4572356 -0.00111915j]\n",
      " [ 0.22815934-0.00194746j -0.42346954+0.11488671j  0.25839487-0.35462296j\n",
      "  -0.00247677+0.00091478j  0.39821923-0.00097469j]], shape=(5, 5), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# x = tf.constant([[1,2], [3,4]])\n",
    "# y = tf.add(x, 1)\n",
    "# print(y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
