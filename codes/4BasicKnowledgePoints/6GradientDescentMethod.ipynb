{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement Gradient Descent Method\n",
    "\n",
    "1. the focus is achieving random gradient descent method.\n",
    "2. 在矩阵因式分解的过程中，为了是得目标函数求得极值，需要将其导数等于0，来找到极值点。\n",
    "3. 为了解决在矩阵分解中两个（或者多个矩阵）同时变化的情况，这里使用的是交替最小二乘法来完成梯度下降的计算。交替最小二乘法ALS的核心思想是：\n",
    "   1. 从多个变量，选定一个变量；\n",
    "   2. 其他的变量都固定住（可以初始化一个值，这个初始化值非常有技术含量，初始化值可以通过多次随机来避免陷入局部最优解的情况），\n",
    "   3. 然后对选定的变量的进行最小二乘法；\n",
    "   4. 然后将该选定的变量进行最小二乘法之后的结果固定住；\n",
    "   5. 再从余下的变量中再选一个变量；\n",
    "   6. 重复2-5步骤，直到达到目标函数的要求。\n",
    "4. 这里为什么要用梯度下降来求矩阵分解？是因为在3中需要求矩阵的逆。一般认为矩阵的逆是不好计算的。所以通过梯度下降来迭代求出矩阵的分解因子。这样就不需要目标函数的导数等于0了，只需要求偏导数乘以步长，来进行迭代即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面的方法实现过程中非常的混乱。这里按照自己的思路来实现一遍。\n",
    "1. 先对二维数据进行来进行实现。\n",
    "2. 使用tf来实现数据的存储和使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "data_set = []\n",
    "for i in range(num_points):\n",
    "    x = np.random.normal(0.0, 0.55)\n",
    "    # 设置一定范围的浮动\n",
    "    y = x*0.1 + 0.3 + np.random.normal(0.0,0.03)\n",
    "    data_set.append([x, y])\n",
    "\n",
    "data_np = np.array(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_np[:,0], data_np[:,1], c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = tf.Variable(data_np, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考使用tensorflow求导来对线性回归拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9817748> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0545703>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # tf为2.0版本 python版本为3.6\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "# 定义原数据\n",
    "X_raw = np.array([2013,2014,2015,2016,2017],dtype=np.float32)\n",
    "Y_raw = np.array([12000,14000,15000,16500,17500],dtype=np.float32)\n",
    "\n",
    "# 数据归一化\n",
    "x = (X_raw - X_raw.min())/(X_raw.max() - X_raw.min())\n",
    "y = (Y_raw - Y_raw.min())/(Y_raw.max() - Y_raw.min())\n",
    "# 定义张量\n",
    "X = tf.constant(x)\n",
    "y = tf.constant(y)\n",
    "# 定义参数\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a,b]\n",
    "\n",
    "num_epoch = 10000 # 定义迭代次数\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3) # 定义优化器，用以后续更新参数\n",
    "for e in range(num_epoch): # 迭代多次，更新参数a与b\n",
    "    # 使用tf.GradientTape() 记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b \n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y)) # 通过预测值与实际值 求出误差\n",
    "    # Tensorflow 自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss,variables) # 求损失关于参数a b的梯度\n",
    "    # Tensorflow 自动根据梯度更新参数，即利用梯度信息修改a与b，使得损失减小\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,variables)) \n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientTape in module tensorflow.python.eager.backprop:\n",
      "\n",
      "class GradientTape(builtins.object)\n",
      " |  GradientTape(persistent=False, watch_accessed_variables=True)\n",
      " |  \n",
      " |  Record operations for automatic differentiation.\n",
      " |  \n",
      " |  Operations are recorded if they are executed within this context manager and\n",
      " |  at least one of their inputs is being \"watched\".\n",
      " |  \n",
      " |  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
      " |  where `trainable=True` is default in both cases) are automatically watched.\n",
      " |  Tensors can be manually watched by invoking the `watch` method on this context\n",
      " |  manager.\n",
      " |  \n",
      " |  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n",
      " |  be computed as:\n",
      " |  \n",
      " |  >>> x = tf.constant(3.0)\n",
      " |  >>> with tf.GradientTape() as g:\n",
      " |  ...   g.watch(x)\n",
      " |  ...   y = x * x\n",
      " |  >>> dy_dx = g.gradient(y, x)\n",
      " |  >>> print(dy_dx)\n",
      " |  tf.Tensor(6.0, shape=(), dtype=float32)\n",
      " |  \n",
      " |  GradientTapes can be nested to compute higher-order derivatives. For example,\n",
      " |  \n",
      " |  >>> x = tf.constant(5.0)\n",
      " |  >>> with tf.GradientTape() as g:\n",
      " |  ...   g.watch(x)\n",
      " |  ...   with tf.GradientTape() as gg:\n",
      " |  ...     gg.watch(x)\n",
      " |  ...     y = x * x\n",
      " |  ...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
      " |  >>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n",
      " |  >>> print(dy_dx)\n",
      " |  tf.Tensor(10.0, shape=(), dtype=float32)\n",
      " |  >>> print(d2y_dx2)\n",
      " |  tf.Tensor(2.0, shape=(), dtype=float32)\n",
      " |  \n",
      " |  By default, the resources held by a GradientTape are released as soon as\n",
      " |  GradientTape.gradient() method is called. To compute multiple gradients over\n",
      " |  the same computation, create a persistent gradient tape. This allows multiple\n",
      " |  calls to the gradient() method as resources are released when the tape object\n",
      " |  is garbage collected. For example:\n",
      " |  \n",
      " |  >>> x = tf.constant(3.0)\n",
      " |  >>> with tf.GradientTape(persistent=True) as g:\n",
      " |  ...   g.watch(x)\n",
      " |  ...   y = x * x\n",
      " |  ...   z = y * y\n",
      " |  >>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n",
      " |  >>> print(dz_dx)\n",
      " |  tf.Tensor(108.0, shape=(), dtype=float32)\n",
      " |  >>> dy_dx = g.gradient(y, x)\n",
      " |  >>> print(dy_dx)\n",
      " |  tf.Tensor(6.0, shape=(), dtype=float32)\n",
      " |  \n",
      " |  By default GradientTape will automatically watch any trainable variables that\n",
      " |  are accessed inside the context. If you want fine grained control over which\n",
      " |  variables are watched you can disable automatic tracking by passing\n",
      " |  `watch_accessed_variables=False` to the tape constructor:\n",
      " |  \n",
      " |  >>> x = tf.Variable(2.0)\n",
      " |  >>> w = tf.Variable(5.0)\n",
      " |  >>> with tf.GradientTape(\n",
      " |  ...     watch_accessed_variables=False, persistent=True) as tape:\n",
      " |  ...   tape.watch(x)\n",
      " |  ...   y = x ** 2  # Gradients will be available for `x`.\n",
      " |  ...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n",
      " |  >>> dy_dx = tape.gradient(y, x)\n",
      " |  >>> print(dy_dx)\n",
      " |  tf.Tensor(4.0, shape=(), dtype=float32)\n",
      " |  >>> # No gradients will be available as `w` isn't being watched.\n",
      " |  >>> dz_dy = tape.gradient(z, w)\n",
      " |  >>> print(dz_dy)\n",
      " |  None\n",
      " |  \n",
      " |  Note that when using models you should ensure that your variables exist when\n",
      " |  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n",
      " |  first iteration not have any gradients:\n",
      " |  \n",
      " |  ```python\n",
      " |  a = tf.keras.layers.Dense(32)\n",
      " |  b = tf.keras.layers.Dense(32)\n",
      " |  \n",
      " |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      " |    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
      " |                             # `a.variables` will return an empty list and the\n",
      " |                             # tape will not be watching anything.\n",
      " |    result = b(a(inputs))\n",
      " |    tape.gradient(result, a.variables)  # The result of this computation will be\n",
      " |                                        # a list of `None`s since a's variables\n",
      " |                                        # are not being watched.\n",
      " |  ```\n",
      " |  \n",
      " |  Note that only tensors with real or complex dtypes are differentiable.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enters a context inside which operations are recorded on this tape.\n",
      " |  \n",
      " |  __exit__(self, typ, value, traceback)\n",
      " |      Exits the recording context, no further operations are traced.\n",
      " |  \n",
      " |  __init__(self, persistent=False, watch_accessed_variables=True)\n",
      " |      Creates a new GradientTape.\n",
      " |      \n",
      " |      Args:\n",
      " |        persistent: Boolean controlling whether a persistent gradient tape\n",
      " |          is created. False by default, which means at most one call can\n",
      " |          be made to the gradient() method on this object.\n",
      " |        watch_accessed_variables: Boolean controlling whether the tape will\n",
      " |          automatically `watch` any (trainable) variables accessed while the tape\n",
      " |          is active. Defaults to True meaning gradients can be requested from any\n",
      " |          result computed in the tape derived from reading a trainable `Variable`.\n",
      " |          If False users must explicitly `watch` any `Variable`s they want to\n",
      " |          request gradients from.\n",
      " |  \n",
      " |  batch_jacobian(self, target, source, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n",
      " |      Computes and stacks per-example jacobians.\n",
      " |      \n",
      " |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant)\n",
      " |      for the definition of a Jacobian. This function is essentially an efficient\n",
      " |      implementation of the following:\n",
      " |      \n",
      " |      `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\n",
      " |      \n",
      " |      Note that compared to `GradientTape.jacobian` which computes gradient of\n",
      " |      each output value w.r.t each input value, this function is useful when\n",
      " |      `target[i,...]` is independent of `source[j,...]` for `j != i`. This\n",
      " |      assumption allows more efficient computation as compared to\n",
      " |      `GradientTape.jacobian`. The output, as well as intermediate activations,\n",
      " |      are lower dimensional and avoid a bunch of redundant zeros which would\n",
      " |      result in the jacobian computation given the independence assumption.\n",
      " |      \n",
      " |      Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      " |      compute one set of gradients (or jacobians).\n",
      " |      \n",
      " |      Note: By default the batch_jacobian implementation uses parallel for (pfor),\n",
      " |      which creates a tf.function under the hood for each batch_jacobian call.\n",
      " |      For better performance, and to avoid recompilation and vectorization\n",
      " |      rewrites on each call, enclose GradientTape code in @tf.function.\n",
      " |      \n",
      " |      \n",
      " |      Example usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.GradientTape() as g:\n",
      " |        x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n",
      " |        g.watch(x)\n",
      " |        y = x * x\n",
      " |      batch_jacobian = g.batch_jacobian(y, x)\n",
      " |      # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].\n",
      " |          `target[i,...]` should only depend on `source[i,...]`.\n",
      " |        source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\n",
      " |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      " |          alters the value which will be returned if the target and sources are\n",
      " |          unconnected. The possible values and effects are detailed in\n",
      " |          'UnconnectedGradients' and it defaults to 'none'.\n",
      " |        parallel_iterations: A knob to control how many iterations are dispatched\n",
      " |          in parallel. This knob can be used to control the total memory usage.\n",
      " |        experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else\n",
      " |          uses a tf.while_loop.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`\n",
      " |        is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked\n",
      " |        per-example jacobians.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called on a used, non-persistent tape.\n",
      " |        RuntimeError: If called on a non-persistent tape with eager execution\n",
      " |          enabled and without enabling experimental_use_pfor.\n",
      " |        ValueError: If vectorization of jacobian computation fails or if first\n",
      " |          dimension of `target` and `source` do not match.\n",
      " |  \n",
      " |  gradient(self, target, sources, output_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n",
      " |      Computes the gradient using operations recorded in context of this tape.\n",
      " |      \n",
      " |      Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      " |      compute one set of gradients (or jacobians).\n",
      " |      \n",
      " |      Args:\n",
      " |        target: a list or nested structure of Tensors or Variables to be\n",
      " |          differentiated.\n",
      " |        sources: a list or nested structure of Tensors or Variables. `target`\n",
      " |          will be differentiated against elements in `sources`.\n",
      " |        output_gradients: a list of gradients, one for each element of\n",
      " |          target. Defaults to None.\n",
      " |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      " |          alters the value which will be returned if the target and sources are\n",
      " |          unconnected. The possible values and effects are detailed in\n",
      " |          'UnconnectedGradients' and it defaults to 'none'.\n",
      " |      \n",
      " |      Returns:\n",
      " |        a list or nested structure of Tensors (or IndexedSlices, or None),\n",
      " |        one for each element in `sources`. Returned structure is the same as\n",
      " |        the structure of `sources`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called on a used, non-persistent tape.\n",
      " |        RuntimeError: If called inside the context of the tape.\n",
      " |        TypeError: If the target is a None object.\n",
      " |        ValueError: If the target is a variable or if unconnected gradients is\n",
      " |         called with an unknown value.\n",
      " |  \n",
      " |  jacobian(self, target, sources, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n",
      " |      Computes the jacobian using operations recorded in context of this tape.\n",
      " |      \n",
      " |      Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
      " |      compute one set of gradients (or jacobians).\n",
      " |      \n",
      " |      Note: By default the jacobian implementation uses parallel for (pfor), which\n",
      " |      creates a tf.function under the hood for each jacobian call. For better\n",
      " |      performance, and to avoid recompilation and vectorization rewrites on each\n",
      " |      call, enclose GradientTape code in @tf.function.\n",
      " |      \n",
      " |      See[wikipedia\n",
      " |      article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant)\n",
      " |      for the definition of a Jacobian.\n",
      " |      \n",
      " |      Example usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.GradientTape() as g:\n",
      " |        x  = tf.constant([1.0, 2.0])\n",
      " |        g.watch(x)\n",
      " |        y = x * x\n",
      " |      jacobian = g.jacobian(y, x)\n",
      " |      # jacobian value is [[2., 0.], [0., 4.]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        target: Tensor to be differentiated.\n",
      " |        sources: a list or nested structure of Tensors or Variables. `target`\n",
      " |          will be differentiated against elements in `sources`.\n",
      " |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
      " |          alters the value which will be returned if the target and sources are\n",
      " |          unconnected. The possible values and effects are detailed in\n",
      " |          'UnconnectedGradients' and it defaults to 'none'.\n",
      " |        parallel_iterations: A knob to control how many iterations are dispatched\n",
      " |          in parallel. This knob can be used to control the total memory usage.\n",
      " |        experimental_use_pfor: If true, vectorizes the jacobian computation. Else\n",
      " |          falls back to a sequential while_loop. Vectorization can sometimes fail\n",
      " |          or lead to excessive memory usage. This option can be used to disable\n",
      " |          vectorization in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list or nested structure of Tensors (or None), one for each element in\n",
      " |        `sources`. Returned structure is the same as the structure of `sources`.\n",
      " |        Note if any gradient is sparse (IndexedSlices), jacobian function\n",
      " |        currently makes it dense and returns a Tensor instead. This may change in\n",
      " |        the future.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called on a used, non-persistent tape.\n",
      " |        RuntimeError: If called on a non-persistent tape with eager execution\n",
      " |          enabled and without enabling experimental_use_pfor.\n",
      " |        ValueError: If vectorization of jacobian computation fails.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Clears all information stored in this tape.\n",
      " |      \n",
      " |      Equivalent to exiting and reentering the tape context manager with a new\n",
      " |      tape. For example, the two following code blocks are equivalent:\n",
      " |      \n",
      " |      ```\n",
      " |      with tf.GradientTape() as t:\n",
      " |        loss = loss_fn()\n",
      " |      with tf.GradientTape() as t:\n",
      " |        loss += other_loss_fn()\n",
      " |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
      " |      \n",
      " |      \n",
      " |      # The following is equivalent to the above\n",
      " |      with tf.GradientTape() as t:\n",
      " |        loss = loss_fn()\n",
      " |        t.reset()\n",
      " |        loss += other_loss_fn()\n",
      " |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
      " |      ```\n",
      " |      \n",
      " |      This is useful if you don't want to exit the context manager for the tape,\n",
      " |      or can't because the desired reset point is inside a control flow construct:\n",
      " |      \n",
      " |      ```\n",
      " |      with tf.GradientTape() as t:\n",
      " |        loss = ...\n",
      " |        if loss > k:\n",
      " |          t.reset()\n",
      " |      ```\n",
      " |  \n",
      " |  stop_recording(self)\n",
      " |      Temporarily stops recording operations on this tape.\n",
      " |      \n",
      " |      Operations executed while this context manager is active will not be\n",
      " |      recorded on the tape. This is useful for reducing the memory used by tracing\n",
      " |      all computations.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> x = tf.constant(4.0)\n",
      " |      >>> with tf.GradientTape() as tape:\n",
      " |      ...   with tape.stop_recording():\n",
      " |      ...     y = x ** 2\n",
      " |      >>> dy_dx = tape.gradient(y, x)\n",
      " |      >>> print(dy_dx)\n",
      " |      None\n",
      " |      \n",
      " |      Yields:\n",
      " |        None\n",
      " |      Raises:\n",
      " |        RuntimeError: if the tape is not currently recording.\n",
      " |  \n",
      " |  watch(self, tensor)\n",
      " |      Ensures that `tensor` is being traced by this tape.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if it encounters something that is not a tensor.\n",
      " |  \n",
      " |  watched_variables(self)\n",
      " |      Returns variables watched by this tape in order of construction.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.GradientTape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照李宏毅课程中的步骤来完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义超参数hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习步长\n",
    "learning_rate = 0.0001\n",
    "# 循环次数\n",
    "circle_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 然后对一个五维的数据进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 没有使用正确的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0],\n",
    "     [0,3,2,4,1]])\n",
    "# 为0的地方并不是评分为0，而是用户并没有对该物品进行评价。没有评分的地方并不用考虑它的误差。\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "输入\n",
    "R 是m*n的评分矩阵\n",
    "K 隐性特征向量维度。注意这个特征维度是人为定义的。但是按道理这不应该有人为定义。\n",
    "steps/max_iter 最大迭代步长\n",
    "alpha 步长\n",
    "lamda 正则化系数\n",
    "\n",
    "输出\n",
    "分解之后的P和Q\n",
    "P 初始化用户特征矩阵m*k\n",
    "Q 初始化物品特征矩阵k*n\n",
    "\"\"\"\n",
    "\n",
    "# 对超参数进行赋值\n",
    "K=2\n",
    "max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "alpha = 0.0002\n",
    "lamda = 0.004\n",
    "\n",
    "def grad(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "    m = len(R)\n",
    "    n = len(R[0])\n",
    "    \n",
    "    P = np.random.rand(m, K)\n",
    "    Q = np.random.rand(K, n)\n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        # 对所有的用户u和物品i做遍历。对对应的Pu和Qi向量进行梯度下降。\n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                # 对于每一个大于0的评分，求出评分误差。\n",
    "                if R[u][i] > 0:\n",
    "                    eui = np.dot(P[u, :],Q[:, i]) - R[u,i]\n",
    "                    \n",
    "                    # 带入梯度下降的公式，按照梯度下降算法更新当前的Pu和Qi。也就是按照K个隐藏维度来更新。\n",
    "                    for k in range(K):\n",
    "                        # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "                        # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "                        # 用再求和了。\n",
    "                        P[u][k] = P[u][k] - alpha * (2 * eui * Q[k][i] - 2 * lamda * P[u][k])\n",
    "                        # 同样的\n",
    "                        Q[k][i] = Q[k][i] - alpha * (2 * eui * P[u][k] - 2 * lamda * Q[k][i])\n",
    "                \n",
    "        # u和i遍历完成。所有特征向量都更新完成。可以计算预测评分矩阵。\n",
    "        # predictR = np.dot(P, Q)\n",
    "        # 计算当前的损失函数。\n",
    "        cost = 0\n",
    "        \n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                # 在评分矩阵R中为0的不计算损失函数，原因依然是为0的评分可能是用户没有评分。\n",
    "                if R[u][i] > 0:\n",
    "                    cost += (np.dot(P[u, :],Q[:, i]) - R[u,i]) ** 2\n",
    "                    for k in range(K):\n",
    "                        cost += lamda * (P[u][k] ** 2 + Q[k][i] ** 2)\n",
    "        # 当损失函数小于某一个特定阈值时退出。\n",
    "        if cost < cost_threshold:\n",
    "            break\n",
    "    return P, Q, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin R is [[4 0 1 0 5]\n",
      " [1 2 1 3 5]\n",
      " [4 5 3 1 0]\n",
      " [2 3 0 2 5]\n",
      " [5 1 4 0 0]\n",
      " [0 3 2 4 1]], \n",
      "\n",
      " predict Matrix is [[ 3.08530702  1.92869752  2.11718031  3.54010476  5.13398751]\n",
      " [ 2.09603817  0.87096131  1.44174417  2.88328844  4.719205  ]\n",
      " [ 3.86158274  5.07022909  2.62923045  1.53899963 -1.01951784]\n",
      " [ 2.84000453  1.96195705  1.94740031  3.05549172  4.20276956]\n",
      " [ 4.67550674  1.81305316  3.21702057  6.57283231 10.89051844]\n",
      " [ 3.33665781  3.41411086  2.27933898  2.38243145  1.82918127]], \n",
      "\n",
      "  User matrix is [[ 1.61932674  0.59191702]\n",
      " [ 1.43891851  0.13441167]\n",
      " [-0.02179166  2.35952158]\n",
      " [ 1.34666796  0.65856847]\n",
      " [ 3.30977281  0.2207581 ]\n",
      " [ 0.72685412  1.44957194]], \n",
      "\n",
      " Item matrix is [[ 1.30267523  0.40421375  0.89710065  1.94118608  3.31718897]\n",
      " [ 1.64862666  2.15257094  1.12259188  0.67017878 -0.40145036]], \n",
      "\n",
      " Cost is 14.632448538433511\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P, Q, cost = grad(R, K, max_iter, alpha, lamda)\n",
    "predictR = np.dot(P, Q)\n",
    "print(\"origin R is {}, \\n\\n predict Matrix is {}, \\n\\n  User matrix is {}, \\n\\n Item matrix is {}, \\n\\n Cost is {}\\n\\n\".format(R, predictR, P, Q, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin R is [[4 0 1 0 5]\n",
      " [1 2 1 3 5]\n",
      " [4 5 3 1 0]\n",
      " [2 3 0 2 5]\n",
      " [5 1 4 0 0]\n",
      " [0 3 2 4 1]], \n",
      "\n",
      " predict Matrix is [[3.98129334 2.88772337 1.03185416 2.2871385  5.00714545]\n",
      " [1.00423263 2.00907678 0.98400247 3.00982762 5.00561081]\n",
      " [4.02301129 4.99966975 2.99214759 0.99586013 8.17229013]\n",
      " [1.98796753 3.02288686 2.15911716 2.00039133 5.00294678]\n",
      " [5.01446274 1.00394671 3.99722158 3.87058576 1.44459909]\n",
      " [2.49623367 2.98782827 2.00730408 4.0123347  1.0034288 ]], \n",
      "\n",
      "  User matrix is [[ 0.77263191  0.39894275  0.44141781  0.83615125  1.64913806]\n",
      " [ 1.87732493  0.30978616  0.10762536  0.35761834  0.20940714]\n",
      " [ 0.76522684  1.97498457 -0.14383774  1.6699223   0.87418461]\n",
      " [ 1.08084512  1.22845156  0.21445352  0.66658305  0.17889472]\n",
      " [ 0.10747017  0.5456395   2.46119379  0.78410461  0.32887995]\n",
      " [ 0.70083521  1.35408608  1.35217766 -0.8015405   0.66165732]], \n",
      "\n",
      " Item matrix is [[-0.00741578  0.6596376   0.17590694  1.51469259  2.00592624]\n",
      " [ 0.54959653  1.66837407  0.97702877  0.18853392  1.10260378]\n",
      " [ 1.36069905 -0.18171945  1.17516206  1.53887665 -0.55154933]\n",
      " [ 1.19028518  0.18216839  0.8996725  -0.41273327  2.16174404]\n",
      " [ 1.31697296  0.99468595 -0.46377891  0.42897822  0.88127599]], \n",
      "\n",
      " Cost is 0.9973872907468129\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过上面的结果观察，评分矩阵和实际的评分直接误差有点大。\n",
    "# 这个时候思考可能分解的维度可能太低了，测试提高分解维度来观察结果。\n",
    "K= 5\n",
    "P, Q, cost = grad(R, K, max_iter, alpha, lamda)\n",
    "predictR = np.dot(P, Q)\n",
    "print(\"origin R is {}, \\n\\n predict Matrix is {}, \\n\\n  User matrix is {}, \\n\\n Item matrix is {}, \\n\\n Cost is {}\\n\\n\".format(R, predictR, P, Q, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tensorflow来实现特征值分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "R_square = np.array([[4,0,1,0,5],\n",
    "     [1,2,1,3,5],\n",
    "     [4,5,3,1,0],\n",
    "     [2,3,0,2,5],\n",
    "     [5,1,4,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新用矩阵的思维来写特征值分解。感觉以前写的有点问题。\n",
    "\n",
    "\n",
    "用tensor来实现。用tensorflow来实现完成。但是还不是用的矩阵思维。还是有问题。主要是对算法理解还有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress is 0.0\n",
      "Progress is 1.0\n",
      "Progress is 2.0\n",
      "Progress is 3.0\n",
      "Progress is 4.0\n",
      "Progress is 5.0\n",
      "Progress is 6.0\n",
      "Progress is 7.0\n",
      "Progress is 8.0\n",
      "Progress is 9.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "K=2\n",
    "max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "alpha = 0.0002\n",
    "lamda = 0.004\n",
    "\n",
    "def GDM_Tensor(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "    m = len(R)\n",
    "    n = len(R[0])\n",
    "\n",
    "    R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "    P = tf.Variable(np.random.rand(m, K), dtype=float)\n",
    "    Q = tf.Variable(np.random.rand(K, n), dtype=float)\n",
    "    # print(\"Display P.shape={} and  Q.shape={}\".format(P.shape, Q.shape))\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        if step%100 == 0:\n",
    "            print(\"Progress is {}\".format(step/100 + 1))\n",
    "        # 按逻辑对于用户必须逐行更新，对于item必须逐列更新。\n",
    "        for u in range(m):\n",
    "            for i in range(n):\n",
    "                if R[u][i] > 0:\n",
    "                    # print(\"Display P[u, :].shape={} and  Q[:, i].shape={}\".format(P[u, :].shape, Q[:, i].shape))\n",
    "                    # eui = tf.matmul(P[u, :], Q[:, i]) - R[u,i]\n",
    "                    eui = (tf.matmul(tf.reshape(P[u, :], [1,K]), tf.reshape(Q[:, i], [K,1])) - R[u,i])[0,0]\n",
    "                    # print(eui)\n",
    "                    # print(type(eui))\n",
    "                    for k in range(K):\n",
    "                        # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "                        # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "                        # 用再求和了。\n",
    "                        P[u, k].assign(P[u, k] - alpha * (2 * eui * Q[k, i] - 2 * lamda * P[u, k]))\n",
    "                        Q[k, i].assign(Q[k, i] - alpha * (2 * eui * P[u, k] - 2 * lamda * Q[k, i]))\n",
    "                        break\n",
    "                    \n",
    "        # print(\"Cycle end.\")\n",
    "        # 这个是收敛条件之一。\n",
    "        cost = tf.Variable(0.0)\n",
    "        Eui_matrix = tf.matmul(P, Q) - R_tf\n",
    "        cost = tf.reduce_sum(tf.reduce_sum(tf.square(Eui_matrix))) + lamda * (tf.reduce_sum(tf.reduce_sum(tf.square(P), 0)) + tf.reduce_sum(tf.reduce_sum(tf.square(Q), 0)))\n",
    "        \n",
    "        if cost <= cost_threshold:\n",
    "            break\n",
    "    \n",
    "    return P, Q, cost\n",
    "\n",
    "P, Q, cost = GDM_Tensor(R_square, K=2, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 2) dtype=float32, numpy=\n",
      "array([[1.5650222 , 0.4134315 ],\n",
      "       [1.3195595 , 0.64713115],\n",
      "       [2.619709  , 0.37340465],\n",
      "       [1.4728941 , 0.84225214],\n",
      "       [2.181799  , 0.8092877 ]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 5) dtype=float32, numpy=\n",
      "array([[1.5793083 , 1.3907872 , 1.1425058 , 0.84974104, 3.2946165 ],\n",
      "       [0.6499688 , 0.14561683, 0.34822193, 0.11311472, 0.2997057 ]],\n",
      "      dtype=float32)>\n",
      "tf.Tensor(167.67068, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(P)\n",
    "print(Q)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用tensorflow来实现这个函数.\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def ModifyTensor(input_tensor, position=None, value=None):\n",
    "#     input_tensor = input_tensor.numpy()\n",
    "#     input_tensor[tuple(position)] = value\n",
    "#     return input_tensor\n",
    "\n",
    "# K=2\n",
    "# max_iter = 5000 #迭代次数多意味着步长比较小。\n",
    "# alpha = 0.0002\n",
    "# lamda = 0.004\n",
    "\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "# def grad_tf(R, K=2, max_iter= 5000, alpha=0.001, lamda= 0.002, cost_threshold = 0.0001):\n",
    "#     R_tf = tf.convert_to_tensor(R, dtype=float)\n",
    "#     m = len(R)\n",
    "#     n = len(R[0])\n",
    "    \n",
    "#     # P = np.random.rand(m, K)\n",
    "#     # Q = np.random.rand(K, n)\n",
    "#     P = tf.Variable(tf.zeros([m, K], dtype=float))\n",
    "#     Q = tf.Variable(tf.zeros([K, n], dtype=float))\n",
    "    \n",
    "#     for step in range(max_iter):\n",
    "#         # 对所有的用户u和物品i做遍历。对对应的Pu和Qi向量进行梯度下降。\n",
    "#         for u in range(m):\n",
    "#             for i in range(n):\n",
    "#                 # 对于每一个大于0的评分，求出评分误差。\n",
    "#                 if R_tf[u][i] > 0:\n",
    "#                     eui = tf.matmul(tf.reshape(P[u, :], [1, -1]),tf.reshape(Q[:, i], [-1, 1])) - R_tf[u,i]\n",
    "                    \n",
    "#                     # 带入梯度下降的公式，按照梯度下降算法更新当前的Pu和Qi。也就是按照K个隐藏维度来更新。\n",
    "#                     for k in range(K):\n",
    "#                         # 注意这里和公式不同的地方在于求和公式。由于求和是对i在求和，而本计算是包含在\n",
    "#                         # for i in range(n):当中的，就相对于每个步骤都减去了一个对于i的元素，所以不\n",
    "#                         # 用再求和了。\n",
    "#                         P = tf.py_function(ModifyTensor, \n",
    "#                                            inp=[P, [u, k], P[u][k] - alpha * (2 * eui * Q[k][i] - 2 * lamda * P[u][k])], \n",
    "#                                            Tout=P.dtype)\n",
    "#                         # 同样的\n",
    "#                         Q = tf.py_function(ModifyTensor, \n",
    "#                                            inp=[Q, [k, i], Q[k][i] - alpha * (2 * eui * P[u][k] - 2 * lamda * Q[k][i])], \n",
    "#                                            Tout=Q.dtype)\n",
    "                \n",
    "#         # u和i遍历完成。所有特征向量都更新完成。可以计算预测评分矩阵。\n",
    "#         # predictR = np.dot(P, Q)\n",
    "#         # 计算当前的损失函数。\n",
    "#         cost = 0\n",
    "        \n",
    "#         for u in range(m):\n",
    "#             for i in range(n):\n",
    "#                 # 在评分矩阵R_tf中为0的不计算损失函数，原因依然是为0的评分可能是用户没有评分。\n",
    "#                 if R_tf[u][i] > 0:\n",
    "#                     cost += (tf.matmul(tf.reshape(P[u, :], [1, -1]),tf.reshape(Q[:, i], [-1, 1])) - R_tf[u,i]) ** 2\n",
    "#                     for k in range(K):\n",
    "#                         cost += lamda * (tf.square(P[u][k]) + tf.square(Q[k][i]))\n",
    "#         # 当损失函数小于某一个特定阈值时退出。\n",
    "#         if cost < cost_threshold:\n",
    "#             break\n",
    "#     return P, Q, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_6888/2097229772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# P, Q, cost = grad_tf(R, K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Eig in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# R_tf = tf.convert_to_tensor(R_square, dtype=float)\n",
    "# # tensorflow进行eig必须是方阵。\n",
    "# # \n",
    "# M, N = tf.eig(R_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.5489885-1.8442735e-08j -1.7649044-1.9651887e+00j\n",
      " -1.7649044+1.9651892e+00j  2.9616973-3.0399860e-08j\n",
      " 11.019123 +6.4537744e-09j], shape=(5,), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.50880915+0.00434304j  0.28622308-0.30314255j -0.38530356+0.15924889j\n",
      "  -0.56747574+0.20958522j  0.35919613-0.00087918j]\n",
      " [ 0.20886296-0.00178275j  0.05577823-0.29754078j -0.29786214-0.05403553j\n",
      "   0.42365906-0.15646952j  0.4714589 -0.00115396j]\n",
      " [ 0.61511016-0.0052503j  -0.1284342 +0.6106717j   0.61630976+0.09786545j\n",
      "   0.6015964 -0.2221869j   0.530147  -0.0012976j ]\n",
      " [-0.5167199 +0.00441057j  0.38827828+0.04287843j -0.09845588+0.37802777j\n",
      "   0.12856518-0.04748289j  0.4572356 -0.00111915j]\n",
      " [ 0.22815934-0.00194746j -0.42346954+0.11488671j  0.25839487-0.35462296j\n",
      "  -0.00247677+0.00091478j  0.39821923-0.00097469j]], shape=(5, 5), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# x = tf.constant([[1,2], [3,4]])\n",
    "# y = tf.add(x, 1)\n",
    "# print(y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
