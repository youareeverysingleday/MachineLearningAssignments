{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合的简单例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 引入投票器模型\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据没有加载。\n",
    "\n",
    "names = []\n",
    "df = pd.read_csv()\n",
    "array = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting融合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "estimators = []\n",
    "model_1 = LogisticRegression()\n",
    "estimators.append(('logistic', model_1))\n",
    "\n",
    "model_2 = DecisionTreeClassifier()\n",
    "estimators.append(('dt', model_2))\n",
    "\n",
    "model_3 = SVC()\n",
    "estimators.append(('svm', model_3))\n",
    "\n",
    "# 构建融合的投票器模型\n",
    "ensemble = VotingClassifier(estimators=estimators)\n",
    "# 通过交叉验证来对投票器进行训练\n",
    "result = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(result.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging融合模型 \\\n",
    "这个例子中使用了100棵决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "number = 100\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "model = BaggingClassifier(base_estimator=dt, n_estimators=number, random_state=2022)\n",
    "result = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(result.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest \\\n",
    "这个例子中使用了100棵决策树 \\\n",
    "每次取样本的时候并没有取全部的样本，取特征也是随机取特征来进行训练。因此，它抗过拟合的性能会好一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "number_trees = 100\n",
    "max_feature_number = 5\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "model = RandomForestClassifier(n_estimators=number_trees, max_features=max_feature_number)\n",
    "result = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(result.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "number_trees = 25\n",
    "max_feature_number = 5\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=number_trees, random_state=2022)\n",
    "result = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(result.mean)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
