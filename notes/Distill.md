# the key and important point

## 1. important point of machine learning

|编号|内容|来源|说明|
|---|---|---|---|
|1|Embedding:用低维空间的向量来表示高维空间的图形。具体要用到的知识点是流形。对应一个例子就是用RGB三色来表示所有的颜色。|[embedding的解释](https://www.zhihu.com/question/38002635)|/|
|3||||
|||||

1. 机器学习：
   1. 理论基础是PAC，具体而言是基于大数定律，换句话说就是基于独立同分布的数据。独立同分布数据的反面例子之一就是符合幂律分布。具体而言比如二八定律的事情，比如出现很多但是权重很小，出现很少但是权重很高的事情。
   2. 机器学习目前是在做的本质是模型识别，而且无法做到因果推断。目前只是做的相关性进行的拟合，也就是对函数曲线的拟合。这样在数据中本身带有的偏见就会被拟合出来，甚至有可能被放大。其中神经网络的优化实际上是在求一个非凸函数的最优解，而且是尽力而为的求解。
   3. 当前的机器学习需要使用大量的数据去解决简单的问题，而人类可以使用有限的知识或者常识来解决复杂的问题，可以过滤掉大量无用的细节，可以使用不完整的信息甚至是不一致的信息中推理出有用的结论。
   4. 改进方向：
      1. 利用已知的知识对AI模型进行优化。并不是使用越底层的数据就说明AI越高级，似乎可以对已有的明显的逻辑规律对AI进行辅助。
      2. 对已知知识的组合能力。
      3. 训练可以之下而上也需要自上而下的方式。
      4. 因果关系。
      5. 利用系统性的设计经验。
2. 三种基础的损失函数：最小二乘法、极大似然估计、交叉熵。
   1. 最小二乘法：以直观的几何的方式来定义距离。
   2. 极大似然估计：猜测一种合理的模型之后，通过求模型参数最大值的方式来求出这个模型。
   3. 交叉熵：所有的模型的出发点是希望将不确定性演变为确定性。因此，借助信息量的定义来度量两个模型之间的差异。KL散度=交叉熵-熵。其中的关键点在于需要以两个相互比较模型中的一个作为比较基准点。
3. Embedding:用低维空间的向量来表示高维空间的图形。具体要用到的知识点是流形。对应一个例子就是用RGB三色来表示所有的颜色。[embedding的解释](https://www.zhihu.com/question/38002635)。
4. 优化问题的方法：非线性优化问题最终采用的数学方法：使用拉格朗日乘子法来解决带条件的约束问题。然后再对使用了约束的方程使用梯度来求极值解决问题。
   1. 极大似然估计。
   2. 出现有条件的情况的时候使用拉格朗日乘子法来解决约束问题。
   3. 使用泰勒展开将其线性化。
   4. 在2的基础上使用梯度或者其他优化方法来求极值。
   5. 凸优化和凸函数。
   6. 迭代过程中在求局部最优解的时候都做了线性化处理。

5. 在矩阵分解中通过Jaccard coefficient来描述两个集合之间的相似性。它是通过内积计算两个集合之间的距离的。这个距离没有方向性所以存在一个显著的缺点：详见Neural Collaborative Filtering 2.2中图1的解释。
6. 常说的逻辑回归实际上是对数几率回归。在周志华-机器学习-第3.3中说明了。实质上是线性回归加上对数投影之后产生分类效果。
7. 决策树的思想实际上是模拟人类决策过程思想的模型。通过多个条件逐一筛选的对象的过程模拟。其中最主要的技术点在于剪枝（为了避免过拟合）和节点的选取（通过信息熵、信息增益、信息增益率、基尼指数）。决策树可以用于分类，也可以用于回归。也就是可以预测连续值也可以预测离散值。
8. 剪枝，不是树是完整的就是好的，有可能已经过拟合了。通过判断每个节点剪枝前后的参数（比如精度等）来判断是否需要剪掉该节点。
9.  图灵通过停机问题确定了可计算的边界。量子计算机的运算方式则建立在原子运动的层面上，突破了分子物理的界限。量子计算机的理论模型-量子图灵机则是一种概率型图灵机。

10. SVM支持向量机，实际上是将数据几何化之后对其进行分类的一种思想。其中支持向量是两个类型最靠近的点，决策边界取决于距离连个最靠近点的距离。其中的关键技术包括：
    1. 最大容错和间隔（对异常数据的处理）
    2. 坐标上升算法（求得全局最优解，使用于光滑凸优化问题。核心思路就是对于多变量求极值，先随机初始化所有变量，使其中一个变量变化其他的全部固定，求得最小值；然后将刚才变化的变量固定，换一个变量再求最小值；直到所有变量的变化都不在影响最小值变化为止）。
    3. 特征映射（将低维空间中不易使用线性分类的数据映射到高维空间进行线性分类）。
11. 贝叶斯模型的是一种概率模型。通过计算条件概率来进行分类。**对于一个未知的分布，先假设其服从先验分布（来源于以前做试验数据计算得到，或来自于人们的主观经验），通过观测到的数据，根据贝叶斯规则计算对应的后验分布**。

12. **梯度就是在多维空间中对每个维度求的偏导数组成的向量**。
13. 正则化是对一些特定项的惩罚。比如推荐系统中，正则化就是对热门item的惩罚。正则化和范数好像是关联的？

## about recommendation system

1. 在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。周志华-机器学习2.3.2
2. 冷启动在协同过滤推荐系统中是不会仅依赖于行为数据来解决的。这个仅依赖于行为数据是无法解决冷启动的问题的。在冷启动的场景中往往采用其他的方式来解决冷启动的问题，比如热门推荐等。
3. 推荐系统是一个系统工程，不是一个单一算法可以解决的。其中对包括对获取信息的难易程度、数据的处理、对信息的挖掘、算法在不同场景和环境下的选择都会极大的影响最终的结果。[详见](RecommendationSystem/1RecommendationSystem.md)。
4. 对于在推荐系统中所说的“隐性空间”，是否可以理解为低维向高维的特征映射。这样做的目的在SVM里面是更加容易的划分不同的类别，而且是使用线性的形式来划分不同的类别。而推荐系统里面这样做的目的是方便将不同的数据放到同一个空间里面进行运算（逻辑和数值运算，从而比较关系和大小等）。
5. 推荐系统的作用是为了缓解信息过载。Neural Collaborative Filtering 1介绍。

## about engineering problems

1. [室内位置定位](https://www.kaggle.com/c/indoor-location-navigation/discussion/240176)。[代码](https://github.com/ttvand/Indoor-Location-Navigation-Public)kaggle的竞赛名称：Indoor Location & Navigation,具体要求是：Identify the position of a smartphone in a shopping mall。

## 其他一些不那么重要的相关信息

1. 在实时下一个POI位置推荐中，最重要的是如何度量用户的偏好。也就是如何通过公式表达用户的偏好。
2. 三种单词来源.
    1. 日常用anglo-saxn的单词，一般成为small words；
    2. 报纸、新闻中常用来源于法语的单词，认为它一般比较优雅；
    3. 学术报告、合同、政府报告、法律文书常用来源于拉丁语的单词，称其为大词big words，含义比较窄，或者说比较准确。
3. 在科技文献方面一般不用形容词来修饰名词，而是用名词串，也就是用名词修饰名词。
4. 有些缩略要用，有些缩略不能用，特别是isn't不能用。
5. IEEE官网上有论文模板。
6. **这里需要强调，在单纯的推荐系统中得到的行为数据也是稀疏的。因为用户不会对所有的物品都有交互行为，有交互行为的商品毕竟是极少数。对应于POI推荐，访问的地点相对于所有地点而言也是极少数**。
7. 经常可以看到为了衡量一种性质时会使用类似$result = \sum XY$的形式，之所以是这种形式的基本原理是概率论里面的乘法原理和加法原理。也就是独立事件的串联和并联思想。比如在推荐系统中为了表示两用户u对物品i的偏好程度在UGC中的公式就是这种体现$p(u,i)=\sum\limits_b n_{u,b}n_{b,i}$。详细说明见1RecommendationSystem.md中的说明。[推荐系统笔记文档](RecommendationSystem/1RecommendationSystem.md)
8. 推荐系统的选择和应用场景有非常大的关系。这种关系在于：该场景中那种研究对象相对比较固定。
9. 强化学习：
    1. 判断是否是强化学习的充分条件是：是否符合马尔科夫决策过程。
    2. 马尔科夫过程的每一个状态只和上一个状态有关，强调一下只和上一个动作有关。当出现一个状态与前一时刻的多个状态有关，那么它是不符合马尔科夫过程的。也就是说不是强化学习。
    3. 强化学习是判断整个马尔科夫链上所有动作的奖励的总和。而不是判断每一步的奖励。也就是说某一步的正奖励非常大，但是不一定会选择下一步的动作。
    4. 深度强化学习使用深度神经网络来拟合Q表（Q-Table）。
10. 无标签学习
    1. 关键点如何生成伪标签。
       1. 对于图像处理任务中
          1. 图片旋转，将图片旋转之后能很好的将其分类。
          2. 图片上色
          3. 图片补全
          4. 关系上下文预测。图片划分之后能够正确预测图片分块的位置。
    2. 对比学习的关键点在于如何划分正负样本。
11. 组会
    1. 建图结构不同直接会影响占用内存的大小。
    2. 在矩阵因式分解中是对所有Item进行的推荐，在个性化推荐的情况下（也就是给定用户，关注用户和Item之间的交互情况下，也就是只有这种数据的情况下如何做到最优）
    3. 无论是有向图还是无向图都不好解决一个序列中的时序问题。
    4. 大类之间尽可能多样，小类之间尽可能相关。这个是否可以用于人的分类上。
    5. 交互式推荐，多次收集用户的行为，然后多轮推荐。
    6. 使用因果推断的时候目前都是先假设一个规律，然后验证这个规律是否正确。无论是否正确然后再对其进行定量的进行讨论。
    7. 使用预训练来解决**多模态数据的对齐问题**。
    8. 图的结构有些冗余？
    9. 图结构的形状研究。
    10. 自表示子空间聚类算法。
    11. 西尔维斯特方程。这个方程有唯一解。
    12. 多模态的数据如何映射到同一个隐性空间中，现在最成熟的方法是使用MLP来完成。
12. 对于训练时没有碰到的数据，如何对其处理。需要模型做到异常检测。

## 2. paper topic

1. Neural Collaborative Filtering，将神经网络应用到了推荐系统中。
   1. 可学习Collaborative Filtering模型有两个关键组件：1）embedding，它将user和item转换为矢量化表示；2）interaction modeling，它基于嵌入重建历史交互。
2. Neural Graph Collaborative Filtering，。
3. Survey on user location prediction based on geo-social networking data，对现在的位置预测做出了总结性的说明，其中比较重要的有如下几点：
   1. 现在的数据都比较稀疏。
   2. 位置预测是提取的人们移动过程中的周期性的内容。
   3. 有效的位置预测不仅需要考虑用户兴趣，还需要考虑时空背景，因为用户在不同的时间和地点往往有不同的选择和需求。
   4. 仍然迫切需要一种能够解决下一次和任何时间的细粒度位置预测问题的统一模型。
   5. 问题分为两类：下一个用户位置预测问题和任何时间的用户位置预测问题。第二是从预测的粒度而言，我们同样分为两类：粗粒度用户位置预测问题和细粒度用户位置预测问题。下面将对用户位置预测问题的类别进行详细说明。
   6. 任何时间+细粒度的位置预测非常困难。
   7. 挑战：
      1. 隐性反馈，
      2. 数据稀疏性，
      3. 冷启动，
      4. 语境意识，
      5. 兴趣漂移。
   8. 趋势：
      1. 建立对于细粒度的任意时间和下一时间的统一模型。
      2. 基于多模态数据。
      3. 使用注意力机制的预测。
      4. 异构信息网络（HIN）嵌入。这个还不理解。


## 3. problems

1. 最大似然到对数损失。
   1. got it.
2. 信息熵information entropy。
   1. got it.
3. 二次规划。
4. 核函数和内积。
5. Lagrange乘子法的简单例子。
6. 贝叶斯网络和有向分离的原理。
7. 要找到一篇最新的强有力的论文来阅读，然后将代码进行复现。最后对其中的算法做出改进。
   1. 图嵌入GNN的方法来解决预测的问题。如何构图。
8. 矩阵分解。
   1. 答：就是将一个比较大的矩阵转化为两个比较小的矩阵相乘。
   2. 作用：对稀疏矩阵进行了降维操作，同时可以提取到特征。举例：任何颜色如果看成0-255范围的一个值，那么可以通过三原色的一个$RGB_{1 \times 3}$和一个权重向量$W_{3 \times 1}$乘积的结果。
9. 是否可以用CNN来生成知识图谱？
   1. 这个问题有点奇怪。知识图谱就现在知道的知识就是一组三元组，用图结构来实现的三元组。
