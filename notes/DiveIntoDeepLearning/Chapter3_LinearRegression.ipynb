{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Linear Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 linear regression\n",
    "\n",
    "1. 放射变换（affline transformation）。线性回归假设的模型是输入特征的一个仿射变换。仿射变换的特点是通过加权和对特征的进行线性变化，并通过偏置项来进行平移得到的。\n",
    "2. 通常用$\\hat{y}$来表示估计值。\n",
    "3. 虽然我们相信给定$\\boldsymbol{x}$预测的最佳模型会是线性的， 但我们很难找到一个有个$n$样本的真实数据集，其中对于所有的$1\\leqslant i \\leqslant y$, $y^{(i)}$完全等于$\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+\\boldsymbol{b}$。无论我们使用什么手段来观察特征$\\boldsymbol{X}$和标签$\\boldsymbol{y}$，**都可能会出现少量的观测误差**。因此，即使确信特征与标签的潜在关系是线性的，我们也会加入一个噪声项来考虑观测误差带来的影响。\n",
    "4. 由于平方误差函数中的二次方项，估计值$\\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。\n",
    "    $$L(\\boldsymbol{w}, \\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n l^{(i)}(\\boldsymbol{w},\\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n \\frac{1}{2} (\\boldsymbol{w}^{T}\\boldsymbol{x}^{(i)}+\\boldsymbol{b}-\\boldsymbol{y}^{(i)})^2 \\tag{3.1.6}$$\n",
    "    在训练模型时，我们希望寻找一组参数$(\\boldsymbol{w}^*, \\boldsymbol{b}^*)$，这组参数能最小化在所有训练样本上的总损失。如下式：$\\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})$。\n",
    "5. 解析解：$\\boldsymbol{w}^* =(\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$。像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。**解析解可以进行很好的数学分析，但解析解对问题的限制很严格**（这个限制条件在于），导致它无法广泛应用在深度学习里。[解答参考文字](https://zhuanlan.zhihu.com/p/74157986)。[解答参考视频](https://www.bilibili.com/video/BV1ro4y1k7YA?spm_id_from=333.337.search-card.all.click)。\n",
    "   1. 这里如何得来的？证明如下。\n",
    "   2. 为什么还有$\\boldsymbol{X}^T$这一项？答：这是为了表示他们之间的距离（L2范数）。**两个形状相同向量之间的距离可以表示为一个向量的转置乘以另外一个向量**。推导过程如下：\n",
    "\n",
    "        $$\n",
    "        \\begin{equation}\n",
    "        \\begin{aligned}\n",
    "        \\text{Known:} & \\text{the sample space is}\\{(x_1,y_1),(x_2,y_2),\\cdots (x_n,y_n)\\}\\\\\n",
    "         & \\text{equation: }\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{B}\\\\\n",
    "         & \\boldsymbol{Y}=\\begin{pmatrix}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            y_n\n",
    "            \\end{pmatrix};\n",
    "        \\boldsymbol{X}=\\begin{bmatrix}\n",
    "            1 & x_1 \\\\\n",
    "            1 & x_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            1 & x_n\n",
    "            \\end{bmatrix};\n",
    "        \\boldsymbol{B}=\\begin{pmatrix}\n",
    "            \\alpha \\\\\n",
    "            \\beta\n",
    "            \\end{pmatrix}.\\\\\n",
    "         & \\alpha\\text{ is intercept,} \\beta \\text{ is slope.} \\\\\n",
    "         & \\therefore \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{B} + \\boldsymbol{\\gamma} \\\\\n",
    "         & \\boldsymbol{\\gamma} =\\begin{pmatrix}\n",
    "            \\epsilon_1 \\\\\n",
    "            \\epsilon_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\epsilon_n\n",
    "            \\end{pmatrix};\\\\\n",
    "\n",
    "        \\text{Target is: } & \\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})\\\\\n",
    "        \\text{solution: } & \\\\\n",
    "         & \\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}} = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b \\\\\n",
    "            \\vec{x}_2^T b \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b\n",
    "            \\end{bmatrix} - \\begin{bmatrix}\n",
    "            \\vec{y}_1 \\\\\n",
    "            \\vec{y}_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "        & = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "         & \\because \\vec{z}^T\\vec{z} = \\sum_i z_i^2;\\\\\n",
    "         & \\therefore \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "            \\end{bmatrix}_{L2} \\text{这个地方还有点问题。} \\\\\n",
    "         & = \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) = \\frac{1}{2} \\sum\\limits_{i=1}^n(\\vec{x}_i^T b - \\vec{y}_i)\\\\\n",
    "         & =L(\\boldsymbol{B})\\\\\n",
    "         & \\nabla_B L(\\boldsymbol{B}) = \\nabla_B \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B ((\\boldsymbol{X}\\boldsymbol{B})^T \\boldsymbol{X}\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B}) + \\vec{y}^T \\vec{y}) \\\\\n",
    "         & \\because \\vec{a}^T\\vec{b} = \\vec{b}^T\\vec{a} \\;\\text{ and }\\;\\vec{y}^T \\vec{y}\\text{ is independent of }\\boldsymbol{B}.\\\\\n",
    "         & \\because \\nabla_x \\vec{b}^T x =  \\vec{b}\\;\\text{ and }\\;\\nabla_x \\vec{x}^T \\boldsymbol{A}x =  2\\boldsymbol{A}\\vec{x}\\;\\text{ for symmetric matrix.}\\\\\n",
    "         & \\therefore = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})  - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2}(2(\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-2\\vec{y}^T \\boldsymbol{X})\\\\\n",
    "         & = (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-\\vec{y}^T \\boldsymbol{X}\\\\\n",
    "         & \\text{To minimize L, L is convex function, we set its derivatives to zero, and obtain the normal equations:}\\\\\n",
    "         & (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} = \\vec{y}^T\\boldsymbol{X} = \\boldsymbol{X}^T\\vec{y}\\\\\n",
    "         & \\Rightarrow \\boldsymbol{B} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\vec{y} \\\\\n",
    "         & \\text{Proof complete.}\n",
    "        \\end{aligned}\n",
    "        \\end{equation}\n",
    "        $$\n",
    "\n",
    "6. 随机梯度下降。梯度下降（gradient descent）的方法，这种方法**几乎可以优化所有深度学习模型**。它通过不断地在损失函数递减的方向上更新参数来降低误差。但实际中的执行可能会非常慢：**因为在每一次更新参数之前，我们必须遍历整个数据集**。 因此，我们通常会在每次需要计算更新的时候**随机抽取一小批样本**，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。\n",
    "   1. **即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值**。\n",
    "   2. 线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，**更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失**，这一挑战被称为泛化（generalization）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CommonCode import Timer\n",
    "import numpy \n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矢量化加速\n",
    "\n",
    "f'{timer.stop():.5f} sec' [python 3.6之后字符串格式化用法参考说明](https://geek-docs.com/python/python-tutorial/python-fstring.html#Python_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用for循环所消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10000,) dtype=float32, numpy=array([2., 2., 2., ..., 2., 2., 2.], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7.23041 sec'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "a = tf.ones(n)\n",
    "b = tf.ones(n)\n",
    "\n",
    "c = tf.Variable(tf.zeros(n))\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i].assign(a[i] + b[i])\n",
    "print(c)\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用矢量化之后消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000 sec'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。\\\n",
    "也就是说：**使用什么损失函数需要考虑噪声的分布情况**。\\\n",
    "\n",
    "极大似然估计（maximum likelihood estimation）的[详细说明详见](../../mathematics/ProbabilityTheory.md)中关于maximum likelihood estimation的说明。\\\n",
    "\n",
    "在3.1.3中说明了服从正态分布的噪声，最小化均方误差等价于对线性模型的极大似然估计。这个中间“可以写出通过给定的x观测到特定y的似然”的这句没有理解?????? \\\n",
    "\n",
    "对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。这里定义了全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计 Maximum Likilihood Estimation\n",
    "\n",
    "1. 一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。极大似然估计就是在估计模型的参数。也就是说**极大似然估计是已知模型和样本，来估计模型的参数**。\n",
    "2. 参考\n",
    "   1. 文字参考\n",
    "      1. [直接的说明](https://zhuanlan.zhihu.com/p/26614750)\n",
    "      2. [比较了概率和统计的区别，同时也说了极大似然估计的概念](https://blog.csdn.net/u011508640/article/details/72815981)。\n",
    "   2. 视频讲解的例子参考<https://www.bilibili.com/video/BV1Hb4y1m7rE?spm_id_from=333.337.search-card.all.click>。\n",
    "\n",
    "3. 离散值的例子：\n",
    "\n",
    "   设一个袋子里有非常多的小球（样本数量：非常大。也就忽略的放回抽样和不放回抽样），**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的**。其中有两种样本，一种是白球，定义为样本1；一种是红球，定义为样本2。两种样本数量的比值如下表所示，其中$\\theta$是未知的：\n",
    "\n",
    "   |X|样本1|样本2|\n",
    "   |---|---|---|\n",
    "   |P|$\\theta$|$1-\\theta$|\n",
    "\n",
    "   如何判断袋子中小球的分布呢？自然就想到的抽样的方式来对袋子中的小球的分布进行判断。现在假设从袋子中按照顺序抽出了5个小球，分别是1、1、2、1、2。那么它们的抽样情况如下表所示：\n",
    "\n",
    "   |抽样结构（按顺序）|1|1|2|1|2|\n",
    "   |---|---|---|---|---|---|\n",
    "   |抽出该样本的概率|$\\theta$|$\\theta$|$1-\\theta$|$\\theta$|$1-\\theta$|\n",
    "\n",
    "   抽出这个顺序样本的概率为$L(\\theta)=\\theta\\theta (1-\\theta)\\theta(1-\\theta)=\\theta^3(1-\\theta)^2$，定义$L(\\theta)$为似然函数。抽出这组样本的概率显然是会随着$\\theta$的变化而变化。随着$\\theta$的变化$L(\\theta)$有无数种值，然后按照“存在即合理”的思想，直接认为应该让$L(\\theta)$最大的$\\theta$为被选择的分布（实际上这个地方还是没有理解清楚其中的逻辑，但是先这样使用）。<https://zhuanlan.zhihu.com/p/26614750>中给出的说明是一个反问句式：“那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是极大似然估计的核心”。\\\n",
    "\n",
    "   这里进行说明为什么需要求最大。在视频<https://www.bilibili.com/video/BV1Y64y1Q7hi?spm_id_from=333.337.search-card.all.click>中的8:30开始说明这个问题。\n",
    "\n",
    "   描述性的说明，这个描述对于理解非常重要：\n",
    "   1. 在抛硬币的例子中，每次抛硬币事件的分布如下：\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|$\\theta$|$1-\\theta$|\n",
    "   2. 如果抛了10次，得到的10次结果为：7次正面，3次反面。\n",
    "   3. 因为只有2个样本，我们猜测模型是二项分布也就是(0-1)分布模型。这个时候就希望能将$\\theta$的值估计出来。\n",
    "   4. 这里需要强调的一点：\n",
    "      一种分布A如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.1|0.9|\n",
    "\n",
    "      一种分布B如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.7|0.3|\n",
    "\n",
    "      一种分布C如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.8|0.2|\n",
    "\n",
    "      在实际抛的过程中都是有可能出现7次正面，3次反面的情况的。只不过它们出现这种情况的概率不同而已。这个过程就是在某种模型的情况下求某次事件的条件概率。\n",
    "   5. 这个时候就需要通过已知的抽样结果：7次正面，3次反面。**来估计具有最大似然（这里表述为似然，而不是概率了。因为是从统计结果分析模型参数了。这里可以理解为可能性最大的结果。同样一个过程从模型到结果称为概率，从结果到模型称为似然）出现这种抽样结果的分布是什么样的**。这就是为什么叫**最大**似然估计的原因。\n",
    "   6. 最后的结果我们就直接认为是符合上述抽样结果的分布！注意，不能认为极大似然估计得出的模型参数就是真实的参数。因为模型对应的真实参数是没有办法确定的。只能估计其可能性最大值。当知道某种模型产生的结果然后去反推概率模型时，往往就会用到极大似然估计。这也是机器学习最重要的理论基础之一。\n",
    "\n",
    "   前提，MLP可以通过添加更多的层数来拟合任意概率模型的曲线。\n",
    "   推导过程如下：\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   & C_i\\text{表示的是事件，}\\theta\\text{表示的是概率模型的参数，这里代指概率模型。}\\\\\n",
    "   & P(C_1,C_2, \\cdots, C_n |\\theta)\\\\\n",
    "   & y_i\\text{表示的是标签，}\\hat{y}_i\\text{表示的是预测值，}\\boldsymbol{W}, \\boldsymbol{b}\\text{表示的是MLP的参数，这里代指神经网络模型。}\\\\\n",
    "   & P(y_1,y_2, \\cdots, y_n |\\boldsymbol{W}, \\boldsymbol{b})\\\\\n",
    "   & = \\prod \\limits_{i=1}^n P(y_i|\\boldsymbol{W}, \\boldsymbol{b})\\\\\n",
    "   & P(y_i|\\boldsymbol{W}, \\boldsymbol{b})\\text{表示在神经网络模型下，和标签对应的结果的概率分别是多少。当这个似然值最大的时候，就可以认定该模型和真实数据对应的模型是最接近的（甚至“武断”的认为就是一样的）。}\\\\\n",
    "   & = \\prod \\limits_{i=1}^n P(y_i|\\hat{y}_i)\\\\\n",
    "   & \\text{如果是一个二分类模型，那么就多次的二项分布（也就是0-1分布），也就是符合伯努利分布。}\\\\\n",
    "   & \\because \\text{伯努利分布为：} x_i \\in \\{0, 1\\};\n",
    "      f(x)=p^x (1-p)^x = \\begin{matrix}\n",
    "      & p, x=1 \\\\\n",
    "      & 1-p, x=0\n",
    "      \\end{matrix}\\\\\n",
    "   & \\therefore \\prod \\limits_{i=1}^n P(y_i|\\hat{y}_i) = \\prod \\limits_{i=1}^n \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i} \\\\\n",
    "   & \\text{对等式求对数。}\\\\\n",
    "   & \\log(\\prod \\limits_{i=1}^n \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i})\\\\\n",
    "   & = \\sum\\limits_{i=1}^n \\log(\\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i})\\\\\n",
    "   & = \\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i))\\\\\n",
    "   & \\text{目的是求上式的最大值：}max(\\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i)))\\\\\n",
    "   & \\text{一般习惯求最小值，所以上式变为：}min-(\\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i)))\\\\\n",
    "   & \\text{Completed.}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "   下面是求$L(\\theta)=\\theta^3(1-\\theta)^2$最大极值点的具体步骤：\n",
    "\n",
    "      1. 由于$\\theta^3(1-\\theta)^2$直接求导不好处理，所以先将公式两边同时求$\\ln$;\n",
    "      2. 等式两边同时求对数。对数函数有个性质，一方面可以将连乘转化为加减；另一方面对数函数不会改变原函数的单调性的(不会改变原函数中点的相对大小)，因为以e为底的对数函数是单增的。等式变为$\\ln L(\\theta) = \\ln (\\theta^3(1-\\theta)^2) = 3\\ln\\theta + 2 \\ln (1-\\theta)$\n",
    "      3. 对$\\theta$求导数。$\\frac{d\\ln L(\\theta)}{d\\theta} = \\frac{3}{\\theta} - \\frac{2}{1-\\theta}$\n",
    "      4. 求极值点，令$\\frac{d\\ln L(\\theta)}{d\\theta} = \\frac{3}{\\theta} - \\frac{2}{1-\\theta} = 0$。求得极值点为$\\hat{\\theta} = \\frac{3}{5}$。\n",
    "4. 连续值的例子：\n",
    "\n",
    "   $X \\sim U(0, a), \\; \\text{a is unknow.} \\\\ f(x)=\\begin{cases} \\frac{1}{a}, & \\text{if a} \\in (0,a) \\\\ 0, & \\text{if a is others} \\end{cases}$。抽取n个样本点，对应的事件分别是$\\{X_1,X_2,\\cdots,X_n\\}$，事件对应的样本点分别是$\\{x_1, x_2, \\cdots ,x_n\\}$，对应每个样本点的概率密度为$f(x_1), f(x_2), \\cdots ,f(x_n)$。那么$\\{X_1,X_2,\\cdots,X_n\\}$的联合概率密度（联合概率就是多个事件同时发生时的概率）为$L(a) =f(x_1)f(x_2)\\cdots f(x_n) = \\frac{1}{a}\\frac{1}{a}\\cdots \\frac{1}{a}=\\frac{1}{a^n}$。\n",
    "   这里需要注意，不能再使用离散值时的例子了。因为先取对数$\\ln L(a) = -n\\ln a$，然后再求导$\\frac{d\\ln L(a)}{da}=\\frac{-n}{a}$，然后令$\\frac{d\\ln L(a)}{da}=\\frac{-n}{a} = 0$的条件是$a \\rightarrow +\\infty$。这显然是不合适的。\\\n",
    "   所以这里采用了另外一种方法。为了使得$L(a)=\\frac{1}{a^n}$取得最大值，就需要a尽可能的小。此时也就需要分析a的取值范围。因为a是一组已经抽样出来的点，而且a是在$\\{x_1, x_2, \\cdots ,x_n\\}$中的一个值。$\\{x_1, x_2, \\cdots ,x_n\\}$是已经存在的抽样样本，也就是已经是事实了。所以a只能取$max\\{x_1, x_2, \\cdots ,x_n\\}$，这样就可以使得“已经是事实”的事件成立（如果取得值小于$x_n$，那么$x_n$是如何取得的呢？）。所以$\\hat{a}=max\\{x_1, x_2, \\cdots ,x_n\\}$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "\n",
    "1. 交叉熵越小越好，说明两个模型越接近。\n",
    "2. 是多分类问题最常用的损失函数之一。\n",
    "\n",
    "霍夫曼编码\n",
    "\n",
    "之所有需要熵这个定义是因为需要度量不同概率模型（比如高斯分布和泊松分布）之间的差异有多大？这是因为之前比较两种分布之间的差异（距离）需要它们是同一种分布才能进行比较。\n",
    "\n",
    "熵代表了一个系统里面的混乱程度。\n",
    "\n",
    "熵既是热力学概念，也是信息论里面的概念。\n",
    "\n",
    "### 信息熵（entropy）\n",
    "\n",
    "1. 参考：\n",
    "   1. 重要参考<https://www.bilibili.com/video/BV15V411W7VB?spm_id_from=333.880.my_history.page.click>\n",
    "   2. 重要参考-机器学习课程笔记。\n",
    "   3. 补充参考，并不重要<https://www.cnblogs.com/Hikigaya-Hachiman/p/10079438.html>。\n",
    "2. 思路：\n",
    "   1. 先定义信息量。信息量是对单个事件的。\n",
    "   2. 然后在通过信息量来定义熵。熵是对整体概率模型或者说系统（是单个事件的集合）而言的。\n",
    "3. 信息熵定义：对于随机的信息源$X$，其中每个事件是$\\{x_1,x_2,\\cdots,x_n\\}$，对应每个事件发生的概率为$\\{p(x_1),p(x_2),\\cdots,p(x_n)\\}$，信息源所代表的样本空间传达的平均的信息量称为熵。信息熵的公式定义为$H(X)=-\\sum\\limits_{i=1}^n [p(x_i)\\log p(x_i)]$。\n",
    "   1. 熵也可以表述为事件从不确定变为确定（概率为1）时的难度有多大。\n",
    "   2. 信息量和熵使用的相通的衡量单位。\n",
    "4. 信息量定义为$h(x)$。注意这个$h(x)$表示的是单个事件的信息量。\n",
    "   1. 单调性。信息$h(x_i)$和事件$x_i$发生的概率$p(x_i)$有关，概率越高信息量越小。也就是$h(x)$是$p(x_i)$单调递减函数。\n",
    "   2. 当$p(x)=1$时，$h(x)=0$。\n",
    "   3. 非负性。信息是非负的。说得任何的话都不会使得信息减少，即使是错误的信息。\n",
    "   4. 信息具有叠加性。两个完全独立的事件$x,y$；$p(xy)=p(x)p(y)\\;,h(xy)=h(x)+h(y)$。\n",
    "   5. 范围要求。$p(x)\\in [0,1],\\; h(x)\\in [-\\infty, +\\infty]$。\n",
    "\n",
    "   $-\\log$函数能够满足上述所有条件。$h(x) \\coloneqq -\\log p(x)$。注意这里是定义，不是发现。也就是说使用$\\log$函数是人为定义的。\n",
    "5. 如果是以2为底来定义信息量，实际上是通过抛硬币的来衡量信息量。举例说明：\n",
    "   1. 如果一件事发生的概率是$\\frac{1}{2}$，它就相当于抛一**个**硬币正面向上发生的概率。\n",
    "   2. 如果一件事发生的概率是$\\frac{1}{8}$，它就相当于抛三个硬币正面全部向上发生的概率。\n",
    "   3. 如果一件事发生的概率是$\\frac{1}{3}$，它发生的概率介于抛一个硬币和抛两个硬币之间正面全部向上发生的概率。在数学上可以理解为大概抛了$\\log \\frac{1}{3}$个硬币正面向上发生的概率。\n",
    "6. 对于随机变量$X$（一组事件的集合），$X$的样本空间为$\\{x_1,x_2,\\cdots,x_n\\}$。$X$所传达的平均信息量是多少？平均信息量就是在求信息量的期望。那么平均信息量，也就是熵定义为$H(X)\\coloneqq E(h(x_i))=\\sum\\limits_{i=1}^n[p(x_i)h(x_i)]=\\sum\\limits_{i=1}^n[p(x_i)(-\\log p(x_i))]$。也就是说单个事件信息量对整个系统信息信息量的贡献一定要乘以该事件发生的比例（概率）才能计算整个系统的信息量。\n",
    "   1. 以2为底，单位为bit。\n",
    "   2. 以e为底，单位为nats。\n",
    "   3. 以10为底，单位为。\n",
    "7. 熵的物理含义：\n",
    "   1. 如果以2为底，那么以二进制编码来传达这些信息，如果接收方能够无歧义的解码，那么不可能设计出来一种编码方式的平均bit数是小于对该信息计算出来的bit数。也就是传递信息无歧义进行识别的最小的**平均**编码个数（编码个数的下限）。\n",
    "   2. 也就描述了一个系统的不确定程度，或者说混乱程度。\n",
    "\n",
    "### 交叉熵（具体）\n",
    "\n",
    "1. 目标比较两个模型。\n",
    "2. 思路：\n",
    "   1. 就需要将两个模型的熵算出来。\n",
    "   2. 然后直接比较两个熵的大小。\n",
    "3. 上面思路的缺陷：\n",
    "   1. 但是这种方式太过简单粗暴。\n",
    "   2. 同时在实际情况中在衡量两个概率分布的差异时，其中一个模型的分布是明确的，另一个模型的分布是一部分未知的。比如说我们在机器学习的时候是建立一个模型是真实世界所代表的模型去比较。\n",
    "   3. 这个时候就出现了相对熵的概念。也就是KL散度。\n",
    "4. 举例：\n",
    "   1. 假设有两个概率系统P和Q系统。\n",
    "   2. 两个系统中每个事件的信息量假设表示为$f_P(p_i)$和$f_Q(q_i)$。\n",
    "   3. KL散度就定义为：$\\coloneqq D_{KL}(P|Q)$。注意P和Q在该式中不是等价的，P在前就代表了是**以P为基准**去考虑Q和P相差有多少。![KL散度参考图](../../pictures/KullbackLeiblerDivergence.png \"KL散度参考图\")\n",
    "      $$\n",
    "      \\begin{aligned}\n",
    "      & D_{KL}(P|Q) \\\\\n",
    "      & = \\sum\\limits_{i=1}^n p_i[f_Q(q_i) - f_P(p_i)] \\\\\n",
    "      & \\text{可以看到如果P和Q相同的情况下，那么}D_{KL}(P|Q)\\text{是等于0的。}\\\\\n",
    "      & = \\sum\\limits_{i=1}^n p_i[(-\\log_2 q_i) - (-\\log_2 p_i )] \\\\\n",
    "      & = \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) - \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i) \\\\\n",
    "      & \\because \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i)\\text{就是P的熵，在P已经确定的情况下，它就是一个恒定值。} \\\\\n",
    "      & \\therefore \\text{重点在于前面的部分。也就是}\\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) \\\\\n",
    "      & \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) \\text{就定义为交叉熵。} \\\\\n",
    "      & \\text{非常关键的地方在于：}\\sum\\limits_{i=1}^n p_i(-\\log_2 q_i)\\text{和}\\sum\\limits_{i=1}^n p_i(-\\log_2 p_i)\\text{之间的大小。}\\\\\n",
    "      & \\text{if } \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) > \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i) \\\\\n",
    "      & \\Rightarrow D_{KL}(P|Q) > 0 \\text{, 也就是说KL散度越大，代表了两个模型差别越大。}\\\\\n",
    "      & \\text{if } \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) < \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i) \\\\\n",
    "      & \\Rightarrow D_{KL}(P|Q) < 0 \\text{, 也就是说KL散度越小，代表了两个模型差别越大。}\\\\\n",
    "      & \\text{这样用交叉熵来表达损失函数时，希望交叉熵要么是比P的熵大，要么只比P的熵小。这样就不需要分段求解。}\\\\\n",
    "      & \\text{吉布斯不等式给出了明确证明。}\\\\\n",
    "      & \\text{if} \\sum\\limits_{i=1}^n p_i = \\sum\\limits_{i=1}^n q_i = 1\\text{, and }p_i,q_i \\in (0,1]\\\\\n",
    "      & \\Rightarrow \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i) \\leqslant \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i) \\text{, when } \\forall i \\; p_i = q_i ,\\; \\sum\\limits_{i=1}^n p_i(-\\log_2 p_i) = \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i)\\\\\n",
    "      & \\Rightarrow D_{KL}(P|Q) \\geqslant 0 \\\\\n",
    "      & \\text{如果希望P和Q相似，那么就希望}\\sum\\limits_{i=1}^n p_i(-\\log_2 q_i)\\text{这个值越小越好。}\\\\\n",
    "      & \\text{交叉熵} \\coloneqq \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i)\n",
    "      \\end{aligned} \n",
    "      $$\n",
    "   4. 可以理解为：Q想达到和P一样的信息分布时还差多少信息量。\n",
    "5. 交叉熵的应用\n",
    "   1. $H(P,Q) = \\sum\\limits_{i=1}^n p_i(-\\log_2 q_i)$这个n的取值是什么需要确定。这个地方在视频中的24:00，下面的评论说这个地方有错误。先给出结论：在P和Q两个模型中将数量大的那个带入n的位置即可。比较两幅图：![KL散度m<n](../../pictures/KullbackLeiblerDivergenceMLessThanN.png \"KL散度m<n\")![KL散度m>n](../../pictures/KullbackLeiblerDivergenceMGreaterThanN.png \"KL散度m>n\")\n",
    "   2. 推导\n",
    "      $$\n",
    "      \\begin{aligned}\n",
    "      & H(P,Q) \\\\\n",
    "      & = \\sum\\limits_{i=1,2} p_i(-\\log_2 q_i) \\\\\n",
    "      & = \\sum\\limits_{i=1,2} x_i(-\\log_2 q_i) \\\\\n",
    "      & \\text{当在二分类模型中，一部分是在$x_i$是1的时候，需要给出与1相近$y_i$等于的概率是多少。而当一部分是在$x_i$是0的时候，需要给出与0相近$y_i$的补集等于的概率是多少。}\\\\\n",
    "      & = - \\sum\\limits_{i=1,2} [x_i \\log_2 y_i + (1-x_i)\\log_2(1-y_i)] \\\\\n",
    "      \\end{aligned} \n",
    "      $$\n",
    "   3. 交叉熵和极大似然估计推导出来的公式形式上是一样的。它们的不同点在于：\n",
    "      1. 极大似然估计的$\\log$是为了方便计算而引入的，它的任务是把连乘换成连加。\n",
    "      2. 交叉熵中的$\\log$是写在定义中，以2为底代表了计算出来信息量的单位是比特。\n",
    "      3. 极大似然估计实际上应该是求最大值的，为了符合人日常的习惯，硬加的负号改为了求最小值。\n",
    "      4. 交叉熵中的负号是写在定义中的，为了满足信息量单调递减的要求。\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "三个重要的损失函数到确实都搞清楚了。1. 最小二乘；2. 极大似然估计；3.交叉熵。\n",
    "用自己的话表达一遍：\n",
    "1. 最小二乘是对两个向量大小的数学表达。\n",
    "2. 极大似然估计是在猜测了研究对象模型情况下如何对模型的参数进行估计的方法。\n",
    "3. 交叉熵是衡量两个分布如何相似的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.6 回答其中的问题\n",
    "\n",
    "1. 假设我们有一些数据$x_1,x_2,\\cdots,x_n \\in \\mathbb{R}$（也就是一维实数集数据）。我们的目标是找到一个常数，使得最小化$\\sum \\limits_{i=1}^n(x_i-b)^2$。\n",
    "   1. 找到最优值$b$的解析解。 \\\n",
    "        答：\n",
    "        $$\n",
    "         \\begin{equation}\n",
    "         \\begin{aligned}\n",
    "         & f(b) = \\sum \\limits_{i=1}^n(x_i-b)^2 \\\\\n",
    "         & \\text{Target is :} \\underset{\\boldsymbol{b}}{argmin} f(b) = \\underset{\\boldsymbol{b}}{argmin} \\sum \\limits_{i=1}^n(x_i-b)^2 \\\\\n",
    "         & \\frac{df}{db} = \\frac{d \\sum \\limits_{i=1}^n(x_i-b)^2}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n\\frac{d(x_i-b)^2}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n\\frac{d(x_i^2 - 2bx_i + b^2)}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n(\\frac{d x_i^2 }{db} - \\frac{d2bx_i}{db} + \\frac{db^2}{db}) \\\\\n",
    "         & \\because \\text{The equation is the derivative of b, so $x_i$ is a constant relative to b.}\\\\\n",
    "         & \\therefore = \\sum \\limits_{i=1}^n(C - 2x_i + 2b) \\;\\text{,C is a constant.} \\\\\n",
    "         &\\text{To minimize f, f is convex function, we set its derivatives to zero, and obtain the normal equations:} \\\\\n",
    "         & \\sum \\limits_{i=1}^n(C - 2x_i + 2b) = 0 \\\\\n",
    "         & nC - 2 \\sum \\limits_{i=1}^n x_i + 2nb = 0 \\\\\n",
    "         & \\because \\text{C, n, 2 are constant.} \\\\\n",
    "         & \\therefore b = \\frac{\\sum \\limits_{i=1}^n x_i}{n} + C \\\\ \n",
    "         & \\text{Completed.}\n",
    "         \\end{aligned}\n",
    "         \\end{equation}\n",
    "        $$\n",
    "   2. 这个问题及其解与正态分布有什么关系？\n",
    "      答：还不知道。\n",
    "2. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置b（我们可以通过向X添加所有值为1的一列来做到这一点）。\n",
    "   1，2，3已经在上面的cell中做出了答案。\n",
    "   4. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？\n",
    "      答：现在的理解是对凸函数时使用随机梯度下降更好。按照理解是不是在数据都比较平坦的时候会失效，因为找不到方向。\n",
    "\n",
    "3. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说$p(\\epsilon)=\\frac{1}{2}exp(-|\\epsilon|)$\n",
    "   1. 写出模型$-\\log P(\\boldsymbol{y}|\\boldsymbol{X})$下数据的负对数似然。\\\n",
    "      答： 就是对公式3.1.12到3.1.13的过程不理解导致的。\\\n",
    "      公式3.1.12到3.1.13中几个需要明确的问题：\n",
    "      1. $P(y|X)$和$P(\\epsilon)$是等价的。\n",
    "      2. $y=\\boldsymbol{w}^T \\boldsymbol{X} + b + \\epsilon$可以变换为$y - \\boldsymbol{w}^T \\boldsymbol{X} - b =\\epsilon$。可以理解为$\\epsilon$是由$\\boldsymbol{w}^T \\boldsymbol{X}$在估计$y$的时候产生的误差。由$\\boldsymbol{w}^T \\boldsymbol{X}$在估计$y$的时候产生的可以理解为是一种条件概率。这个误差在3.1.12之后说明服从正态分布。\n",
    "      推导如下：\n",
    "      $$\n",
    "      \\begin{equation}\n",
    "      \\begin{aligned}\n",
    "      & \\because \\epsilon \\backsim \\frac{1}{2}exp(-|\\epsilon|) \\\\\n",
    "      & \\therefore P(y|x) = \\frac{1}{2}exp(-|y-\\boldsymbol{w}^T \\boldsymbol{x} -b|) \\\\\n",
    "      & P(y|X) = \\prod \\limits_{i=1}^n \\frac{1}{2}exp(-|y_i-\\boldsymbol{w}^T x_i - b|) \\\\\n",
    "      & \\Rightarrow -\\ln P(y|X) = -\\sum\\limits_{i=1}^n [(-|y_i-\\boldsymbol{w}^T x_i - b|) -\\ln 2] \\\\\n",
    "      & = \\sum\\limits_{i=1}^n [(|y_i-\\boldsymbol{w}^T x_i - b|) + \\ln 2] \\\\\n",
    "      & = n \\ln2 + \\sum\\limits_{i=1}^n (|y_i-\\boldsymbol{w}^T x_i - b|)\\\\\n",
    "      & \\text{Completed 3.1.6 3.1. }\\\\\n",
    "      & \\text{问题是：这个条件概率和认为y是x的线性模型有什么关系吗？另外损失函数是人为定义的，依然还是可以定义为平方误差啊？难道说均方误差就不影响正态分布的噪声，而均方误差会影响指数分布的噪声？}\\\\\n",
    "      & \\text{损失函数是人为定义的，计算损失函数来求线性函数的参数时还是和在高斯噪声的假设下一样的。}\\\\\n",
    "      \\end{aligned}\n",
    "      \\end{equation}\n",
    "      $$\n",
    "      \n",
    "   2. 你能写出解析解吗？\n",
    "      $$\n",
    "      \\begin{equation}\n",
    "      \\begin{aligned}\n",
    "      & \\text{Target is :}\\underset{\\boldsymbol{w,b}}{argmin}\\sum\\limits_{i=1}^n (|y_i-\\boldsymbol{w}^T x_i - b|). \\\\\n",
    "      & \\text{Target is :}\\underset{\\boldsymbol{w,b}}{argmin} \\boldsymbol{L}(\\boldsymbol{w},\\boldsymbol{b}) = \\sum\\limits_{i=1}^n (|\\boldsymbol{Y}-\\boldsymbol{w} \\boldsymbol{X} - \\boldsymbol{b}|). \\\\\n",
    "      & \\text{注意，这不是损失函数，这是极大似然估计的过程。只不过将函数定义为了$\\boldsymbol{L}(\\boldsymbol{w},\\boldsymbol{b})$这种名称。}\\\\\n",
    "      & \\text{set :} \\boldsymbol{X} = \\begin{bmatrix}\n",
    "            1 & x_1 \\\\\n",
    "            1 & x_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            1 & x_n\n",
    "            \\end{bmatrix}; \n",
    "            \\boldsymbol{B} = \\begin{bmatrix}\n",
    "            b_1 & w_1 \\\\\n",
    "            b_2 & w_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            b_n & w_n\n",
    "            \\end{bmatrix} ^T; \\\\\n",
    "      & L(\\boldsymbol{B}) = \\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B}\\\\\n",
    "      & \\nabla_BL(\\boldsymbol{B}) = \\nabla_B (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B})\\\\\n",
    "      & \\text{set :}\\nabla_BL(\\boldsymbol{B}) = 0\\\\\n",
    "      & \\nabla_B (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B}) = 0\\\\\n",
    "      & \\Rightarrow \\boldsymbol{X} = 0 \\\\\n",
    "      & \\text{Completed.}\n",
    "      \\end{aligned}\n",
    "      \\end{equation}\n",
    "      $$\n",
    "  \n",
    "   3. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）你能解决这个问题吗？\n",
    "      答：还想不出来。问题也不知道在哪里出错了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3问的思考的逻辑是：\n",
    "1. 使用线性回归模型的是先观察数据，可能符合线性分布；\n",
    "2. 定义一个线性模型；\n",
    "3. 人为定义一个损失函数（均方损失函数）；\n",
    "4. 求模型中的参数。\n",
    "5. 如果存在噪声，那么即使加上了符合正态分布噪声，也不影响对参数的计算。这就是讨论噪声的意义。\n",
    "6. 那么第3问的问题本质不是在于噪声会影响x的分布（x的分布时独立于噪声的），而只是影响对参数的求解吗？\n",
    "   1. 极大似然估计是在对模型的参数进行估计（模型已经假设好了）。\n",
    "   2. 而线性模型的损失函数也是对模型参数进行估计的一种方式。\n",
    "   3. 两种方式求解参数的形式是一样的就进行了相互验证。（不知道这种理解对不对）\n",
    "7. 注意这句话“在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计”。而在我的推导里面**在指数噪声的假设下，最小均方误差与对线性模型的极大似然估计的形式并不相同**；均方误差是二次函数，而在指数噪声的假设下极大似然估计是一次函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考<https://blog.csdn.net/u010462995/article/details/70847146>中的解释：当噪声符合正态分布N(0,delta^2)时，因变量则符合正态分布N(ax(i)+b,delta^2)，其中预测函数y=ax(i)+b。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。\n",
    "\n",
    "在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。\n",
    "\n",
    "若本身样本不符合正态分布或不近似服从正态分布，则要采用其他的拟合方法，比如对于服从二项式分布的样本数据，可以采用logistics线性回归。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误的推导:\n",
    "1. 错误的地方在于自认为x是符合高斯分布的。题目就没有给出条件说x是符合哪种分布的。\n",
    "2. 可取的地方在于极大似然估计的思路是对的。使用极大似然估计的时候也是首先猜测一个模型，然后计算其中的参数。\n",
    "\n",
    "总体思路如下：\n",
    "1. 显然噪声和数据是相互独立的。如果真不是相互独立的，那么这里就武断的认为它们是相互独立的。\n",
    "2. 噪声符合指数分布，数据符合正态分布。那么就是找出一种模型可以满足它们两种分布的叠加。这个过程就是似然！\n",
    "3. 要做似然首先需要猜测它们叠加之后的模型。那么就需要先计算它们的数字特征。这里就需要计算它们的期望和方差。然后通过这两个数字特征来估计它们的模型。\n",
    "4. 在有符合的模型之后，通过似然来计算它们的概率密度函数，从而求出它们叠加之后的模型。\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\because \\epsilon \\backsim \\frac{1}{2}exp(-|\\epsilon|) \\;\\text{and} \\;x \\backsim \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)\\\\\n",
    "& \\boldsymbol{Y} = \\boldsymbol{X} +\\epsilon \\\\\n",
    "& \\because \\text{$\\epsilon$ and $\\boldsymbol{X}$ are independent of each other.} \\\\\n",
    "& \\therefore E(\\boldsymbol{Y}) = E(\\epsilon + \\boldsymbol{X}) = E(\\epsilon) + E(\\boldsymbol{X}) \\;\\text{ and }\\; D(\\boldsymbol{Y}) = D(\\epsilon + \\boldsymbol{X}) = D(\\epsilon) + D(\\boldsymbol{X}) \\\\\n",
    "& E(\\boldsymbol{Y}) = E(\\epsilon) + E(\\boldsymbol{X}) \\\\\n",
    "& = 1 + \\mu \\\\\n",
    "& D(\\boldsymbol{Y}) = D(\\epsilon) + D(\\boldsymbol{X}) \\\\\n",
    "& = 1 + \\sigma^2 \\\\\n",
    "& \\text{set: }\\; \\mu' = 1 + \\mu \\;\\text{ , }\\; \\sigma' =\\sqrt{1+\\sigma^2}\\\\\n",
    "& \\Rightarrow \\boldsymbol{Y} \\backsim \\mathcal{N}(\\mu', \\sigma') \\\\\n",
    "& \\text{如果这个推理是对的，按照推理，那么最终的标签也应该是符合高斯分布的。那也也可以使用均方误差作为损失函数来进行计算。}\\\\\n",
    "& \\text{这下面错得更离谱。瞎用乘法原理。}\\\\\n",
    "& \\text{进行n次实验，按照概率的乘法原理}\\\\\n",
    "& P(\\boldsymbol{Y}|\\boldsymbol{X})= \\prod \\limits_{i=1}^n p(y_i|x_i) \\\\\n",
    "& \\therefore -\\ln P(\\boldsymbol{Y}|\\boldsymbol{X}) = \\sum\\limits_{i=1}^n[\\frac{1}{2}\\log (2\\pi \\sigma'^2) + \\frac{1}{2\\sigma'^2}(y_i-\\mu')^2]\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 线性回归从零开始实现\n",
    "\n",
    "1. 生成1000个二维数据集$\\boldsymbol{X}\\in \\mathbb{R}^{1000 \\times 2}$。\n",
    "2. 模型参数$\\boldsymbol{w} = [2, -3.4]^T , b=4.2$。\n",
    "3. $\\epsilon \\backsim \\mathcal{N}(0, 0.01^2)$。\n",
    "4. $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w} + b + \\epsilon$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples): \n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    # num_examples数据数量，w.shape[0]表示另外一个维度。\n",
    "    # 初始化为全零矩阵。\n",
    "    X = tf.zeros((num_examples, w.shape[0]))\n",
    "    # 按照正态分布生成数据。\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    # 生成y。\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n",
    "    # print(y)\n",
    "    # 加上一个噪声。\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    # 将形状修改为列的形式。\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y\n",
    "\n",
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "# print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features[:, (0)].numpy(), labels.numpy(), 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features[:, (1)].numpy(), labels.numpy(), 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用GPU：中数据也是分成小批量的分别放到不同的GPU中。\n",
    "\n",
    "[yield的说明参见](../BasicKnowledgePoints/2Python.md)\n",
    "\n",
    "[yield的代码参见](../../codes/4BasicKnowledgePoints/4BasicPython.ipynb)中关于return和yield的代码。\n",
    "\n",
    "[tf.gather()的说明详见tensorflow中的代码](../../codes/8TensorFlowGuide/1TensorFlowFoundation.ipynb)。作用就是获取指定维度上的所有元素。以二维矩阵为例，输入多个行的index，就提取指定行的所有元素；所欲多个列的index，就提取指定列的所有元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    # 获取所有数据的index。\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序。\n",
    "    # random.shuffle()将indices列表中元素的顺序打乱了。\n",
    "    random.shuffle(indices)\n",
    "    # range(0, num_examples, batch_size)以0为下限，以num_examples为上限（不含num_examples），以batch_size为步长。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 每次取一个batch_size的样本。\n",
    "        # 样本数不能被批次数整除的时候提取出来的最后一个j的样本数比其他的小批量的样本数少。\n",
    "        j = tf.constant(indices[i: min(i + batch_size, num_examples)])\n",
    "        # 使用next()来一次一次的取出一个小批量的样本。\n",
    "        yield tf.gather(features, j), tf.gather(labels, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.06179392  0.28019327]\n",
      " [ 0.8674119  -0.94935095]\n",
      " [-0.97674966 -1.2697598 ]\n",
      " [-0.46796203 -1.2076122 ]\n",
      " [ 0.6196967   0.25969803]\n",
      " [ 0.5880746   0.26511076]\n",
      " [ 0.934167   -1.3496933 ]\n",
      " [-1.4634264   0.25826314]\n",
      " [ 0.15503864 -1.649009  ]\n",
      " [-1.6336747   1.6844238 ]], shape=(10, 2), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[ 3.1243377]\n",
      " [ 9.168131 ]\n",
      " [ 6.57205  ]\n",
      " [ 7.370267 ]\n",
      " [ 4.550445 ]\n",
      " [ 4.48085  ]\n",
      " [10.6481695]\n",
      " [ 0.4052545]\n",
      " [10.099318 ]\n",
      " [-4.7873726]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    # 取一次的验证一下打印出来。\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "书中说明：当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。**上面实现的迭代**对于教学来说很好，但它的**执行效率很低**，可能会在实际问题上陷入麻烦。例如，**它要求我们将所有数据加载到内存中，并执行大量的随机内存访问**。在深度学习框架中实现的内置迭代器效率要高得多，它可以处理存储在文件中的数据和数据流提供的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型参数。\n",
    "\n",
    "两种定义权重$\\boldsymbol{w}$的方式。后面3.2.9有练习需要回答。从结果来看两者并没有什么不同。\n",
    "\n",
    "1. 将权重初始化为$\\boldsymbol{w} \\backsim \\mathcal{N}(0, 0.01^2)$；\n",
    "2. 将权重初始化为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal(shape=(2, 1), mean=0, stddev=0.01), trainable=True)\n",
    "# w = tf.Variable(tf.zeros([2, 1], dtype=float), trainable=True)\n",
    "b = tf.Variable(tf.zeros(1), trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return tf.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化算法\n",
    "\n",
    "1. [zip的使用详见](../../codes/4BasicKnowledgePoints/4BasicPython.ipynb)中关于zip的说明。\n",
    "2. [param.assign_sub的使用详见中关于assign_sub函数的说明](../../codes/8TensorFlowGuide/1TensorFlowFoundation.ipynb)。param_1.assign_sub(param_2)的作用是将矩阵param_1**减去**矩阵param_2。实际上就是完成的Tensor之间的减法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    # print(type(params))\n",
    "    # params是list类型。\n",
    "    for param, grad in zip(params, grads):\n",
    "        # print(type(param))\n",
    "        # param是<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>类型。\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练\n",
    "\n",
    "执行的步骤如下：\n",
    "1. 初始化参数\n",
    "2. 重复以下训练，直到完成：\n",
    "   1. 计算梯度$\\boldsymbol{g} \\leftarrow \\partial_(\\boldsymbol{w}, \\boldsymbol{b})\\frac{1}{|B|}\\sum\\limits_{i\\in B}l(x_i, y_i, \\boldsymbol{w}, \\boldsymbol{b})$;\n",
    "   2. 更新参数$(\\boldsymbol{w},\\boldsymbol{b})\\leftarrow (\\boldsymbol{w},\\boldsymbol{b})-\\eta \\boldsymbol{g}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.038158\n",
      "epoch 2, loss 0.000146\n",
      "epoch 3, loss 0.000047\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "# 传指针了。\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with tf.GradientTape() as g:\n",
    "            l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 计算l关于[w,b]的梯度\n",
    "        dw, db = g.gradient(l, [w, b])\n",
    "        # 使用参数的梯度更新参数\n",
    "        sgd([w, b], [dw, db], lr, batch_size)\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: [ 8.332729e-05 -1.718998e-04]\n",
      "b的估计误差: [0.0006876]\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - tf.reshape(w, true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.9 回答其中的问题\n",
    "\n",
    "1. 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？\n",
    "   1. 将权重初始化为0时的误差：\n",
    "      w的估计误差: [-5.29289246e-05 -1.16586685e-04]\n",
    "      b的估计误差: [0.00093889]\n",
    "\n",
    "   2. 将权重初始化$\\boldsymbol{w} \\backsim \\mathcal{N}(0, 0.01^2)$是的误差：\n",
    "      w的估计误差: [-0.00013304  0.00035238]\n",
    "      b的估计误差: [0.00056028]\n",
    "   \n",
    "   答：通过实验可以看到算法依然有效。并没有发生想象中误差增大的情况，反而$\\boldsymbol{w}$的误差变小了。\n",
    "2. 假设你是乔治·西蒙·欧姆，试图为电压和电流的关系建立一个模型。你能使用自动微分来学习模型的参数吗?\n",
    "3. 您能基于普朗克定律使用光谱能量密度来确定物体的温度吗？\n",
    "4. 如果你想计算二阶导数可能会遇到什么问题？你会如何解决这些问题？\n",
    "5. 为什么在squared_loss函数中需要使用reshape函数？\n",
    "6. 尝试使用不同的学习率，观察损失函数值下降的快慢。\n",
    "7. 如果样本个数不能被批量大小整除，data_iter函数的行为会有什么变化？"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf831573a0ff294614842876d2763885d6da16fb80bd95fae4076843946dd1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
