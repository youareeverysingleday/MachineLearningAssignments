{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Linear Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 linear regression\n",
    "\n",
    "1. 放射变换（affline transformation）。线性回归假设的模型是输入特征的一个仿射变换。仿射变换的特点是通过加权和对特征的进行线性变化，并通过偏置项来进行平移得到的。\n",
    "2. 通常用$\\hat{y}$来表示估计值。\n",
    "3. 虽然我们相信给定$\\boldsymbol{x}$预测的最佳模型会是线性的， 但我们很难找到一个有个$n$样本的真实数据集，其中对于所有的$1\\leqslant i \\leqslant y$, $y^{(i)}$完全等于$\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+\\boldsymbol{b}$。无论我们使用什么手段来观察特征$\\boldsymbol{X}$和标签$\\boldsymbol{y}$，**都可能会出现少量的观测误差**。因此，即使确信特征与标签的潜在关系是线性的，我们也会加入一个噪声项来考虑观测误差带来的影响。\n",
    "4. 由于平方误差函数中的二次方项，估计值$\\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。\n",
    "    $$L(\\boldsymbol{w}, \\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n l^{(i)}(\\boldsymbol{w},\\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n \\frac{1}{2} (\\boldsymbol{w}^{T}\\boldsymbol{x}^{(i)}+\\boldsymbol{b}-\\boldsymbol{y}^{(i)})^2 \\tag{3.1.6}$$\n",
    "    在训练模型时，我们希望寻找一组参数$(\\boldsymbol{w}^*, \\boldsymbol{b}^*)$，这组参数能最小化在所有训练样本上的总损失。如下式：$\\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})$。\n",
    "5. 解析解：$\\boldsymbol{w}^* =(\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$。像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。**解析解可以进行很好的数学分析，但解析解对问题的限制很严格**（这个限制条件在于），导致它无法广泛应用在深度学习里。[解答参考文字](https://zhuanlan.zhihu.com/p/74157986)。[解答参考视频](https://www.bilibili.com/video/BV1ro4y1k7YA?spm_id_from=333.337.search-card.all.click)。\n",
    "   1. 这里如何得来的？证明如下。\n",
    "   2. 为什么还有$\\boldsymbol{X}^T$这一项？答：这是为了表示他们之间的距离（L2范数）。**两个形状相同向量之间的距离可以表示为一个向量的转置乘以另外一个向量**。推导过程如下：\n",
    "\n",
    "        $$\n",
    "        \\begin{equation}\n",
    "        \\begin{aligned}\n",
    "        \\text{Known:} & \\text{the sample space is}\\{(x_1,y_1),(x_2,y_2),\\cdots (x_n,y_n)\\}\\\\\n",
    "         & \\text{equation: }\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{B}\\\\\n",
    "         & \\boldsymbol{Y}=\\begin{pmatrix}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            y_n\n",
    "            \\end{pmatrix};\n",
    "        \\boldsymbol{X}=\\begin{bmatrix}\n",
    "            1 & x_1 \\\\\n",
    "            1 & x_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            1 & x_n\n",
    "            \\end{bmatrix};\n",
    "        \\boldsymbol{B}=\\begin{pmatrix}\n",
    "            \\alpha \\\\\n",
    "            \\beta\n",
    "            \\end{pmatrix}.\\\\\n",
    "         & \\alpha\\text{ is intercept,} \\beta \\text{ is slope.} \\\\\n",
    "         & \\therefore \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{B} + \\boldsymbol{\\gamma} \\\\\n",
    "         & \\boldsymbol{\\gamma} =\\begin{pmatrix}\n",
    "            \\epsilon_1 \\\\\n",
    "            \\epsilon_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\epsilon_n\n",
    "            \\end{pmatrix};\\\\\n",
    "\n",
    "        \\text{Target is: } & \\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})\\\\\n",
    "        \\text{solution: } & \\\\\n",
    "         & \\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}} = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b \\\\\n",
    "            \\vec{x}_2^T b \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b\n",
    "            \\end{bmatrix} - \\begin{bmatrix}\n",
    "            \\vec{y}_1 \\\\\n",
    "            \\vec{y}_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "        & = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "         & \\because \\vec{z}^T\\vec{z} = \\sum_i z_i^2;\\\\\n",
    "         & \\therefore \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "            \\end{bmatrix}_{L2} \\text{这个地方还有点问题。} \\\\\n",
    "         & = \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) = \\frac{1}{2} \\sum\\limits_{i=1}^n(\\vec{x}_i^T b - \\vec{y}_i)\\\\\n",
    "         & =L(\\boldsymbol{B})\\\\\n",
    "         & \\nabla_B L(\\boldsymbol{B}) = \\nabla_B \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B ((\\boldsymbol{X}\\boldsymbol{B})^T \\boldsymbol{X}\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B}) + \\vec{y}^T \\vec{y}) \\\\\n",
    "         & \\because \\vec{a}^T\\vec{b} = \\vec{b}^T\\vec{a} \\;\\text{ and }\\;\\vec{y}^T \\vec{y}\\text{ is independent of }\\boldsymbol{B}.\\\\\n",
    "         & \\because \\nabla_x \\vec{b}^T x =  \\vec{b}\\;\\text{ and }\\;\\nabla_x \\vec{x}^T \\boldsymbol{A}x =  2\\boldsymbol{A}\\vec{x}\\;\\text{ for symmetric matrix.}\\\\\n",
    "         & \\therefore = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})  - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2}(2(\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-2\\vec{y}^T \\boldsymbol{X})\\\\\n",
    "         & = (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-\\vec{y}^T \\boldsymbol{X}\\\\\n",
    "         & \\text{To minimize L, L is convex function, we set its derivatives to zero, and obtain the normal equations:}\\\\\n",
    "         & (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} = \\vec{y}^T\\boldsymbol{X} = \\boldsymbol{X}^T\\vec{y}\\\\\n",
    "         & \\Rightarrow \\boldsymbol{B} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\vec{y} \\\\\n",
    "         & \\text{Proof complete.}\n",
    "        \\end{aligned}\n",
    "        \\end{equation}\n",
    "        $$\n",
    "\n",
    "6. 随机梯度下降。梯度下降（gradient descent）的方法，这种方法**几乎可以优化所有深度学习模型**。它通过不断地在损失函数递减的方向上更新参数来降低误差。但实际中的执行可能会非常慢：**因为在每一次更新参数之前，我们必须遍历整个数据集**。 因此，我们通常会在每次需要计算更新的时候**随机抽取一小批样本**，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。\n",
    "   1. **即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值**。\n",
    "   2. 线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，**更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失**，这一挑战被称为泛化（generalization）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CommonCode import Timer\n",
    "import numpy \n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矢量化加速\n",
    "\n",
    "f'{timer.stop():.5f} sec' [python 3.6之后字符串格式化用法参考说明](https://geek-docs.com/python/python-tutorial/python-fstring.html#Python_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用for循环所消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10000,) dtype=float32, numpy=array([2., 2., 2., ..., 2., 2., 2.], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7.23041 sec'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "a = tf.ones(n)\n",
    "b = tf.ones(n)\n",
    "\n",
    "c = tf.Variable(tf.zeros(n))\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i].assign(a[i] + b[i])\n",
    "print(c)\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用矢量化之后消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000 sec'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。\\\n",
    "也就是说：**使用什么损失函数需要考虑噪声的分布情况**。\\\n",
    "\n",
    "极大似然估计（maximum likelihood estimation）的[详细说明详见](../../mathematics/ProbabilityTheory.md)中关于maximum likelihood estimation的说明。\\\n",
    "\n",
    "在3.1.3中说明了服从正态分布的噪声，最小化均方误差等价于对线性模型的极大似然估计。这个中间“可以写出通过给定的x观测到特定y的似然”的这句没有理解?????? \\\n",
    "\n",
    "对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。这里定义了全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计 Maximum Likilihood Estimation\n",
    "\n",
    "1. 一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。极大似然估计就是在估计模型的参数。也就是说**极大似然估计是已知模型和样本，来估计模型的参数**。\n",
    "2. 参考\n",
    "   1. 文字参考\n",
    "      1. [直接的说明](https://zhuanlan.zhihu.com/p/26614750)\n",
    "      2. [比较了概率和统计的区别，同时也说了极大似然估计的概念](https://blog.csdn.net/u011508640/article/details/72815981)。\n",
    "   2. 视频讲解的例子参考<https://www.bilibili.com/video/BV1Hb4y1m7rE?spm_id_from=333.337.search-card.all.click>。\n",
    "\n",
    "3. 离散值的例子：\n",
    "\n",
    "   设一个袋子里有非常多的小球（样本数量：非常大。也就忽略的放回抽样和不放回抽样），**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的**。其中有两种样本，一种是白球，定义为样本1；一种是红球，定义为样本2。两种样本数量的比值如下表所示，其中$\\theta$是未知的：\n",
    "\n",
    "   |X|样本1|样本2|\n",
    "   |---|---|---|\n",
    "   |P|$\\theta$|$1-\\theta$|\n",
    "\n",
    "   如何判断袋子中小球的分布呢？自然就想到的抽样的方式来对袋子中的小球的分布进行判断。现在假设从袋子中按照顺序抽出了5个小球，分别是1、1、2、1、2。那么它们的抽样情况如下表所示：\n",
    "\n",
    "   |抽样结构（按顺序）|1|1|2|1|2|\n",
    "   |---|---|---|---|---|---|\n",
    "   |抽出该样本的概率|$\\theta$|$\\theta$|$1-\\theta$|$\\theta$|$1-\\theta$|\n",
    "\n",
    "   抽出这个顺序样本的概率为$L(\\theta)=\\theta\\theta (1-\\theta)\\theta(1-\\theta)=\\theta^3(1-\\theta)^2$，定义$L(\\theta)$为似然函数。抽出这组样本的概率显然是会随着$\\theta$的变化而变化。随着$\\theta$的变化$L(\\theta)$有无数种值，然后按照“存在即合理”的思想，直接认为应该让$L(\\theta)$最大的$\\theta$为被选择的分布（实际上这个地方还是没有理解清楚其中的逻辑，但是先这样使用）。<https://zhuanlan.zhihu.com/p/26614750>中给出的说明是一个反问句式：“那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是极大似然估计的核心”。\\\n",
    "\n",
    "   这里进行说明为什么需要求最大。在视频<https://www.bilibili.com/video/BV1Y64y1Q7hi?spm_id_from=333.337.search-card.all.click>中的8:30开始说明这个问题。\n",
    "\n",
    "   描述性的说明，这个描述对于理解非常重要：\n",
    "   1. 在抛硬币的例子中，每次抛硬币事件的分布如下：\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|$\\theta$|$1-\\theta$|\n",
    "   2. 如果抛了10次，得到的10次结果为：7次正面，3次反面。\n",
    "   3. 因为只有2个样本，我们猜测模型是二项分布也就是(0-1)分布模型。这个时候就希望能将$\\theta$的值估计出来。\n",
    "   4. 这里需要强调的一点：\n",
    "      一种分布A如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.1|0.9|\n",
    "\n",
    "      一种分布B如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.7|0.3|\n",
    "\n",
    "      一种分布C如下：\n",
    "\n",
    "      |X|正面|反面|\n",
    "      |---|---|---|\n",
    "      |P|0.8|0.2|\n",
    "\n",
    "      在实际抛的过程中都是有可能出现7次正面，3次反面的情况的。只不过它们出现这种情况的概率不同而已。这个过程就是在某种模型的情况下求某次事件的条件概率。\n",
    "   5. 这个时候就需要通过已知的抽样结果：7次正面，3次反面。**来估计具有最大似然（这里表述为似然，而不是概率了。因为是从统计结果分析模型参数了。这里可以理解为可能性最大的结果。同样一个过程从模型到结果称为概率，从结果到模型称为似然）出现这种抽样结果的分布是什么样的**。这就是为什么叫**最大**似然估计的原因。\n",
    "   6. 最后的结果我们就直接认为是符合上述抽样结果的分布！注意，不能认为极大似然估计得出的模型参数就是真实的参数。因为模型对应的真实参数是没有办法确定的。只能估计其可能性最大值。当知道某种模型产生的结果然后去反推概率模型时，往往就会用到极大似然估计。这也是机器学习最重要的理论基础之一。\n",
    "\n",
    "   前提，MLP可以通过添加更多的层数来拟合任意概率模型的曲线。\n",
    "   推导过程如下：\n",
    "\n",
    "   $$\n",
    "   \\begin{equation}\n",
    "   \\begin{aligned}\n",
    "   & C_i\\text{表示的是事件，}\\theta\\text{表示的是概率模型的参数，这里代指概率模型。}\\\\\n",
    "   & P(C_1,C_2, \\cdots, C_n |\\theta)\\\\\n",
    "   & y_i\\text{表示的是标签，}\\hat{y}_i\\text{表示的是预测值，}\\boldsymbol{W}, \\boldsymbol{b}\\text{表示的是MLP的参数，这里代指神经网络模型。}\\\\\n",
    "   & P(y_1,y_2, \\cdots, y_n |\\boldsymbol{W}, \\boldsymbol{b})\\\\\n",
    "   & = \\prod \\limits_{i=1}^n P(y_i|\\boldsymbol{W}, \\boldsymbol{b})\\\\\n",
    "   & P(y_i|\\boldsymbol{W}, \\boldsymbol{b})\\text{表示在神经网络模型下，和标签对应的结果的概率分别是多少。当这个似然值最大的时候，就可以认定该模型和真实数据对应的模型是最接近的（甚至“武断”的认为就是一样的）。}\\\\\n",
    "   & = \\prod \\limits_{i=1}^n P(y_i|\\hat{y}_i)\\\\\n",
    "   & \\text{如果是一个二分类模型，那么就多次的二项分布（也就是0-1分布），也就是符合伯努利分布。}\\\\\n",
    "   & \\because \\text{伯努利分布为：} x_i \\in \\{0, 1\\};\n",
    "      f(x)=p^x (1-p)^x = \\begin{matrix}\n",
    "      & p, x=1 \\\\\n",
    "      & 1-p, x=0\n",
    "      \\end{matrix}\\\\\n",
    "   & \\therefore \\prod \\limits_{i=1}^n P(y_i|\\hat{y}_i) = \\prod \\limits_{i=1}^n \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i} \\\\\n",
    "   & \\text{对等式求对数。}\\\\\n",
    "   & \\log(\\prod \\limits_{i=1}^n \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i})\\\\\n",
    "   & = \\sum\\limits_{i=1}^n \\log(\\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i})\\\\\n",
    "   & = \\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i))\\\\\n",
    "   & \\text{目的是求上式的最大值：}max(\\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i)))\\\\\n",
    "   & \\text{一般习惯求最小值，所以上式变为：}min-(\\sum\\limits_{i=1}^n (y_i\\log(\\hat{y}_i) + (1-y_i)\\dot\\log(1-\\hat{y}_i)))\\\\\n",
    "   & \\text{Completed.}\n",
    "   \\end{aligned}\n",
    "   \\end{equation}\n",
    "   $$\n",
    "\n",
    "   下面是求$L(\\theta)=\\theta^3(1-\\theta)^2$最大极值点的具体步骤：\n",
    "\n",
    "      1. 由于$\\theta^3(1-\\theta)^2$直接求导不好处理，所以先将公式两边同时求$\\ln$;\n",
    "      2. 等式两边同时求对数。对数函数有个性质，一方面可以将连乘转化为加减；另一方面对数函数不会改变原函数的单调性的(不会改变原函数中点的相对大小)，因为以e为底的对数函数是单增的。等式变为$\\ln L(\\theta) = \\ln (\\theta^3(1-\\theta)^2) = 3\\ln\\theta + 2 \\ln (1-\\theta)$\n",
    "      3. 对$\\theta$求导数。$\\frac{d\\ln L(\\theta)}{d\\theta} = \\frac{3}{\\theta} - \\frac{2}{1-\\theta}$\n",
    "      4. 求极值点，令$\\frac{d\\ln L(\\theta)}{d\\theta} = \\frac{3}{\\theta} - \\frac{2}{1-\\theta} = 0$。求得极值点为$\\hat{\\theta} = \\frac{3}{5}$。\n",
    "4. 连续值的例子：\n",
    "\n",
    "   $X \\sim U(0, a), \\; \\text{a is unknow.} \\\\ f(x)=\\begin{cases} \\frac{1}{a}, & \\text{if a} \\in (0,a) \\\\ 0, & \\text{if a is others} \\end{cases}$。抽取n个样本点，对应的事件分别是$\\{X_1,X_2,\\cdots,X_n\\}$，事件对应的样本点分别是$\\{x_1, x_2, \\cdots ,x_n\\}$，对应每个样本点的概率密度为$f(x_1), f(x_2), \\cdots ,f(x_n)$。那么$\\{X_1,X_2,\\cdots,X_n\\}$的联合概率密度（联合概率就是多个事件同时发生时的概率）为$L(a) =f(x_1)f(x_2)\\cdots f(x_n) = \\frac{1}{a}\\frac{1}{a}\\cdots \\frac{1}{a}=\\frac{1}{a^n}$。\n",
    "   这里需要注意，不能再使用离散值时的例子了。因为先取对数$\\ln L(a) = -n\\ln a$，然后再求导$\\frac{d\\ln L(a)}{da}=\\frac{-n}{a}$，然后令$\\frac{d\\ln L(a)}{da}=\\frac{-n}{a} = 0$的条件是$a \\rightarrow +\\infty$。这显然是不合适的。\\\n",
    "   所以这里采用了另外一种方法。为了使得$L(a)=\\frac{1}{a^n}$取得最大值，就需要a尽可能的小。此时也就需要分析a的取值范围。因为a是一组已经抽样出来的点，而且a是在$\\{x_1, x_2, \\cdots ,x_n\\}$中的一个值。$\\{x_1, x_2, \\cdots ,x_n\\}$是已经存在的抽样样本，也就是已经是事实了。所以a只能取$max\\{x_1, x_2, \\cdots ,x_n\\}$，这样就可以使得“已经是事实”的事件成立（如果取得值小于$x_n$，那么$x_n$是如何取得的呢？）。所以$\\hat{a}=max\\{x_1, x_2, \\cdots ,x_n\\}$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.6 回答其中的问题\n",
    "\n",
    "1. 假设我们有一些数据$x_1,x_2,\\cdots,x_n \\in \\mathbb{R}$（也就是一维实数集数据）。我们的目标是找到一个常数，使得最小化$\\sum \\limits_{i=1}^n(x_i-b)^2$。\n",
    "   1. 找到最优值$b$的解析解。 \\\n",
    "        答：\n",
    "        $$\n",
    "         \\begin{equation}\n",
    "         \\begin{aligned}\n",
    "         & f(b) = \\sum \\limits_{i=1}^n(x_i-b)^2 \\\\\n",
    "         & \\text{Target is :} \\underset{\\boldsymbol{b}}{argmin} f(b) = \\underset{\\boldsymbol{b}}{argmin} \\sum \\limits_{i=1}^n(x_i-b)^2 \\\\\n",
    "         & \\frac{df}{db} = \\frac{d \\sum \\limits_{i=1}^n(x_i-b)^2}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n\\frac{d(x_i-b)^2}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n\\frac{d(x_i^2 - 2bx_i + b^2)}{db} \\\\\n",
    "         & = \\sum \\limits_{i=1}^n(\\frac{d x_i^2 }{db} - \\frac{d2bx_i}{db} + \\frac{db^2}{db}) \\\\\n",
    "         & \\because \\text{The equation is the derivative of b, so $x_i$ is a constant relative to b.}\\\\\n",
    "         & \\therefore = \\sum \\limits_{i=1}^n(C - 2x_i + 2b) \\;\\text{,C is a constant.} \\\\\n",
    "         &\\text{To minimize f, f is convex function, we set its derivatives to zero, and obtain the normal equations:} \\\\\n",
    "         & \\sum \\limits_{i=1}^n(C - 2x_i + 2b) = 0 \\\\\n",
    "         & nC - 2 \\sum \\limits_{i=1}^n x_i + 2nb = 0 \\\\\n",
    "         & \\because \\text{C, n, 2 are constant.} \\\\\n",
    "         & \\therefore b = \\frac{\\sum \\limits_{i=1}^n x_i}{n} + C \\\\ \n",
    "         & \\text{Completed.}\n",
    "         \\end{aligned}\n",
    "         \\end{equation}\n",
    "        $$\n",
    "   2. 这个问题及其解与正态分布有什么关系？\n",
    "      答：还不知道。\n",
    "2. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置b（我们可以通过向X添加所有值为1的一列来做到这一点）。\n",
    "   1，2，3已经在上面的cell中做出了答案。\n",
    "   4. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？\n",
    "      答：现在的理解是对凸函数时使用随机梯度下降更好。按照理解是不是在数据都比较平坦的时候会失效，因为找不到方向。\n",
    "\n",
    "3. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说$p(\\epsilon)=\\frac{1}{2}exp(-|\\epsilon|)$\n",
    "   1. 写出模型$-\\log P(\\boldsymbol{y}|\\boldsymbol{X})$下数据的负对数似然。\\\n",
    "      答： 就是对公式3.1.12到3.1.13的过程不理解导致的。\\\n",
    "      公式3.1.12到3.1.13中几个需要明确的问题：\n",
    "      1. $P(y|X)$和$P(\\epsilon)$是等价的。\n",
    "      2. $y=\\boldsymbol{w}^T \\boldsymbol{X} + b + \\epsilon$可以变换为$y - \\boldsymbol{w}^T \\boldsymbol{X} - b =\\epsilon$。可以理解为$\\epsilon$是由$\\boldsymbol{w}^T \\boldsymbol{X}$在估计$y$的时候产生的误差。由$\\boldsymbol{w}^T \\boldsymbol{X}$在估计$y$的时候产生的可以理解为是一种条件概率。这个误差在3.1.12之后说明服从正态分布。\n",
    "      推导如下：\n",
    "      $$\n",
    "      \\begin{equation}\n",
    "      \\begin{aligned}\n",
    "      & \\because \\epsilon \\backsim \\frac{1}{2}exp(-|\\epsilon|) \\\\\n",
    "      & \\therefore P(y|x) = \\frac{1}{2}exp(-|y-\\boldsymbol{w}^T \\boldsymbol{x} -b|) \\\\\n",
    "      & P(y|X) = \\prod \\limits_{i=1}^n \\frac{1}{2}exp(-|y_i-\\boldsymbol{w}^T x_i - b|) \\\\\n",
    "      & \\Rightarrow -\\ln P(y|X) = -\\sum\\limits_{i=1}^n [(-|y_i-\\boldsymbol{w}^T x_i - b|) -\\ln 2] \\\\\n",
    "      & = \\sum\\limits_{i=1}^n [(|y_i-\\boldsymbol{w}^T x_i - b|) + \\ln 2] \\\\\n",
    "      & = n \\ln2 + \\sum\\limits_{i=1}^n (|y_i-\\boldsymbol{w}^T x_i - b|)\\\\\n",
    "      & \\text{Completed 3.1.6 3.1. }\\\\\n",
    "      & \\text{问题是：这个条件概率和认为y是x的线性模型有什么关系吗？另外损失函数是人为定义的，依然还是可以定义为平方误差啊？难道说均方误差就不影响正态分布的噪声，而均方误差会影响指数分布的噪声？}\\\\\n",
    "      & \\text{损失函数是人为定义的，计算损失函数来求线性函数的参数时还是和在高斯噪声的假设下一样的。}\\\\\n",
    "      \\end{aligned}\n",
    "      \\end{equation}\n",
    "      $$\n",
    "      \n",
    "   2. 你能写出解析解吗？\n",
    "      $$\n",
    "      \\begin{equation}\n",
    "      \\begin{aligned}\n",
    "      & \\text{Target is :}\\underset{\\boldsymbol{w,b}}{argmin}\\sum\\limits_{i=1}^n (|y_i-\\boldsymbol{w}^T x_i - b|). \\\\\n",
    "      & \\text{Target is :}\\underset{\\boldsymbol{w,b}}{argmin} \\boldsymbol{L}(\\boldsymbol{w},\\boldsymbol{b}) = \\sum\\limits_{i=1}^n (|\\boldsymbol{Y}-\\boldsymbol{w} \\boldsymbol{X} - \\boldsymbol{b}|). \\\\\n",
    "      & \\text{注意，这不是损失函数，这是极大似然估计的过程。只不过将函数定义为了$\\boldsymbol{L}(\\boldsymbol{w},\\boldsymbol{b})$这种名称。}\\\\\n",
    "      & \\text{set :} \\boldsymbol{X} = \\begin{bmatrix}\n",
    "            1 & x_1 \\\\\n",
    "            1 & x_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            1 & x_n\n",
    "            \\end{bmatrix}; \n",
    "            \\boldsymbol{B} = \\begin{bmatrix}\n",
    "            b_1 & w_1 \\\\\n",
    "            b_2 & w_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            b_n & w_n\n",
    "            \\end{bmatrix} ^T; \\\\\n",
    "      & L(\\boldsymbol{B}) = \\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B}\\\\\n",
    "      & \\nabla_BL(\\boldsymbol{B}) = \\nabla_B (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B})\\\\\n",
    "      & \\text{set :}\\nabla_BL(\\boldsymbol{B}) = 0\\\\\n",
    "      & \\nabla_B (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{B}) = 0\\\\\n",
    "      & \\Rightarrow \\boldsymbol{X} = 0 \\\\\n",
    "      & \\text{Completed.}\n",
    "      \\end{aligned}\n",
    "      \\end{equation}\n",
    "      $$\n",
    "  \n",
    "   3. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）你能解决这个问题吗？\n",
    "      答：还想不出来。问题也不知道在哪里出错了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3问的思考的逻辑是：\n",
    "1. 使用线性回归模型的是先观察数据，可能符合线性分布；\n",
    "2. 定义一个线性模型；\n",
    "3. 人为定义一个损失函数（均方损失函数）；\n",
    "4. 求模型中的参数。\n",
    "5. 如果存在噪声，那么即使加上了符合正态分布噪声，也不影响对参数的计算。这就是讨论噪声的意义。\n",
    "6. 那么第3问的问题本质不是在于噪声会影响x的分布（x的分布时独立于噪声的），而只是影响对参数的求解吗？\n",
    "   1. 极大似然估计是在对模型的参数进行估计（模型已经假设好了）。\n",
    "   2. 而线性模型的损失函数也是对模型参数进行估计的一种方式。\n",
    "   3. 两种方式求解参数的形式是一样的就进行了相互验证。（不知道这种理解对不对）\n",
    "7. 注意这句话“在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计”。而在我的推导里面**在指数噪声的假设下，最小均方误差与对线性模型的极大似然估计的形式并不相同**；均方误差是二次函数，而在指数噪声的假设下极大似然估计是一次函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误的推导:\n",
    "1. 错误的地方在于人为x是符合高斯分布的。题目就没有给出条件说x是符合哪种分布的。\n",
    "2. 可取的地方在于极大似然估计的思路是对的。使用极大似然估计的时候也是首先猜测一个模型，然后计算其中的参数。\n",
    "\n",
    "总体思路如下：\n",
    "1. 显然噪声和数据是相互独立的。如果真不是相互独立的，那么这里就武断的认为它们是相互独立的。\n",
    "2. 噪声符合指数分布，数据符合正态分布。那么就是找出一种模型可以满足它们两种分布的叠加。这个过程就是似然！\n",
    "3. 要做似然首先需要猜测它们叠加之后的模型。那么就需要先计算它们的数字特征。这里就需要计算它们的期望和方差。然后通过这两个数字特征来估计它们的模型。\n",
    "4. 在有符合的模型之后，通过似然来计算它们的概率密度函数，从而求出它们叠加之后的模型。\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\because \\epsilon \\backsim \\frac{1}{2}exp(-|\\epsilon|) \\;\\text{and} \\;x \\backsim \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)\\\\\n",
    "& \\boldsymbol{Y} = \\boldsymbol{X} +\\epsilon \\\\\n",
    "& \\because \\text{$\\epsilon$ and $\\boldsymbol{X}$ are independent of each other.} \\\\\n",
    "& \\therefore E(\\boldsymbol{Y}) = E(\\epsilon + \\boldsymbol{X}) = E(\\epsilon) + E(\\boldsymbol{X}) \\;\\text{ and }\\; D(\\boldsymbol{Y}) = D(\\epsilon + \\boldsymbol{X}) = D(\\epsilon) + D(\\boldsymbol{X}) \\\\\n",
    "& E(\\boldsymbol{Y}) = E(\\epsilon) + E(\\boldsymbol{X}) \\\\\n",
    "& = 1 + \\mu \\\\\n",
    "& D(\\boldsymbol{Y}) = D(\\epsilon) + D(\\boldsymbol{X}) \\\\\n",
    "& = 1 + \\sigma^2 \\\\\n",
    "& \\text{set: }\\; \\mu' = 1 + \\mu \\;\\text{ , }\\; \\sigma' =\\sqrt{1+\\sigma^2}\\\\\n",
    "& \\Rightarrow \\boldsymbol{Y} \\backsim \\mathcal{N}(\\mu', \\sigma') \\\\\n",
    "& \\text{这下面错得更离谱。瞎用乘法原理。}\\\\\n",
    "& \\text{进行n次实验，按照概率的乘法原理}\\\\\n",
    "& P(\\boldsymbol{Y}|\\boldsymbol{X})= \\prod \\limits_{i=1}^n p(y_i|x_i) \\\\\n",
    "& \\therefore -\\ln P(\\boldsymbol{Y}|\\boldsymbol{X}) = \\sum\\limits_{i=1}^n[\\frac{1}{2}\\log (2\\pi \\sigma'^2) + \\frac{1}{2\\sigma'^2}(y_i-\\mu')^2]\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 线性回归从零开始实现"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
