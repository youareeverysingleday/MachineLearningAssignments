{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Linear Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression\n",
    "\n",
    "1. 放射变换（affline transformation）。线性回归假设的模型是输入特征的一个仿射变换。仿射变换的特点是通过加权和对特征的进行线性变化，并通过偏置项来进行平移得到的。\n",
    "2. 通常用$\\hat{y}$来表示估计值。\n",
    "3. 虽然我们相信给定$\\boldsymbol{x}$预测的最佳模型会是线性的， 但我们很难找到一个有个$n$样本的真实数据集，其中对于所有的$1\\leqslant i \\leqslant y$, $y^{(i)}$完全等于$\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+\\boldsymbol{b}$。无论我们使用什么手段来观察特征$\\boldsymbol{X}$和标签$\\boldsymbol{y}$，**都可能会出现少量的观测误差**。因此，即使确信特征与标签的潜在关系是线性的，我们也会加入一个噪声项来考虑观测误差带来的影响。\n",
    "4. 由于平方误差函数中的二次方项，估计值$\\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。\n",
    "    $$L(\\boldsymbol{w}, \\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n l^{(i)}(\\boldsymbol{w},\\boldsymbol{b})=\\frac{1}{n}\\sum\\limits_{i=1}^n \\frac{1}{2} (\\boldsymbol{w}^{T}\\boldsymbol{x}^{(i)}+\\boldsymbol{b}-\\boldsymbol{y}^{(i)})^2 \\tag{3.1.6}$$\n",
    "    在训练模型时，我们希望寻找一组参数$(\\boldsymbol{w}^*, \\boldsymbol{b}^*)$，这组参数能最小化在所有训练样本上的总损失。如下式：$\\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})$。\n",
    "5. 解析解：$\\boldsymbol{w}^* =(\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$。像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。**解析解可以进行很好的数学分析，但解析解对问题的限制很严格**（这个限制条件在于），导致它无法广泛应用在深度学习里。[解答参考文字](https://zhuanlan.zhihu.com/p/74157986)。[解答参考视频](https://www.bilibili.com/video/BV1ro4y1k7YA?spm_id_from=333.337.search-card.all.click)。\n",
    "   1. 这里如何得来的？证明如下。\n",
    "   2. 为什么还有$\\boldsymbol{X}^T$这一项？答：这是为了表示他们之间的距离（L2范数）。**两个形状相同向量之间的距离可以表示为一个向量的转置乘以另外一个向量**。推导过程如下：\n",
    "\n",
    "        $$\n",
    "        \\begin{cases}\n",
    "        \\text{Known:} & \\text{the sample space is}\\{(x_1,y_1),(x_2,y_2),\\cdots (x_n,y_n)\\}\\\\\n",
    "         & \\text{equation: }\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{B}\\\\\n",
    "         & \\boldsymbol{Y}=\\begin{pmatrix}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            y_n\n",
    "            \\end{pmatrix};\n",
    "        \\boldsymbol{X}=\\begin{bmatrix}\n",
    "            1 & x_1 \\\\\n",
    "            1 & x_2 \\\\\n",
    "            & \\vdots \\\\\n",
    "            1 & x_n\n",
    "            \\end{bmatrix};\n",
    "        \\boldsymbol{B}=\\begin{pmatrix}\n",
    "            \\alpha \\\\\n",
    "            \\beta\n",
    "            \\end{pmatrix}.\\\\\n",
    "         & \\alpha\\text{ is intercept,} \\beta \\text{ is slope.} \\\\\n",
    "         & \\therefore \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{B} + \\boldsymbol{\\gamma} \\\\\n",
    "         & \\boldsymbol{\\gamma} =\\begin{pmatrix}\n",
    "            \\epsilon_1 \\\\\n",
    "            \\epsilon_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\epsilon_n\n",
    "            \\end{pmatrix};\\\\\n",
    "\n",
    "        \\text{Target is: } & \\boldsymbol{w}^*, \\boldsymbol{b}^* =\\underset{\\boldsymbol{w},\\boldsymbol{b}}{argmin}L(\\boldsymbol{w},\\boldsymbol{b})\\\\\n",
    "        \\text{solution: } & \\\\\n",
    "         & \\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}} = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b \\\\\n",
    "            \\vec{x}_2^T b \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b\n",
    "            \\end{bmatrix} - \\begin{bmatrix}\n",
    "            \\vec{y}_1 \\\\\n",
    "            \\vec{y}_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "        & = \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "        \\end{bmatrix}\\\\\n",
    "         & \\because \\vec{z}^T\\vec{z} = \\sum_i z_i^2;\\\\\n",
    "         & \\therefore \\begin{bmatrix}\n",
    "            \\vec{x}_1^T b - \\vec{y}_1\\\\\n",
    "            \\vec{x}_2^T b - \\vec{y}_2\\\\\n",
    "            \\vdots \\\\\n",
    "            \\vec{x}_n^T b - \\vec{y}_n\n",
    "            \\end{bmatrix} \\\\\n",
    "         & = \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) = \\frac{1}{2} \\sum\\limits_{i=1}^n(\\vec{x}_i^T b - \\vec{y}_i)\\\\\n",
    "         & =L(\\boldsymbol{B})\\\\\n",
    "         & \\nabla_B L(\\boldsymbol{B}) = \\nabla_B \\frac{1}{2} (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}})^T (\\boldsymbol{X}\\boldsymbol{B} - \\vec{\\boldsymbol{y}}) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B ((\\boldsymbol{X}\\boldsymbol{B})^T \\boldsymbol{X}\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B}) + \\vec{y}^T \\vec{y}) \\\\\n",
    "         & \\because \\vec{a}^T\\vec{b} = \\vec{b}^T\\vec{a} \\;\\text{ and }\\;\\vec{y}^T \\vec{y}\\text{ is independent of }\\boldsymbol{B}.\\\\\n",
    "         & \\because \\nabla_x \\vec{b}^T x =  \\vec{b}\\;\\text{ and }\\;\\nabla_x \\vec{x}^T \\boldsymbol{A}x =  2\\boldsymbol{A}\\vec{x}\\;\\text{ for symmetric matrix.}\\\\\n",
    "         & \\therefore = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - (\\boldsymbol{X}\\boldsymbol{B})^T \\vec{y} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2} \\nabla_B (\\boldsymbol{B}^T (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})  - \\vec{y}^T(\\boldsymbol{X}\\boldsymbol{B})) \\\\\n",
    "         & = \\frac{1}{2}(2(\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-2\\vec{y}^T \\boldsymbol{X})\\\\\n",
    "         & = (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B}-\\vec{y}^T \\boldsymbol{X}\\\\\n",
    "         & \\text{To minimize L, L is convex function, we set its derivatives to zero, and obtain the normal equations:}\\\\\n",
    "         & (\\boldsymbol{X}^T \\boldsymbol{X})\\boldsymbol{B} = \\vec{y}^T\\boldsymbol{X} = \\boldsymbol{X}^T\\vec{y}\\\\\n",
    "         & \\Rightarrow \\boldsymbol{B} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\vec{y} \\\\\n",
    "         & \\text{Proof complete.}\n",
    "        \\end{cases}\n",
    "        $$\n",
    "6. 随机梯度下降。梯度下降（gradient descent）的方法，这种方法**几乎可以优化所有深度学习模型**。它通过不断地在损失函数递减的方向上更新参数来降低误差。但实际中的执行可能会非常慢：**因为在每一次更新参数之前，我们必须遍历整个数据集**。 因此，我们通常会在每次需要计算更新的时候**随机抽取一小批样本**，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。\n",
    "   1. **即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值**。\n",
    "   2. 线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。事实上，**更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失**，这一挑战被称为泛化（generalization）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CommonCode import Timer\n",
    "import numpy \n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矢量化加速\n",
    "\n",
    "f'{timer.stop():.5f} sec' [python 3.6之后字符串格式化用法参考说明](https://geek-docs.com/python/python-tutorial/python-fstring.html#Python_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用for循环所消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10000,) dtype=float32, numpy=array([2., 2., 2., ..., 2., 2., 2.], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7.23041 sec'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "a = tf.ones(n)\n",
    "b = tf.ones(n)\n",
    "\n",
    "c = tf.Variable(tf.zeros(n))\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i].assign(a[i] + b[i])\n",
    "print(c)\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用矢量化之后消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000 sec'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。\\\n",
    "也就是说：**使用什么损失函数需要考虑噪声的分布情况**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大似然估计（maximum likelihood estimation）的[详细说明详见](../../mathematics/ProbabilityTheory.md)中关于maximum likelihood estimation的说明。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d94ea807e9dd88dec85d6135010093db08445b4f78f2386ac1d177de969ce657"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
