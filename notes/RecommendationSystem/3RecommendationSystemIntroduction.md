# 推荐系统

1. 参考书：《推荐系统实践》，作者：项亮

## 分类

1. 按应用场景分类
   1. 个性化推荐
   2. 相关推荐
   3. 热门推荐
2. 按使用技术分类
   1. 基于item的推荐
   2. 基于user的推荐（也就是基于用户好友关系的推荐）
   3. 基于协同过滤的推荐

## 1. 好的推荐系统

1. 推荐系统和搜索引擎是互补的关系。使用搜索引擎时，用户在知道准确的关键词。在用户无法提供关键词时，推荐系统通过通过分析用户的历史行为给用户的兴趣建模来满足用户的兴趣需求。
2. 推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。
3. 从物品的角度出发，推荐系统可以更好地发掘物品的长尾（long tail）。
4. 推荐算法的本质是通过一定的方式将用户和物品联系起来，而不同的推荐系统利用了不同的方式。
5. 推荐系统应用都是由前台的展示页面、后台的日志系统以及推荐算法系统3部分构成的。
6. 针对不同的推荐应用场景，推荐算法有很大的不同。比如音乐、电影、电子商务这三种物品本身的属性差别就非常大。其他的比如广告推荐、位置推荐等，应用场景的不同、需要推荐物品的不同对推荐算法的影响非常大。

### 评测标准

1. 括准确度、覆盖度、新颖度、惊喜度、信任度、透明度等。这**些指标中，有些可以离线计算，有些只有在线才能计算，有些只能通过用户问卷获得**。
2. 主要有3种评测推荐效果的实验方法，即离线实验（offline experiment）、用户调查（user study）和在线实验（online experiment）。
3. 离线实验的缺点：无法计算商业上关心的指标。离线实验的指标和商业指标存在差距。
4. 用户调查的优缺点也很明显。它的优点是可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。缺点是**招募测试用户代价较大**，很难组织大规模的测试用户，因此会使测试结果的**统计意义不足**。此外，在很多时候设计双盲实验非常困难，而且**用户在测试环境下的行为和真实环境下的行为可能有所不同**，因而在测试环境下收集的测试指标可能在真实环境下无法重现。
5. 在线实验，一般采用AB测试的方式。
   1. AB测试：通过规则将用户分为几组，对不同的组采用不同的算法，然后对比各个组之间的评价指标。
      1. 优点：以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。
      2. 缺点：是周期比较长，必须进行长期的实验才能得到可靠的结果。
      3. 适用场景：因此一般不会用AB测试测试所有的算法，而只是用它测试那些在离线实验和用户调查中表现很好的算法。
6. 推荐算法上线的步骤：
   1. 首先，需要通过离线实验证明它在很多离线指标上优于现有的算法。
   2. 然后，需要通过用户调查确定它的用户满意度不低于现有的算法。
   3. 最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。
7. 用户满意度。无法通过离线计算时获得。大部分需要通过用户调查或者在线实验的形式获得。
8. 预测准确度。这是最重要的推荐系统**离线**评测指标。
   1. 评分预测：预测用户对物品评分的行为。一般使用均方根误差和平均绝对误差来衡量。就是L1和L2范数。
      1. **Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚）**，因而对系统的评测更加苛刻。研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整会降低MAE的误差。
   2. TopN推荐：一般是给用户一个个性化的推荐列表。TopN推荐的预测准确率一般通过准确率（precision）/召回率（recall）度量。令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。那么，推荐结果的召回率定义为：
      $$Recall = \frac{\sum \limits_{u\in U}|R(u) \cap T(u)|}{\sum \limits_{u\in U}|T(u)|}$$
      推荐结果的准确率为：
      $$Percision = \frac{\sum \limits_{u\in U}|R(u) \cap T(u)|}{\sum \limits_{u\in U}|R(u)|}$$
   3. 覆盖率coverage：描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。假设系统的用户集合为$U$，推荐系统给每个用户推荐一个长度为N的物品列表$R(u)$。那么推荐系统的覆盖率可以通过下面的公式计算：$Coverage = \frac{|U_{u\in U}R(u)|}{I}$。**覆盖率是一个内容提供商会关心的指标**。两个具体的参数来衡量覆盖率：
      1. 信息熵：$H=-\sum \limits_{i=1}^n p(i)\log p(i)$，其中$p(i)$是物品$i$的流行度除以所有物品流行度之和。
      2. 基尼系数（Gini Index）：$G=\frac{1}{n-1}\sum \limits_{j=1}^n (2j-n-1)p(i)$，其中，$i_j$是按照物品流行度$p()$从小到大排序的物品列表中第$j$个物品。
   4. 多样性diversity：即推荐结果需要具有多样性。多样性推荐列表的好处用一句俗话表述就是“不在一棵树上吊死”。多样性描述了推荐列表中物品两两之间的不相似性。假设$s(i, j)\in [0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下：$Diversity = 1 - \frac{\sum \limits_{i,j \in R(u), i \neq j}s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)}$。而推荐系统的整体多样性可以定义为所有用户推荐列表多样性的平均值：$AllDiversity = \frac{1}{|U|} \sum \limits_{u \in U}Diversity(R(U))$。
      1. 总结：即需要考虑用户的主要兴趣点，又需要考虑用户的次要兴趣点。而且需要考虑时间特性，也就是不同时间点上用户的兴趣不同。在不考虑时间的情况下，需要满足用户兴趣的分布。在考虑时间的情况下，是不同时间点上不同的兴趣分布。
      2. 这个例子举得非常清晰：假设用户喜欢动作片和动画片，且用户80%的时间在看动作片，20%的时间在看动画片。那么，可以提供4种不同的推荐列表：A列表中有10部动作片，没有动画片；B列表中有10部动画片，没有动作片；C列表中有8部动作片和2部动画片；D列表有5部动作片和5部动画片。在这个例子中，一般认为C列表是最好的，因为它具有一定的多样性，但又考虑到了用户的主要兴趣。A满足了用户的主要兴趣，但缺少多样性，D列表过于多样，没有考虑到用户的主要兴趣。B列表即没有考虑用户的主要兴趣，也没有多样性，因此是最差的。
   5. 新颖性：新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。具体评价方法：
      1. 比较简单、粗糙的方法：。评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。但是，用推荐结果的平均流行度度量新颖性比较粗略，**因为不同用户不知道的东西是不同的。因此，要准确地统计新颖性需要做用户调查**。
      2. 难点：**通过牺牲精度来提高多样性和新颖性是很容易的，而困难的是如何在不牺牲精度的情况下提高多样性和新颖性**。关心这两个指标的读者可以关注一下这个研讨会最终发表的论文。
      3. 参考论文：
         1. [Music Recommendation and Discovery in the Long Tail]<http://mtg.upf.edu/static/media/PhD_ocelma.pdf>
         2. [International Workshop on Novelty and Diversity in Recommender Systems]<http://ir.ii.uam.es/divers2011/>
   6. 惊喜度serendipity：
      1. 度量方法：没有。目前并没有什么公认的惊喜度指标定义方式。
      2. 一句话总结：对于将不属于用户兴趣分布范围之内的物品推荐给用户，同时用户对这个物品还非常满意，可以认为是一种惊喜。
      3. 惊喜度和新颖性的区别：符合用户兴趣分布之内，但是用户之前从来没有用过的称为新颖性。重要的是如何度量物品之间的在某个重要特征之间的差别。
      4. 参考论文
         1. Guy Shani和 Asela Gunawardana的“Evaluating Recommendation Systems”。

## 问题

1. 覆盖率还需要好好研究一下。