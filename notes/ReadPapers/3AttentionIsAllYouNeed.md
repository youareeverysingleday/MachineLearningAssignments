# 读论文笔记

Transformer实现。

## 1. 重要知识点

1. 序列转录模型。也就是给一个序列转换为另外一个序列。翻译就属于序列转录模型。这种模型一般使用包含encode和decode结构的循环或者CNN来实现。在transformer之前的也会运用到attention机制（注意力机制）。
2. Transformer将所有的循环层都换成了multi-head self-attention。作者希望生成的数据不那么序列化。
3. RNN的特点是在一个序列中从左往右一步一步的向前做。假如是一个句子的话，RNN会一个词一个词的向前看。对于第t个词，RNN会计算一个输出叫做$h_t$为这个词的隐藏状态。$h_t$是有前面一个词的隐藏状态$h_{t-1}$和当前第t个词来决定的。这样就可以通过RNN学到的前面词的信息通过$h_{t-1}$来放到当下，然和后当前的词进行交互。这也是RNN能够有效处理时序信息的关键所在。不足之处：
   1. RNN把之前的信息放在隐藏状态里面然后逐个传递下去。这也导致了RNN的不足，由于是逐个处理的，所以导致了RNN比较难以并行。在计算第t个词的时候，必须保证前面的t-1个词已经输入完成了。这导致了在时域上无法并行。
   2. 由于历史信息是一步一步向后传递的，如果序列比较长的话，比如时序序列很长，就会导致早期的时序信息，在后面计算的时候可能会已经被丢掉了。如果不想丢掉的可能得要$h_t$要比较大。如果使用了比较大的$h_t$了，那么在每个时间都需要把$h_t$存下来，导致了非常大的内存消耗。
4. 之前的attention机制主要是用在RNN中的encode-decode上，用途是将encode的信息更好的传递给decode。
5. 卷积神经网络比较难以对较长的时序建模。因为卷积核是一个非常小的模块，如果要将两个间隔比较远的元素联系起来的时候需要上升到比较高的卷积层才能看到。而transformer每次都可以看到所有的元素。

## 2. 问题

1. BLEU score具体是什么评价标准？

## 3. 其他

1. 四类基础模型：MLP、CNN、RNN和Transformer。
2. 通常代码都会放在摘要的最后一句话。