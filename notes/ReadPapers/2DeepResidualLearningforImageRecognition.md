# 读论文笔记

这篇论文是ResNet的实现。

## 1. 重要知识点

1. 网络比较深的时候就会出现一个问题梯度爆炸或者梯度消失。解决的办法是：第一，权重在初始化的时候做得好一点，也就是说在随机初始化的时候权重不要太大也不要太小。第二，在层中间加一些batch normalization(批标准化)用于效验每一层的输出和它的梯度的均值和方差，从而是的权重不会出现分布不均匀的情况。使用了上面两种方法之后网络可以训练了（网络收敛了），但是网络的性能变差了。这种情况不是因为层数变多之后导致的过拟合，而是训练和测试误差都会变高。
2. identity mapping，按道理说在浅的网络添加了更多的层之后，网络还应该是identity mapping（恒等映射），但是实际情况不是这样的。
3. 这篇文章提出了一种方法可以显式的构造一个identity mapping。使得深的网络不会比浅的网络变得更差。核心思想：
   1. 假设已经有一个相对浅的网络已经学习到了相关的权重，并且输出设置为$x$；
   2. 假设理想的输出时$H(x)$；
   3. 那么新加的层不是对所有的权重进行训练，而是只需要训练$H(x)-x$即可。这里设$F(x)=H(x)-x$。
   4. 新加层之后的输出按照3中的说法就是$F(x)+x$。也就是说这两层的输出不再是总体网络的最后输出，而是需要加上这两层的输入。这就是残差。
4. 使用residual是机器学习里面比较常用的一个思路。比如gradient boosting就是通过不停的迭代残差来学习一个网络将弱的分类器叠加起来变成强的分类器。
5. 残差连接如何处理输入和输出形状不相等的情况，这篇文章提供了2个方案：
   1. 第一个方案是在输入和输出上分别添加一些额外的0，使得输入和输出时形状相等的。
   2. 通过$1*1$的卷积来调整输入和输出的形状，从而使得两者的形状相等。这里之所以用$1*1$的卷积，是因为这个时候的输入和输出是在通道维度上不同。也就是说在做卷积的时候是将空间进行了压缩，但是通道维度上进行了拓展。NIN网络中用$1*1$的卷积也是起到类似的作用。一般这种情况下将通道数量上的增加会使得空间维度数值的减少，这个时候通过跨度来降低空间维度上数值。
6. 卷积层浮点运算次数：输入它的高乘以它的宽乘以通道数乘以输出通道数再乘以核的窗口的高和宽。这是一个卷积层的浮点运算次数。如果有多层就再乘以层数。
7. 在训练的期间什么时候降低学习率。在错误率平稳的时候要多训练一段时间之后再调整学习率，因为这个过程中网络中的参数还在做细微的调整。如果直接降低可能导致后期收敛无力（这个确实还没有遇到）。
8. bottleneck的设计。
9. 具体网络中的层的数量为什么要这样设计，作者并没有说。这种数量的设计是调参的结果。
10. ImageNet上大约有1%的打标错误率。
11. 数学原理解释为什么使用ResNet之后梯度消失会被解决：
    $$
    \begin{aligned}
       & \text{设输入为:}x, \text{浅层网络为：}g(x), \text{新加的层为：}f(g(x)); \\
       & \text{那么计算梯度即为：} \frac{\partial f(g(x))}{\partial x} = \frac{\partial f(g(x))}{\partial g(x)} \cdot \frac{\partial g(x)}{\partial x} \\
       & \text{使用残差网络之后计算梯度为：}\frac{\partial (f(g(x)) + g(x))}{\partial x} = \frac{\partial f(g(x))}{\partial g(x)} \cdot \frac{\partial g(x)}{\partial x} + \frac{\partial g(x)}{\partial x}\\
    \end{aligned}
    $$
    一般而言，梯度都是一个在0附近的高斯分布非常小的数值，所以在计算多层神经网络的时候，梯度在多次求偏导之后都会变得非常小，这是就是梯度消失了。但是在浅层的神经网络中相对而言这个梯度是比较大的，也就是对应于$\frac{\partial g(x)}{\partial x}$这一部分。也就是说在使用了残差网络之后，梯度由于$\frac{\partial g(x)}{\partial x}$值的存在，始终会保持在一个比较大的值（也就是浅层网络上的梯度值，而且之前已经把浅层网络的梯度值已经学习、训练好了），这样无论添加多少层都不会出现梯度消失的情况了。由于梯度始终保持在一个值左右，所以出现误差反传的时候ResNet学习（或者说训练）得就比较快了。
12. SGD的收敛是没有意义的。实际上是说训练不动了，而不是收敛了。这个地方需要再理解一下。
13. gradient boosting是在标号上做residual，而ResNet试下feature上做residual。
14. 虽然添加了ResNet，但是实际上网络的结构的复杂程度反而降低了。模型并没有往人希望的方向走，通过添加这部分内容引导网络向这个方向发展。

## 2. 问题

1. 为什么在CIFAR-10上为什么ResNet的过拟合并不明显，目前还是open question。另外，transformer那么多的参数为什么还可以训练得很好，而且不会过拟合呢？一种解释是：虽然参数很多、层数很深，但是模型的构造是的它的intrinsic内在的模型复杂度并不高，很有可能是加了残差结构之后使得模型的复杂度降低了，从而使得过拟合没那么严重了。所谓模型复杂度降低，不是说不能表示很多东西了，而是能更方便的找到一个不那么复杂的模型去拟合你的数据。模型并没有往人希望的方向走，通过添加这部分内容引导网络向这个方向发展。不加ResNet的NN中，后面的很多层都是无效的。加了ResNet之后等于说把模型复杂度降低了。

## 3. 其他

1. 说人生就像SGD，要保证梯度足够大，才能训练得动，最后的记过才会是好的。
