# 相关知识点笔记

Embedding

## 参考

1. <https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/text/word_embeddings.ipynb>原理部分辅助理解，实现部分没有说清楚。
2. <https://tensorflow.google.cn/text/guide/word_embeddings>是1的英文原版。
3. <https://zhuanlan.zhihu.com/p/46016518>这篇只做了介绍，并没有说得很深入。
4. <https://www.zhihu.com/question/38002635>原理说明的非常清楚。
5. <https://zhuanlan.zhihu.com/p/138310401>这篇好像是所有原理的来源。其中把3种实现方法也说清楚了。
6. <https://zhuanlan.zhihu.com/p/85802954>

## 理解

1. **容易理解**有个举例的例子对于理解embedding非常有用：[就是通过RGB3色来表示所有的颜色](https://www.zhihu.com/question/38002635)。**一句话解释：用低维的向量表示高维空间的图形**。我们已经知道表示颜色的三个维度有明确对应的物理意义（即RGB），直接使用物理原理就可以知道某一个颜色对应的RGB是多少。但是对于词，**我们无法给出每个维度所具备的可解释的意义，也无法直接求出一个词的词向量的值应该是多少**。所以我们需要使用语料和模型来训练词向量——把嵌入矩阵当成模型参数的一部分，通过词与词间的共现或上下文关系来优化模型参数，最后得到的矩阵就是词表中所有词的词向量。这里需要说明的是，有的初学者可能没绕过一个弯，就是“最初的词向量是怎么来的”——其实你只要知道最初的词向量是瞎JB填的就行了。嵌入矩阵最初的参数跟模型参数一样是随机初始化的，然后前向传播计算损失函数，反向传播求嵌入矩阵里各个参数的导数，再梯度下降更新，这个跟一般的模型训练都是一样的。等训练得差不多的时候，嵌入矩阵就是比较准确的词向量矩阵了。
2. 数学解释：**一句话解释：就是将高维空间的图形通过低维空间的形状进行归纳和解释。这其中就需要流形的概念来进行说明**。Embedding（嵌入）是拓扑学里面的词，在深度学习领域经常和Manifold（流形）搭配使用。可以用几个例子来说明，比如三维空间的球体是一个二维流形嵌入在三维空间（2D manifold embedded in 3D space）。之所以说他是一个二维流形，是因为球上的任意一个点只需要用一个二维的经纬度来表达就可以了。又比如一个二维空间的旋转矩阵是2x2的矩阵，其实只需要一个角度就能表达了，这就是一个一维流形嵌入在2x2的矩阵空间。什么是深度学习里的Embedding？这个概念在深度学习领域最原初的切入点是所谓的Manifold Hypothesis（流形假设）。流形假设是指“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”。那么，深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。比如Word Embedding，就是把单词组成的句子映射到一个表征向量。但后来不知咋回事，开始把低维流形的表征向量叫做Embedding，其实是一种误用。如果按照现在深度学习界通用的理解（其实是偏离了原意的），Embedding就是从原始数据提取出来的Feature，也就是那个通过神经网络映射之后的低维向量。
3. 实现的过程：**和训练神经网络一样，也是训练一组权重值**，开始的时候随机初始化，然后在训练过程中通过反向传播来调整这一组权重值。嵌入向量的权重会随机初始化（就像其他任何层一样）。在训练过程中，通过反向传播来逐渐调整这些权重。训练后，学习到的单词嵌入向量将粗略地编码单词之间的相似性（因为它们是针对训练模型的特定问题而学习的）。其实你只要知道最初的词向量是瞎JB填的就行了。嵌入矩阵最初的参数跟模型参数一样是随机初始化的，然后前向传播计算损失函数，反向传播求嵌入矩阵里各个参数的导数，再梯度下降更新，这个跟一般的模型训练都是一样的。等训练得差不多的时候，嵌入矩阵就是比较准确的词向量矩阵了。

## 重要知识点

1. 来源：需要可靠的表示离散变量。比如文字中的单词或者单字。
2. 其他的表示方法的优缺点：
   1. 独热编码：过于稀疏。
   2. 唯一的数字对每个单词编码：
      1. 解决了稀疏的问题。
      2. 不能表示单词之间的关系；
      3. 对模型进行解释的时候学习到的权重没有含义。
3. 在2的基础上需要一种**高效、密集、能表示关系、能自动编码**的表示方法。
4. 生成Embedding的方法可以归类为3种，分别是矩阵分解，无监督建模和有监督建模。
5. 在无监督建模中最经典的方法是word2vec。
6. 相比RGB，Embedding最大的劣势是无法解释每个维度的含义，这也是复杂机器学习模型的通病。
7. embedding层其实是一个全连接神经网络层。最后的输出结果就是一个代表离散量的向量。
