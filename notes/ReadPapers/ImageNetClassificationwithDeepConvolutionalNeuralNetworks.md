# 读论文笔记

这篇论文是AlexNet的实现。

## 1. 重要知识点

1. 深度学习在前主要的期望的应用场景是无监督数据的学习，把没有标注的数据的内在结构/联系提取出来。AlexNet在2010年左右使用深度神经网络对有标注的数据进行了处理。也就是说作者尝试改变了一下关注的数据类型。在BERT和GAN出现之后，大家的关注点又回到了无监督学习（也就是逻辑推理能力）。目前之后的一段时间可能会持续关注这个方向上。
2. video比图片的处理要难很多。原因是：a. 计算量增加比较大；b.video都是有版权的。
3. 深度神经网络最终训练出来的向量在语义空间中的表示特别好。也就是说像是的图片都会把它们放到一起。简单的分类器可以做得非常好。
4. 在论文中AlexNet的第一层卷积到第二层卷积的卷积核的stride为2。所以才会由55*55变为了27*27。第三层到第四层的卷积核的stride也是2，所以才会由27*27变为了13*13。
5. MaxPooling是在一个n*n的采样中取样本中最大的值。MeanPooling是取所有样本的平均值。
6. 卷积立方体是如何变成向量的？如文中所示，全连接层之前是13*13*128的三维数据，是前一层的卷积输出层。这时再次使用卷积运算，卷积核的大小为13*13*128*2048，就将这个立方体变为一维向量了。可以将其理解为一次特殊的卷积操作。2048这个维度上的卷积系数是通过训练得到的。**全连接的作用在于分类，卷积层的作用在于提取特征**。另外，**全连接层的作用是将特征和处于图片的中位置去相关**。这也是AlexNet中对后来最重要的一个启示之一。[重要参考：卷积层向全连接层转化的详细说明，其中还说明了全连接层的作用](https://www.pianshen.com/article/29342003916/)。由于这篇文章讲得非常透彻，所以将其保存下来地址为[WhatisFullyConnectedLayer](/references/WhatisFullyConnectedLayer.pdf)。
7. SGD随机梯度下降，调参相对难一些。但是SGD中的噪音对模型的泛化有好处。
8. 学习率的调参在论文中使用过固定轮次降低10来完成的。现在常用的是通过cos函数来作为学习率调整的函数。

## 2. 问题

1. 正则如何对过拟合进行优化？
2. weight decay权重衰减和L2正则等价？如何理解weigth deacy不是加在模型上面而是加在优化算法上了？
3. momentum的含义？
