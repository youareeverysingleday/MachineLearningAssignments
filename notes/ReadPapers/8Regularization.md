# Regularization 正则化

## 为什么要正则化

一般会碰到3个问题：

1. 如何防止模型的过拟合？
2. 正则化为什么能防止过拟合？如何从数学的角度来描述一下正则化。
3. 什么$l_1$正则化具有稀疏性？为什么$l_1$可以进行特征选择？

另外还会问到：$l_1$和$l_2$之间的区别？

## 详述

1. 为了防止过拟合问题？采用的方法之一就是添加正则项。
2. $L_1:||\omega||_1 = |\omega_1| + |\omega_2| + \cdots + |\omega_n|$
3. $L_2:||\omega||_2 = \omega_1^2 + \omega_2^2 + \cdots + \omega_n^2$
4. 模型的复杂度和参数向量有关。参数越多模型越复杂，参数越少模型越简单。基于此，使得某些参数趋向于或者等于0，那么就降低了模型的复杂度。
5. 由于$l_1$具有稀疏性，使得某些参数等于0了。相当于进行了特征选择过程。
6. [经验风险和结构风险](https://blog.csdn.net/jieming2002/article/details/79229832)：
   1. 经验风险：模型关于训练数据集的平均损失称为经验风险。度量平均意义下模型预测效果的好坏。
   2. 结构风险：结构风险是在经验风险的基础上加上表示模型复杂度的正则项（罚项）。
7. 经验风险最小值，以平方损失函数为例，由经验风险最小化变为结构风险最小化。$L(\omega)=\frac{1}{n} [\sum \limits_{i=1}^{n}(f(x_{i})-y_{i})^2 + \lambda \sum \limits_{i=1}^{n}\omega_i^2]$。为了使得结构风险最小化，就需要对上面的式子求偏导数。
   1. $\frac{\partial L(\omega)}{\partial \omega_i}=0 (i=1,2,3,\cdots ,n) \\
   \frac{\partial L(\omega)}{\partial \lambda_i}=0 (i=1,2,3,\cdots ,n)  \\
   \text{当两个偏导等于0的时候，就可以使得}L(\omega)\text{取得最小值。此时假设}\omega\text{和}\lambda\text{的取值分别为：}\omega^*, \lambda^*$
   2. 为了使得模型不太复杂，希望$\omega$受到一定的约束。那么就可以让$\omega_1^2 + \omega_2^2 + \cdots + \omega_n^2  \leq m$。如果在二维平面上，那么$\omega_1^2 + \omega_2^2 \leq m$就是一个圆形。这就要求$\omega$的取值只能在圆形范围内取值。如果选择$L_1$，那么在二维空间中$|\omega_1| + |\omega_2| \leq m$表示的是一个正方形。这样模型的复杂度就受到的约束。那么问题就转化为了一个有不等式约束的最优化问题。
   3. 为了解决2中的不等式约束的最优化问题，使用KKT给出的判断条件来判断最优解的必要条件。
   4. 通过KKT条件作为结论
   5. 正则化通过3和4中的说明，正则化和带约束条件求解等价(因为它们的解空间是一样的)。正则化带的隐含条件就是带约束。
