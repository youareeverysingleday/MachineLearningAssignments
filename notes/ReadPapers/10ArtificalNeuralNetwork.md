# 人工神经网络

这篇笔记主要记录其中一些比较重要的点。

## 基础

1. 每个神经元是一个抽象的函数表达式。
2. 多层NN使用的就是Taylor展开类似的概念。然后去拟合目标函数的形状。
3. 简单的神经网络和逻辑回归没有区别。通过一个非线性激活函数来使得NN区别于LR。
4. 反向传播：在神经网络输出结果和目标值进行比较的时候，通过求导的链式法则来求每一层的梯度。
5. 梯度消失，在sigmod作为激活函数时会出现这种情况。
   1. ReLu会在一定程度上解决梯度消失的问题，原因是由于ReLu有一部分是0，那么使得梯度较小的神经元直接就死掉了。它的缺点也就在于这里，死掉的神经元就无法再激活了。
   2. 如果激活函数没有选择好的话或者神经网络的层数非常多，由于激活函数求了梯度之后相对于前一层是一个比较小的东西（对于所有梯度而言，输出层的梯度较大，越靠近输入层梯度越小），由于每一层都乘以相对较小的值之后，到最后几层时神经网络的参数并不更新了。
6. 随机梯度下降（stochastic gradient descent, SGD）算法。
7. 学习速率。学习速率过大会导致收敛震荡。过小会导致收敛速度太慢。
8. 神经网络开始训练的时候会给定一个随机的初始值，之所以给的是一个随机值是为了避免神经网络收敛在局部最优点。
9. 损失函数：**训练的是分类目标就不能用均方误差作为损失函数，而是使用零一或者是交叉熵之类的损失函数。如果是回归目标，那么就使用均方误差或者绝对值误差比较合适**。

## 卷积神经网络