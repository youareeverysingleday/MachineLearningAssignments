# 第3章 线性模型

## 目标问题

线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想。许多功能更为强大的非线性模型(nonlinear model)可在线性模型的基础上通过引入层级结构或高维映射而得。

## 名词解释

|编号|英文|中文|章节|理解|参考|
|---|---|---|---|---|---|
|1|nonlinear model|非线性模型|3.1||/|
|2|comprehensibility/understandability|可解释性/可理解性|3.1||/|
|3|parameter estimation|参数估计|3.2||/|
|4|closed-form|闭式解也叫解析解|3.2||/|
|5|multivariate linear regression|多元线性回归|3.2||/|
|6|full-rank matrix|满秩矩阵|3.2||/|
|7|positive definite matrix|正定矩阵|3.2||/|
|8|logistics regression|一般成为逻辑回归，实际上是**对数几率回归**。详细的解释是在于第58页中。|3.||/|
|9|||3.||/|
|10|||3.||/|
||||3.||/|
||||3.||/|

## 关键知识点

1. 均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称"欧氏距离" (Euclidean distance)。**基于均方误差最小化来进行模型求解的方法称为"最小二乘法" (least square method)**。在线性回归中，最小A乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
2. 闭式解（维基百科）又称为解析解，是可以用解析表达式来表达的解。在数学上，如果一个方程或者方程组存在的某些解，是由有限次常见运算的组合给出的形式，则称该方程存在解析解。二次方程的根就是一个解析解的典型例子。在低年级数学的教学当中，解析解也被称为公式解。数值解（维基百科）当解析解不存在时，比如五次以及更高次的代数方程，则该方程只能用数值分析的方法求解近似值。大多数偏微分方程，尤其是非线性偏微分方程，都只有数值解。
3. 现实任务中$X^T X$往往不是满秩矩阵.例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致X 的列数多于行数，$X^T X$显然不满秩.此时可解出多个$\omega^T$， 它们都能使均方误差最小化.选择哪一个解作为输出，将由学习算法的归纳偏好决定， 常见的做法是引入正则化(regularization) 项。公式3.7好像有点问题$w=\frac{\sum\limits_{i=1}^{m}(x_iy_i)-\overline{x}\sum\limits_{i=1}^{m}(y_i)}{m\sum\limits_{i=1}^{m}x_i^2 + \overline{x}\sum\limits_{i=1}^{m}x_i}$，也就是分子部分$\sum\limits_{i=1}^{m}(x_iy_i)$中的$\sum\limits_{i=1}^{m}y_i$不能提取出来。
4. 最简单的线性模型可以参数替换成复杂的非线性模型。**它的实质是在求取输入空间到输出空间的非线性函数映射**。第56-57页。
5. 对于分类问题可以将线性回归通过阶跃函数之后变为分类函数。
6. 理解：基本的线性回归函数是最基本的形式，后面的各种都可以在其上进行变换之后得到。特别是对公式(3.15)这个位置的讲述，定义了广义线性模型（generalized linear model）$y=g^{-1}(w^T x+b)$，其中要求$g(.)$单调可微，也就是连续且充分光滑。那么就称$g(.)$为联系函数（link function）。通过这个定义就可以实现从线性回归到分类回归的转变。需要特别注意这里面的演进过程，是一个从简单到复杂的过程。
7. 对数几率回归$y=\frac{1}{1+e^{-(w^Tx+b)}}$实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为"对数几率回归" (logistic regression，亦称logit regression) .特别需注意到，虽然它的名字是"回归"，但实际却是一种分类学习方法.这种方法有很多优点，例如它是直接对分类可能性进行建模，**无需事先假设数据分布,这样就避免了假设分布不准确所带来的问题(这个地方没有理解)**;它不是仅预测出"类别"，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用;此外，对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。
8. 