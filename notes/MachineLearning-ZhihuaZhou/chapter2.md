# 第2章 模型的评估和选择

## 目标问题

本章的目标问题：“理想的解决方案当然是对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型.然而如上面所讨论的，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，那么，在现实中如何进行模型评估与选择呢?”这是一个“两头堵”的问题。

## 名词解释

|编号|英文|中文|章节|理解|参考|
|---|---|---|---|---|---|
|1|error rate|错误率|2.1|分类错误的样本数占样本总数的比例。|/|
|2|accuracy|精度|2.1|=(1-错误率)，也就是错误率的补集。|/|
|3|error|误差，注意：你没有看错，书上就是error。|2.1|学习器的实际预测输出与样本的真实输出之间的差异称为"误差"|/|
|4|training erro/empirical error|训练误差|2.1|学习器在训练集上的误差称为"训练误差" (training error)或"经验误差" (empirical error)。很多地方看到经验、经验的说法，原来就是**对训练集上取得的结果或者逻辑可以称其为经验**。在训练集上的训练误差很小或者为零并不意味着模型的效果会很好，很可能意味着模型过拟合了。|/|
|5|generalization error|泛化误差|2.1|在新样本上的误差称为"泛化误差" (generalization error)。|/|
|6|overfitting|过拟合|2.1|可能巳经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下|/|
|7|underfitting|欠拟合|2.1|是指对训练样本的一般性质尚未学好|/|
|8|model selection|模型选择|2.1|/|/|
|9|testing set|测试集|2.2|/|/|
|10|testing error|测试误差|2.2|/|/|
|11|hold-out|留出法|2.2.1|对数据集进行处理的一种一般方法。**数据集的划分要尽可能保证数据分别的一致性**。|/|
|12|sampling|采样|2.2.1|/|/|
|13|stratified sampling)|分层采样|2.2.1|则保留类别比例的采样方式通常称为"分层采样"。举例：例如通过对D 进行分层采样而获得含70% 样本的训练集S 和含30% 样本的测试集T， 若D 包含500 个正例、500 个反例，则分层采样得到的S 应包含350 个正例、350 个反例?而T 则包含150 个正例和150 个反例;若S、T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。不同的类别的样本按照一定比例进行采样。|/|
|14|cross validation)|交叉验证法|2.2.2||/|
|15|k折交叉验证|k-fold cross validation|2.2.2|将全集划分为k个子集，其中k-1个子集作为训练集，1个作为测试集。然后换一个子集作为测试集，剩余的k-1个作为训练集。这样就可以得到k个测试结果，然后再取平局值。“交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值”。|/|
|16|bootstraping|自助法|2.2.3||/|
|17|bootstrap sampling|自助法采样|2.||/|
||||2.||/|
||||2.||/|
||||2.||/|
||||2.||/|
||||2.||/|
||||2.||/|
||||2.||/|
||||2.||/|

## 关键知识点

1. 欠拟合通常比较容易解决，但是过拟合是机器学习面临的主要麻烦。这是不是也意味着小样本和对抗数据一方面是解决实际问题，另一方面也是解决机器学习自己本身的问题？
2. 各类学习算法都必然带有一些针对过拟合的措施。然而必须认识到，过拟合是无法彻底避免的，我们所能做的只是"缓解'气或者说减小其风险。这些方法具体是那些呢？
3. 文章中说明的一种保证训练集和测试集采样分布一致的解决方法：单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果.例如进行100 次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果?而留出法返回的则是这100个结果的平均。
4. 对于样本划分大小的详细说明：此外，我们希望评估的是用D 训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:若令训练集S包含绝大多数样本7 则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T 多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D 训练出的模型相比可能有较大差别?从而降低了评估结果的保真性(fidelity) 这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试.
   1. 一句话总结：**这个问题没有完美的解决方案，划分的比例是经验值**。
5. k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值。
