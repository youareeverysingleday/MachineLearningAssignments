# 第2章 模型的评估和选择

## 目标问题

本章的目标问题：“理想的解决方案当然是对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型.然而如上面所讨论的，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，那么，在现实中如何进行模型评估与选择呢?”这是一个“两头堵”的问题。

## 名词解释

|编号|英文|中文|章节|理解|参考|
|---|---|---|---|---|---|
|1|error rate|错误率|2.1|分类错误的样本数占样本总数的比例。|/|
|2|accuracy|精度|2.1|=(1-错误率)，也就是错误率的补集。|/|
|3|error|误差，注意：你没有看错，书上就是error。|2.1|学习器的实际预测输出与样本的真实输出之间的差异称为"误差"|/|
|4|training erro/empirical error|训练误差|2.1|学习器在训练集上的误差称为"训练误差" (training error)或"经验误差" (empirical error)。很多地方看到经验、经验的说法，原来就是**对训练集上取得的结果或者逻辑可以称其为经验**。在训练集上的训练误差很小或者为零并不意味着模型的效果会很好，很可能意味着模型过拟合了。|/|
|5|generalization error|泛化误差|2.1|在新样本上的误差称为"泛化误差" (generalization error)。|/|
|6|overfitting|过拟合|2.1|可能巳经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下|/|
|7|underfitting|欠拟合|2.1|是指对训练样本的一般性质尚未学好|/|
|8|model selection|模型选择|2.1|/|/|
|9|testing set|测试集|2.2|/|/|
|10|testing error|测试误差|2.2|/|/|
|11|hold-out|留出法|2.2.1|对数据集进行处理的一种一般方法。**数据集的划分要尽可能保证数据分别的一致性**。|/|
|12|sampling|采样|2.2.1|/|/|
|13|stratified sampling)|分层采样|2.2.1|则保留类别比例的采样方式通常称为"分层采样"。举例：例如通过对D 进行分层采样而获得含70% 样本的训练集S 和含30% 样本的测试集T， 若D 包含500 个正例、500 个反例，则分层采样得到的S 应包含350 个正例、350 个反例?而T 则包含150 个正例和150 个反例；若S、T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。不同的类别的样本按照一定比例进行采样。|/|
|14|cross validation)|交叉验证法|2.2.2||/|
|15|k折交叉验证|k-fold cross validation|2.2.2|将全集划分为k个子集，其中k-1个子集作为训练集，1个作为测试集。然后换一个子集作为测试集，剩余的k-1个作为训练集。这样就可以得到k个测试结果，然后再取平局值。“交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值”。|/|
|16|bootstraping|自助法|2.2.3||/|
|17|bootstrap sampling|自助法采样|2.2.3|有放回采样|对于数据集较小，难以有效划分训练/测试集是很有用。|
|18|parameter tuning|调参|2.2.4||/|
|19|true positive, TP|真正例|2.3.2|本身是正例，预测也是正例|/|
|20|false positive, FP|假正例|2.3.2|本身是反例，预测为正例|/|
|21|true negative, TN|真反例|2.3.2|本身是反例，预测也是反例|/|
|22|false negative, FN|假反例|2.3.2|本身是正例，预测为反例|/|
|23|Break Event Point, BEP|平衡点|2.3.2||/|
|24|unequa1 cost|非均等代价|2.3.4|对于某些结果的代价并不均等。|/|
||||2.||/|

## 关键知识点

1. 欠拟合通常比较容易解决，但是过拟合是机器学习面临的主要麻烦。这是不是也意味着小样本和对抗数据一方面是解决实际问题，另一方面也是解决机器学习自己本身的问题？
2. 各类学习算法都必然带有一些针对过拟合的措施。然而必须认识到，过拟合是无法彻底避免的，我们所能做的只是"缓解"气或者说减小其风险。这些方法具体是那些呢？
3. 文章中说明的一种保证训练集和测试集采样分布一致的解决方法：单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果.例如进行100 次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果?而留出法返回的则是这100个结果的平均。
4. 对于样本划分大小的详细说明：此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境:若令训练集S包含绝大多数样本7 则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T 多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D 训练出的模型相比可能有较大差别?从而降低了评估结果的保真性(fidelity) 这个问题没有完美的解决方案，常见做法是将大约2/3~4/5的样本用于训练，剩余样本用于测试.
   1. 一句话总结：**这个问题没有完美的解决方案，划分的比例是经验值**。
5. k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值。
6. 留出法：将数据集分成多份，每次留一份作为测试集。然后循环多次，每次测试集都是分成多份中的一份，而且是没用使用过的一份。这样再求平均值作为输出结果。
7. 常用性能评估参数：
   |编号|名称|离散值公式|连续值公式|含义|
   |---|---|---|---|---|
   |1|均方误差|$E(f;D)=\frac{1}{m}\sum \limits_{i=1}^{m} (f(x_i)-y_i)^2$|$E(f;D)=\int_{x-D}(f(x)-y)^2 p(x)dx$|离散值是比较两个向量的距离，连续值的含义还不清楚|
   |2|错误率|$E(f;D)=\frac{1}{m}\sum \limits_{i=1}^{m} (f(x_i)\neq y_i)$|$E(f;D)=\int_{x-D}f(x_i)\neq y_i) p(x)dx$|适用于二分类或者多分类问题。描述分类错误的样本数占样本总数的比例。|
   |3|精度|$acc(f;D)=\frac{1}{m}\sum \limits_{i=1}^{m} (f(x_i)= y_i)=1-E(f;D)$|$E(f;D)=\int_{x-D}(f(x_i)= y_i)p(x)dx=1-E(f;D)$||
   |4|查准率(precision)|$P=\frac{TP}{TP+FP}$|/|二分类问题|
   |5|查全率(recall)|$P=\frac{TP}{TP+FN}$|||
   |6|平衡点(Break Event Point, BEP)|/|/|用于度量查准率和查全率的综合性能评价。它可以通过P-R图来展示的。|
   |7|F1度量|$F1=\frac{2\times P\times R}{P+R}=\frac{2 \times TP}{\text{样例总数}+TP-TN}$。F1是基于查准率与查全率的调和平均(harinonicmean)定义的:$\frac{1}{F1}=\frac{1}{2}(\frac{1}{P} + \frac{1}{R})$。另外$F_{\beta}$则是加权调和平均：$\frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}(\frac{1}{P} + \frac{\beta^2}{R})$。|||
   |8|平衡点Break-Event Point|/|/|就是这样一个度量，它是" 查准率=查全率"时的取值。但是这种方法过于简单了。不利于评价。|
   |9|cut point截断点|/|/|"截断点" (cut point)将样本分为两部分，前一部分判作正例，后一部分则判作反例.|
   |10|Receiver Operating Characteristic, ROC受试者工作特征|/|/|ROC 曲线的纵轴是"真正例率" (True ositive Rate，简称TPR) ，横轴是"假正例率" (False Positive Rate，简称FPR)|
   |11|Area Under ROC Curve，在ROC曲线下的面积|/|/|/|
   |12|unequa1 cost|/|/|非均等代价|
   ||||||
   ||||||
   ||||||
   ||||||
   ||||||

8. 查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。例如，若希望将好瓜尽可能多地选出来，则可通过增加选瓜的数量来实现，如果将所有西瓜都选上，那么所有的好瓜也必然都被选上了，但这样查准率就会较低；若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜， 但这样就难免会漏掉不少好瓜，使得查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。
   1. 在一些应用中，对查准率和查全率的重视程度有所不同。**例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，此时查准率更重要;而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要**。
   2. 将数据集进行分割之后（比如，上面所说的留出法）会有多次训练/测试，每次得到一个混淆矩阵之后需要将不同混淆矩阵上计算的查准率和查全率在进行平均。

9. [ROC曲线的原理](https://www.jianshu.com/p/2ca96fce7e81)ROC曲线正是通过不断移动分类器的“阈值”来生成曲线上的一组关键点的。也就是说ROC曲线的生成除了坐标轴上的两个坐标之外，还体现了**分类阈值**这个参数。
   1. 目的：用于区分区分器的性能的。最初是用来区分雷达操作员对产生的雷达信号的判断能力的。
   2. 步骤：
      1. 首先设置一组有正样例和负样例组成的样本，正样例和负样例的比例确定。
      2. 然后统计区分器预测正样例和负样例的数据。
      3. 计算TPR和FPR。TPR和FPR分别作为纵坐标和横坐标的坐标画在图上。
      4. 重新设置一组不同正负样本比例的样本，然后重复2-3步骤，将所有生成的点画在图上。
   3. AUC是计算的ROC的面积。
   4. ROC图的横纵坐标轴的范围都是0到1之间。
10. 非均等代价，也就是说在某些问题的时候不同的结果对应不同的代价。这个时候衡量模型的指标时的主要思路是**将不同结果的代价进行归一化**，然后再进行评估。
    1. 非均等代价举例：不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了"一次错误"但后者的影响是增加了进→步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予"非均等代价" (unequa1 cost).
11. 统计假设检验（hypothesis test）
