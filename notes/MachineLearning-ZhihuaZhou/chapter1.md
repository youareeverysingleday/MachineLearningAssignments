# 机器学习-周志华学习笔记

入坑周老师的《机器学习》。
先粗读一遍，然后再仔细研究里面的内容。

## 绪论

### 名词解释

|编号|英文|中文|章节|理解|参考|
|---|---|---|---|---|---|
|1|classification|分类|1.2|/|/|
|2|regression|回归|1.2|/|/|
|3|binary classification|二分类|1.2|/|/|
|4|positive sample|正样本|1.2|/|/|
|5|negative sample|负样本|1.2|/|/|
|6|independent |独立同分布|1.2|我们获得的每个样本都是独立地从这个分布上采样获得的。一般而言，训练样本越，我们得到的关于D的信息这样就越有可能通过学习获得具有强泛化能力的模型。|/|
|7|clustering|聚类|1.2|/|/|
|8|cluster|簇（cu4）|1.2|/|/|
|9|supervised learning|监督学习|1.2|分类和回归是前者的代表|/|
|10|unsupervised learning|无监督学习|1.2|聚类是后者的代表|/|
|11|generalization|泛化|1.2|对新样本的适应性|/|
|12|inductive bias|归纳偏好|1.4 p6|这种偏好看书里面的说明好像是必须的。不然就没有意义了。还有点不理解。和奥卡姆剃刀原则发生了联系。|/|
|13|Sparse Distributed Memory|稀疏分布存储|1.6|过建立一些关于学习的计算模型来促进我们理解"人类如何学习。结果是一定程度上模拟了仿脑生理结构。SDM 的稀疏编码机制在视觉、昕觉、嗅觉功能的脑皮层中广泛存在|/|

### 关键知识点

1. 如果预测的值是离散值那么就是分类任务（classification）
2. 如果预测的是连续值，那么就是回归任务（regression）。注意这里就是一种定义。
3. 正类：positive class，负类：negative class。二分类任务：binary classification。
4. 聚类之前并不知道聚类之后的类别之间的分类逻辑是什么。训练的过程中通常都是不知道类别的标签的。分类之后的每个组成为称为"簇cluster（cu4）"。
5. 分类和回归都是监督学习，聚类是无监督学习。
6. 独立同分布，这是特别在说明训练集和全体样本空间之间的关系。
7. 一般都假设样本空间是满足某一个特定的分布的。
8. 归纳偏好，如果，即"若有多个假设与观察一致，则选最简单的那个"如果采用这个原则，并且假设我们认为"更平滑"意味着"更简单"。
9. 奥卡姆剃刀本身存在不同的诠释，使用奥卡姆剃刀原则并不平凡.例如对我们已经很熟悉的西瓜问题来说，"假设1: 好瓜件(色泽=*)^(根蒂=蜷缩)^(敲声=浊响)"和假设2:"好瓜件(色泽=*)^(根蒂=蜷缩)^(敲声=*)"这两个假设，哪一个更"简单"呢?这个问题并不简单，需借助其他机制才能解决。（这里不应该是假设2更简单吗？我理解这里应该是对应不同的问题可能有不同的评价方式，这里就不能确定到底是假设1好还是假设2好了。）
10. 站在不同问题的角度上看算法，那么**任何**（注意强调是任何）一种在某些问题上表现优异的算法有可能在另外一些问题上表现的不行。这不就是有位名人说的：“不能指望一种算法解决所有问题”吗？（所以说“很难找到全局最优解，只能在代价可以接受的情况下的局部最优解”。）
11. NFL定理最重要的寓意?是让我们清楚地认识到，脱离具体问题，空泛地谈论"什么学习算法更好"毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好.要谈论算法的相对优劣，必须要针对具体的学习问题;在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用.
    1. 不可能找到对一个很广泛问题的最优解，一定要找到一个具体问题，然后将其解决。另外这个“广泛”如何确定呢？学习算法自身的归纳偏好与问题是否相配，非常重要。
12. **典型的决策树学习以信息论为基础，以信息熵的最小化为目标**，直接模拟了人类对概念进行判定的树形流程。
13. 神经网络显然降低了机器学习的门槛。有两个基本原因:数据大了、计算能力强了。原因是需要大量的数据去反向求导，并且需要大量的计算能力去计算反向求导。神经网络是一种连接主义学习技术。
