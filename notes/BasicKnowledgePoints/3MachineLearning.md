# 机器学习

一些基本概念。只做参考。
这门课内容是按照周志华《机器学习》来讲的。

## 1. 机器学习概述

1. 概念：机器学习研究的是计算机怎样模拟人类的学习行为，以获得新的知识或技能，并重新组织已有的知识结构使之不断改善自身。就是计算机从数据中学习出规律和模式，以应用在新数据上做预测的任务。
2. 能解决的问题：聚类、分类、回归、强化学习。（实际上就是两类：分类和回归，分别对应离散值和连续值）。聚类是无监督学习，挖掘数据的关联关系。强化学习主要用于互动环境中。
3. 机器学习分类：
   1. 监督学习，特征和标签。
   2. 无监督学习，关联规则。
   3. 强化学习，从环境到行为映射的学习。
4. 工作阶段：数据预处理（特征抽取，幅度缩放，特征选择，降维，采样），模型学习（模型选择，交叉验证，结果评估，超参选择，模型训练），模型评估，新样本预测（机器学习实用阶段）。
5. 评估指标：错误率低，准确率高。
6. 数据采样的方法：
   1. 留出法：保持数据分布一致，多次重复划分，测试集不能太大和太小。
   2. k折交叉验证。
   3. 自助采样法。对样本进行有放回重复采样，取到的数据作为训练集，没有取到的数据作为测试集。
7. 度量标准：
   1. 性能度量：衡量模型泛化能力的数值评价标准。回归问题常采用的是均方误差。
   2. 分类问题常用的性能度量：错误率和精度。
      1. 错误率：分类错误的样本数除以总数。
      2. 精度：分类准确的样本数除以总数。
      3. 混淆矩阵：二分类混淆矩阵。查准率和查全率。
      4. $F_1$值和$F_{\beta}$。
      5. ROC和AUC，曲线是ROC，曲线下的面积就是AUC。
      6. 回归问题的度量标准：
         1. 平均绝对误差(Mean Absolute Error)$\text{MAE}=\frac{1}{n} \sum\limits_{i=1}^{n}|f_i -y_i|$。
         2. 均方误差(Mean Square Error)$\text{MSE}=\frac{1}{n} \sum\limits_{i=1}^{n}(f_i -y_i)^2$。
         3. 方根误差(Root Mean Square Error)$\text{RMSE}=\sqrt{\text{MSE}}$。
         4. R平方$r^2=1-\frac{SS_{res}}{SS_{tot}}=1-\frac{\sum(y_i-f_i)^2}{\sum(y_i-\overline{y})^2}$。
   3. 机器学习的目标：找到具有泛化能力的“好模型”（这句话不准确，泛化能力强的情况下，大概率其他性能就会下降，实际上是对应不同任务性能就会有所倾斜。应该是综合能力好的模型）。
8. 机器学习算法一览：
   1. 非监督算法：
      1. 对于连续值（continuous）：聚类和降维算法（Clustering and Dimensionality Reduction）。具体包含：SVD，PCA，K-Means。
      2. 对于离散值（分类值，categorical）。
         1. 关联分析（association analysis），具体包含Apriori和FP-Growth。
         2. 马尔科夫链（Hidden markov model）。
   2. 监督算法：
      1. 对于连续值：回归（）决策树，随机森林。
      2. 对于离散值（分类值，categorical）。
         1. 分类具体包含：KNN，Trees，Logistic Regression，Naive-Bayes，SVM。
9. 一般选择模型的流程![GeneralProcessofModelSelection](../../pictures/GeneralProcessofModelSelection.jpg)：

    ```mermaid
    graph TD
        A[start]-->B{数据量是否少于50}
        B-->|no|C[补充数据]
        B-->|yes|D{分类还是回归问题要求输出的是连续值还是离散值}
        D-->|分类|E{数据中是否含有标签数据}
        D-->|分类|F{数据中是否含有标签数据}
        E-->|有标签监督|G[classification分类问题]
        E-->|没有标签无监督|H[clustering聚类问题]
        F-->|有标签|I[regression回归问题]
        F-->|没有标签|J[dimenisonality reduction降维问题]
    ```

10. 不同的算法对相同的问题有不同的处理方法。不同的算法带来的决策边界是不一样的。 回归问题有不同的拟合方式。

## 2. 线性回归和逻辑回归

1. 线性模型（linear model）。特点：简单、基本、可解释性好。通过样本属性的线性组合来进行。
   1. 分类：通过一条线来将两类数据分开。
   2. 回归，通过一条线对所有数据进行拟合。
2. 损失函数loss function。
3. 通过损失函数就将回归问题转换为了优化问题。即为对凸函数求极值。
4. 梯度下降法来求凸函数的极值。梯度是决定迭代的方向。迭代的步长通过其他方法来决定。对于一元函数的损失函数计算方法$\theta_1 = \theta_1-\alpha \frac{dJ(\theta_1)}{d\theta_1}$，其中步长通过$\alpha$决定，方向由$\frac{dJ(\theta_1)}{d\theta_1}$决定。
   1. 梯度就是损失函数的切线方向。
   2. 超参数$\alpha$决定步长。
   3. 每次都需要更新$\theta$，最终找到最优$\theta$。
5. $\alpha$也称为学习率，不能太小也不能太大。
6. 欠拟合和过拟合。欠拟合好解决，过拟合不好解决。
   1. 欠拟合：模型没有很好的捕捉到数据特征，不能很好的拟合数据。
   2. 过拟合：把样本中的一些噪声特性也学习了下来，泛化能力差。
7. 减小过拟合的方法：正则化。通过正则化添加参数“惩罚”，控制参数幅度限制参数搜索空间，减小过拟合风险。原始的损失函数是：$J(\theta)=\frac{1}{2m} \sum \limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$修改为了$J(\theta)=\frac{1}{2m} [\sum \limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2 + \lambda \sum \limits_{i=1}^{m}\theta_i^2]$。将模型的参数添加到了损失函数中，期望模型的参数越小越好。
   1. 一般而言是参数越少那
   2. 么模型的复杂度就越低，通过将参数设置为0就可以降低模型的复杂度。
   3. 从数学的角度而言：
   4. 为什么$l_1$正则化具有稀疏性？为什么$l_1$可以进行特征选择？[详细说明](../ReadPapers/8Regularization.md)
   5. $l_1$和$l_2$之间的区别？
8. 广义线性模型，比如$lny=\boldsymbol{w}^T \boldsymbol{x}+b$，这样就得到了对数线性回归。
9. 逻辑回归：
   1. 线性回归与逻辑回归的关系。在逻辑回归中并不会去拟合样本的分布，而是确定决策边界。包括线性决策边界和非线性决策边界。这里需要说到的是sigmod函数作为非线性函数来对线性值进行映射。
      1. 线性的对应的就是的直线函数。
      2. 非线性的对应的就是各种非直线的函数，比如圆形、抛物线等等。
   2. 逻辑回归的决策边界。
   3. 逻辑回归损失函数。
      1. 损失函数不能再使用均方差损失（MSE），这样可能导致出现局部最优解（local cost minimum）。而实际上是期望求得全局最优解（global cost minimum）的。因为在局部最优解的时候梯度已经为0了（也就是说对应的损失函数不是凸函数）。
      2. 使用的损失函数是对数损失/二元交叉熵损失（~~最大似然到对数损失，这个位置不清楚~~）
         $$
         Cost(h_{\theta}(x),y) = \left\{
         \begin{aligned}
         & -log(h_{\theta}(x)) \text{, if y=1}\\
         & -log(1-h_{\theta}(x)) \text{, if y=0}
         \end{aligned}
         \right.
         $$
      3. 损失函数与正则化。依旧存在过拟合问题，决策边界可能抖动得很厉害。(下面的公式可能有问题。)
         $$
         \begin{aligned}
         &\text{损失函数：} \\
         &J(\theta)=\frac{1}{m} \sum\limits_{i=1}^{m}Cost(h_{\theta}(x_i),y_i)\\
         &=-\frac{1}{m} [\sum\limits_{i=1}^{m}y_i logh_{\theta}(x_i)+(1-y_i)log(1-h_{\theta}(x_i))]\\
         &\text{添加正则化项之后的损失函数：}\\
         &J(\theta)=-\frac{1}{m} [\sum\limits_{i=1}^{m}y_i logh_{\theta}(x_i)+(1-y_i)log(1-h_{\theta}(x_i))] + \frac{\lambda}{2m}\sum\limits_{j=1}^{m}\theta_j^2\\
         & \text{使用梯度下降的方法来求最小值：}\\
         & \theta_j=\theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}
         \end{aligned}\\
         $$
         同样是使用梯度下降的方法来求最小值。
   4. 从二分类到多分类。有两种思路，这两种思路共同特点就是将多分类问题转换为二分类问题来解决。
      1. 思路1：将每个类别和除该类别之外的认为是两类，然后分类。针对所有类别逐一做二分类。
      2. 思路2：~~显然没有说清楚（理解好像是对每两个类别之间做一个分类器，也就是两两分类）~~。
10. 工程应用经验
    1. 逻辑回归和其他模型。
       1. 逻辑回归的特点：
          1. LR能以概率的形式输出结果，而非0，1判定。
          2. LR的可解释性强，可控度高。
          3. 训练快，特征工程（feature engineering）之后效果很好。
          4. 因为结果是概率，可以做排序模型。
          5. 添加特征非常简单。
       2. 应用
          1. CTR预估和推荐系统的learning to rank各种分类场景。
          2. 很多搜索引擎厂的公告CTR预估基线版是LR。
          3. 电商搜索排序/广告CTR预估基线版是LR。
          4. 新闻APP的推荐和排序基线也是LR。
    2. 样本处理。
       1. 样本特征处理：离散化后用独热向量编码（one-hot encoding）处理成0，1值。LR训练连续值，注意做幅度缩放（scaling，不同特征的取值需要在同一个范围之内）。
       2. 处理大样本量：事实spark或者MLib，试试采样（注意是否需要分层采样）。
       3. 注意样本平衡：对样本分布敏感（不能使得不同类别的样本的分布不均匀）。通过欠采样和过采样来处理不同的样本数量。另外也可以修改损失函数给不同的样本以不同的权重来解决样本不平衡问题。
    3. 工具包和库。
       1. 常用python库sklearn。
       2. python绘图库

         ```Python
         from mpl_toolkits.mplot3d import axes3d
         from sklean.preprocessing import PolynomialFeatures
         poly = PloyomialFeatures(6) #引入多项式特征，用于将二维数据映射到高维空间。
         ```

    4. 正则化系数太大或者太小会出现的情况，分别对决策边界产生的影响。**正则化牺牲了模型的精度提高了泛化性能**。
       1. lambda=0 就是没有正则化，这样的话就会过拟合。
       2. lambda=1 这是正常值。
       3. lambda=100 正则化项太激进，导致基本没有拟合出决策边界。

## 3. 决策树模型概述
