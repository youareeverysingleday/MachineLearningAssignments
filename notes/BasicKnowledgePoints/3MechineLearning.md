# 机器学习

一些基本概念。只做参考。
这门课内容是按照周志华《机器学习》来讲的。

## 1. 机器学习概述

1. 概念：机器学习研究的是计算机怎样模拟人类的学习行为，以获得新的知识或技能，并重新组织已有的知识结构使之不断改善自身。就是计算机从数据中学习出规律和模式，以应用在新数据上做预测的任务。
2. 能解决的问题：聚类、分类、回归、强化学习。（实际上就是两类：分类和回归，分别对应离散值和连续值）。聚类是无监督学习，挖掘数据的关联关系。强化学习主要用于互动环境中。
3. 机器学习分类：
   1. 监督学习，特征和标签。
   2. 无监督学习，关联规则。
   3. 强化学习，从环境到行为映射的学习。
4. 工作阶段：数据预处理（特征抽取，幅度缩放，特征选择，降维，采样），模型学习（模型选择，交叉验证，结果评估，超参选择，模型训练），模型评估，新样本预测（机器学习实用阶段）。
5. 评估指标：错误率低，准确率高。
6. 数据采样的方法：
   1. 留出法：保持数据分布一致，多次重复划分，测试集不能太大和太小。
   2. k折交叉验证。
   3. 自助采样法。对样本进行有放回重复采样，取到的数据作为训练集，没有取到的数据作为测试集。
7. 度量标准：
   1. 性能度量：衡量模型泛化能力的数值评价标准。回归问题常采用的是均方误差。
   2. 分类问题常用的性能度量：错误率和精度。
      1. 错误率：分类错误的样本数除以总数。
      2. 精度：分类准确的样本数除以总数。
      3. 混淆矩阵：二分类混淆矩阵。查准率和查全率。
      4. $F_1$值和$F_{\beta}$。
      5. ROC和AUC，曲线是ROC，曲线下的面积就是AUC。
      6. 回归问题的度量标准：
         1. 平均绝对误差(Mean Absolute Error)$\text{MAE}=\frac{1}{n} \sum\limits_{i=1}^{n}|f_i -y_i|$。
         2. 均方误差(Mean Square Error)$\text{MSE}=\frac{1}{n} \sum\limits_{i=1}^{n}(f_i -y_i)^2$。
         3. 方根误差(Root Mean Square Error)$\text{RMSE}=\sqrt{\text{MSE}}$。
         4. R平方$r^2=1-\frac{SS_{res}}{SS_{tot}}=1-\frac{\sum(y_i-f_i)^2}{\sum(y_i-\overline{y})^2}$。
   3. 机器学习的目标：找到具有泛化能力的“好模型”（这句话不准确，泛化能力强的情况下，大概率其他性能就会下降，实际上是对应不同任务性能就会有所倾斜。应该是综合能力好的模型）。
8. 机器学习算法一览：
   1. 非监督算法：
      1. 对于连续值（continuous）：聚类和降维算法（Clustering and Dimensionality Reduction）。具体包含：SVD，PCA，K-Means。
      2. 对于离散值（分类值，categorical）。
         1. 关联分析（association analysis），具体包含Apriori和FP-Growth。
         2. 马尔科夫链（Hidden markov model）。
   2. 监督算法：
      1. 对于连续值：回归（）决策树，随机森林。
      2. 对于离散值（分类值，categorical）。
         1. 分类具体包含：KNN，Trees，Logistic Regression，Naive-Bayes，SVM。
9. 一般选择模型的流程![GeneralProcessofModelSelection](../../pictures/GeneralProcessofModelSelection.jpg)：

    ```mermaid
    graph TD
        A[start]-->B{数据量是否少于50}
        B-->|no|C[补充数据]
        B-->|yes|D{分类还是回归问题要求输出的是连续值还是离散值}
        D-->|分类|E{数据中是否含有标签数据}
        D-->|分类|F{数据中是否含有标签数据}
        E-->|有标签监督|G[classification分类问题]
        E-->|没有标签无监督|H[clustering聚类问题]
        F-->|有标签|I[regression回归问题]
        F-->|没有标签|J[dimenisonality reduction降维问题]
    ```

10. 不同的算法对相同的问题有不同的处理方法。不同的算法带来的决策边界是不一样的。 回归问题有不同的拟合方式。

## 2. 线性回归和逻辑回归

1. 线性模型（linear model）。特点：简单、基本、可解释性好。通过样本属性的线性组合来进行。
   1. 分类：通过一条线来将两类数据分开。
   2. 回归，通过一条线对所有数据进行拟合。
2. 损失函数loss function。
3. 通过损失函数就将回归问题转换为了优化问题。即为对凸函数求极值。
4. 梯度下降法来求凸函数的极值。梯度是决定迭代的方向。迭代的步长通过其他方法来决定。对于一元函数的损失函数计算方法$\theta_1 = \theta_1-\alpha \frac{dJ(\theta_1)}{d\theta_1}$，其中步长通过$\alpha$决定，方向由$\frac{dJ(\theta_1)}{d\theta_1}$决定。
   1. 梯度就是损失函数的切线方向。
   2. 超参数$\alpha$决定步长。
   3. 每次都需要更新$\theta$，最终找到最优$\theta$。
5. $\alpha$也称为学习率，不能太小也不能太大。
6. 欠拟合和过拟合。欠拟合好解决，过拟合不好解决。
   1. 欠拟合：模型没有很好的捕捉到数据特征，不能很好的拟合数据。
   2. 过拟合：把样本中的一些噪声特性也学习了下来，泛化能力差。
7. 减小过拟合的方法：正则化。通过正则化添加参数“惩罚”，控制参数幅度限制参数搜索空间，减小过拟合风险。原始的损失函数是：$J(\theta)=\frac{1}{2m} \sum \limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$修改为了$J(\theta)=\frac{1}{2m} [\sum \limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2 + \lambda \sum \limits_{i=1}^{m}\theta_i^2]$。将模型的参数添加到了损失函数中，期望模型的参数越小越好。
   1. 一般而言是参数越少那么模型的复杂度就越低，通过将参数设置为0就可以降低模型的复杂度。
   2. 从数学的角度而言：
   3. 为什么$l_1$正则化具有稀疏性？为什么$l_1$可以进行特征选择？
   4. $l_1$和$l_2$之间的区别？